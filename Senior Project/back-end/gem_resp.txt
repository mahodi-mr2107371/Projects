2025-04-05 17:29:45.870676 - Text query received: hi...
2025-04-05 17:29:45.870676 - Processing request after 0.00s in queue
2025-04-05 17:29:48.444663 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi there! How can I help you today?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 1,
        "candidates_token_count": 11,
        "total_token_count": 12
      }
    }),
)
2025-04-05 17:32:58.421354 - Text query received: Hello, world!...
2025-04-05 17:32:58.422351 - Text query received: Hello, world!...
2025-04-05 17:32:58.422351 - Processing request after 0.00s in queue
2025-04-05 17:32:59.129741 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:32:59.132433 - Processing request after 0.71s in queue
2025-04-05 17:32:59.897795 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:32:59.897795 - Text query received: Hello, world!...
2025-04-05 17:32:59.897795 - Processing request after 0.00s in queue
2025-04-05 17:33:00.592512 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:33:00.594707 - Text query received: Hello, world!...
2025-04-05 17:33:00.595770 - Processing request after 0.00s in queue
2025-04-05 17:33:01.358399 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:33:01.365459 - Text query received: Hello, world!...
2025-04-05 17:33:01.365459 - Processing request after 0.00s in queue
2025-04-05 17:33:02.061998 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:33:02.061998 - Text query received: Hello, world!...
2025-04-05 17:33:02.061998 - Processing request after 0.00s in queue
2025-04-05 17:33:02.813969 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-05 17:33:02.817130 - Text query received: Hello, world!...
2025-04-05 17:33:02.821650 - Processing request after 0.00s in queue
2025-04-05 17:33:03.609437 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:33:03.609437 - Text query received: Hello, world!...
2025-04-05 17:33:03.609437 - Processing request after 0.00s in queue
2025-04-05 17:33:04.331832 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:33:04.331832 - Text query received: Hello, world!...
2025-04-05 17:33:04.331832 - Processing request after 0.00s in queue
2025-04-05 17:33:05.248982 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:33:05.251714 - Text query received: Hello, world!...
2025-04-05 17:33:05.251714 - Processing request after 0.00s in queue
2025-04-05 17:33:05.990802 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-05 17:33:06.006691 - Text query received: Hello, world!...
2025-04-05 17:33:06.006691 - Processing request after 0.00s in queue
2025-04-05 17:33:06.890951 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:33:06.894949 - Text query received: Hello, world!...
2025-04-05 17:33:06.894949 - Processing request after 0.00s in queue
2025-04-05 17:33:07.712814 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:33:07.713592 - Text query received: Hello, world!...
2025-04-05 17:33:07.716617 - Processing request after 0.00s in queue
2025-04-05 17:33:08.528594 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:33:08.530855 - Text query received: Hello, world!...
2025-04-05 17:33:08.530855 - Processing request after 0.00s in queue
2025-04-05 17:33:09.236142 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:33:09.238932 - Text query received: Hello, world!...
2025-04-05 17:33:09.238932 - Processing request after 0.00s in queue
2025-04-05 17:33:10.001654 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:33:10.006611 - Text query received: Hello, world!...
2025-04-05 17:33:10.006611 - Processing request after 0.00s in queue
2025-04-05 17:33:10.011683 - Throttling request, waiting for 49.42 seconds
2025-04-05 17:33:10.013432 - Text query received: Hello, world!...
2025-04-05 17:34:00.195694 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:34:00.203514 - Processing request after 50.19s in queue
2025-04-05 17:34:00.931746 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-05 17:34:00.931746 - Text query received: Hello, world!...
2025-04-05 17:34:00.931746 - Processing request after 0.00s in queue
2025-04-05 17:34:01.673938 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:34:01.675959 - Text query received: Hello, world!...
2025-04-05 17:34:01.677970 - Processing request after 0.00s in queue
2025-04-05 17:34:02.453137 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-05 17:34:02.457163 - Text query received: Hello, world!...
2025-04-05 17:34:02.457163 - Processing request after 0.00s in queue
2025-04-05 17:34:03.179216 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:34:03.186023 - Text query received: Hello, world!...
2025-04-05 17:34:03.186529 - Processing request after 0.00s in queue
2025-04-05 17:34:03.894731 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:34:03.902144 - Text query received: Hello, world!...
2025-04-05 17:34:03.903740 - Processing request after 0.00s in queue
2025-04-05 17:34:04.647398 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-05 17:34:04.647398 - Text query received: Hello, world!...
2025-04-05 17:34:04.647398 - Processing request after 0.00s in queue
2025-04-05 17:34:05.466518 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:34:05.466518 - Text query received: Hello, world!...
2025-04-05 17:34:05.466518 - Processing request after 0.00s in queue
2025-04-05 17:34:06.199382 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:34:06.201858 - Text query received: Hello, world!...
2025-04-05 17:34:06.201858 - Processing request after 0.00s in queue
2025-04-05 17:34:07.098172 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-05 17:34:07.103366 - Text query received: Hello, world!...
2025-04-05 17:34:07.103366 - Processing request after 0.00s in queue
2025-04-05 17:34:07.786811 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:34:07.790211 - Text query received: Hello, world!...
2025-04-05 17:34:07.792133 - Processing request after 0.00s in queue
2025-04-05 17:34:08.532022 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-05 17:34:08.532022 - Text query received: Hello, world!...
2025-04-05 17:34:08.538285 - Processing request after 0.01s in queue
2025-04-05 17:34:09.261465 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:34:09.261465 - Text query received: Hello, world!...
2025-04-05 17:34:09.261465 - Processing request after 0.00s in queue
2025-04-05 17:34:09.961569 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:34:09.966544 - Text query received: Hello, world!...
2025-04-05 17:34:09.966544 - Processing request after 0.00s in queue
2025-04-05 17:34:10.648904 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:34:10.657022 - Text query received: Hello, world!...
2025-04-05 17:34:10.659123 - Processing request after 0.00s in queue
2025-04-05 17:34:10.659123 - Throttling request, waiting for 49.77 seconds
2025-04-05 17:34:10.661668 - Text query received: Hello, world!...
2025-04-05 17:35:25.137739 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:35:25.139760 - Processing request after 74.48s in queue
2025-04-05 17:35:25.861076 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:43.876827 - Text query received: Hello, world!...
2025-04-05 17:40:43.878014 - Text query received: Hello, world!...
2025-04-05 17:40:43.879091 - Processing request after 0.00s in queue
2025-04-05 17:40:46.007764 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:46.007764 - Processing request after 2.13s in queue
2025-04-05 17:40:46.869066 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-05 17:40:46.872505 - Text query received: Hello, world!...
2025-04-05 17:40:46.872505 - Text query received: Hello, world!...
2025-04-05 17:40:46.875609 - Text query received: Hello, world!...
2025-04-05 17:40:46.875609 - Text query received: Hello, world!...
2025-04-05 17:40:46.877626 - Text query received: Hello, world!...
2025-04-05 17:40:46.877626 - Text query received: Hello, world!...
2025-04-05 17:40:46.879642 - Text query received: Hello, world!...
2025-04-05 17:40:46.880439 - Text query received: Hello, world!...
2025-04-05 17:40:46.881164 - Text query received: Hello, world!...
2025-04-05 17:40:46.883578 - Processing request after 0.01s in queue
2025-04-05 17:40:47.620267 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:47.620267 - Processing request after 0.75s in queue
2025-04-05 17:40:48.320973 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:48.320973 - Processing request after 1.45s in queue
2025-04-05 17:40:49.133951 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:49.135787 - Processing request after 2.26s in queue
2025-04-05 17:40:49.948153 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:49.950684 - Processing request after 3.07s in queue
2025-04-05 17:40:50.769309 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:50.769309 - Processing request after 3.89s in queue
2025-04-05 17:40:51.455305 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:51.455305 - Processing request after 4.58s in queue
2025-04-05 17:40:52.401710 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:52.401710 - Processing request after 5.52s in queue
2025-04-05 17:40:53.226058 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:53.228058 - Processing request after 6.35s in queue
2025-04-05 17:40:54.001916 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  Back at you!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 10,
        "total_token_count": 14
      }
    }),
)
2025-04-05 17:40:54.013460 - Text query received: Hello, world!...
2025-04-05 17:40:54.014026 - Text query received: Hello, world!...
2025-04-05 17:40:54.015256 - Text query received: Hello, world!...
2025-04-05 17:40:54.016461 - Text query received: Hello, world!...
2025-04-05 17:40:54.016998 - Processing request after 0.00s in queue
2025-04-05 17:40:54.814360 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:54.814360 - Processing request after 0.80s in queue
2025-04-05 17:40:55.512018 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:55.512018 - Processing request after 1.50s in queue
2025-04-05 17:40:56.227419 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:56.230638 - Processing request after 2.21s in queue
2025-04-05 17:40:56.956499 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:40:56.956499 - Text query received: Hello, world!...
2025-04-05 17:40:56.967789 - Text query received: Hello, world!...
2025-04-05 17:40:56.973152 - Text query received: Hello, world!...
2025-04-05 17:40:56.973782 - Text query received: Hello, world!...
2025-04-05 17:40:56.973782 - Processing request after 0.02s in queue
2025-04-05 17:40:56.973782 - Throttling request, waiting for 47.91 seconds
2025-04-05 17:40:56.976921 - Text query received: Hello, world!...
2025-04-05 17:40:56.976921 - Text query received: Hello, world!...
2025-04-05 17:40:56.976921 - Text query received: Hello, world!...
2025-04-05 17:40:56.976921 - Text query received: Hello, world!...
2025-04-05 17:40:56.976921 - Text query received: Hello, world!...
2025-04-05 17:40:56.976921 - Text query received: Hello, world!...
2025-04-05 17:40:56.976921 - Text query received: Hello, world!...
2025-04-05 17:40:56.983967 - Processing request after 0.01s in queue
2025-04-05 17:40:56.983967 - Throttling request, waiting for 47.90 seconds
2025-04-05 17:40:56.983967 - Processing request after 0.01s in queue
2025-04-05 17:40:56.983967 - Throttling request, waiting for 47.90 seconds
2025-04-05 17:40:56.983967 - Processing request after 0.01s in queue
2025-04-05 17:40:56.983967 - Throttling request, waiting for 47.90 seconds
2025-04-05 17:40:56.988179 - Processing request after 0.01s in queue
2025-04-05 17:40:56.988179 - Throttling request, waiting for 47.89 seconds
2025-04-05 17:40:56.991971 - Processing request after 0.02s in queue
2025-04-05 17:40:56.991971 - Throttling request, waiting for 47.89 seconds
2025-04-05 17:40:56.991971 - Processing request after 0.02s in queue
2025-04-05 17:40:56.991971 - Throttling request, waiting for 47.89 seconds
2025-04-05 17:40:56.991971 - Processing request after 0.02s in queue
2025-04-05 17:40:56.991971 - Throttling request, waiting for 47.89 seconds
2025-04-05 17:40:56.991971 - Processing request after 0.02s in queue
2025-04-05 17:40:56.991971 - Throttling request, waiting for 47.89 seconds
2025-04-05 17:40:56.991971 - Processing request after 0.02s in queue
2025-04-05 17:40:56.991971 - Throttling request, waiting for 47.89 seconds
2025-04-05 17:40:56.991971 - Processing request after 0.02s in queue
2025-04-05 17:40:56.991971 - Throttling request, waiting for 47.89 seconds
2025-04-05 17:40:57.001235 - Text query received: Hello, world!...
2025-04-05 17:40:57.001235 - Text query received: Hello, world!...
2025-04-05 17:40:57.003454 - Text query received: Hello, world!...
2025-04-05 17:40:57.006594 - Text query received: Hello, world!...
2025-04-05 17:40:57.006594 - Processing request after 0.01s in queue
2025-04-05 17:40:57.006594 - Throttling request, waiting for 47.87 seconds
2025-04-05 17:40:57.006594 - Processing request after 0.01s in queue
2025-04-05 17:40:57.009558 - Throttling request, waiting for 47.87 seconds
2025-04-05 17:40:57.009558 - Processing request after 0.00s in queue
2025-04-05 17:40:57.009558 - Throttling request, waiting for 47.87 seconds
2025-04-05 17:40:57.009558 - Processing request after 0.00s in queue
2025-04-05 17:40:57.009558 - Throttling request, waiting for 47.87 seconds
2025-04-05 17:41:45.649218 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:41:45.649218 - Throttling request, waiting for 1.36 seconds
2025-04-05 17:41:45.651235 - Throttling request, waiting for 1.36 seconds
2025-04-05 17:41:45.651235 - Throttling request, waiting for 1.36 seconds
2025-04-05 17:41:45.651235 - Throttling request, waiting for 1.36 seconds
2025-04-05 17:41:45.651235 - Throttling request, waiting for 1.36 seconds
2025-04-05 17:41:45.651235 - Throttling request, waiting for 1.36 seconds
2025-04-05 17:41:45.653249 - Throttling request, waiting for 1.35 seconds
2025-04-05 17:41:45.653249 - Throttling request, waiting for 1.35 seconds
2025-04-05 17:41:45.653249 - Throttling request, waiting for 1.35 seconds
2025-04-05 17:41:45.655261 - Throttling request, waiting for 1.35 seconds
2025-04-05 17:41:45.655261 - Throttling request, waiting for 1.35 seconds
2025-04-05 17:41:45.655261 - Throttling request, waiting for 1.35 seconds
2025-04-05 17:41:45.655261 - Throttling request, waiting for 1.35 seconds
2025-04-05 17:41:45.655261 - Throttling request, waiting for 1.35 seconds
2025-04-05 17:41:45.661141 - Text query received: Hello, world!...
2025-04-05 17:41:45.661141 - Processing request after 0.00s in queue
2025-04-05 17:41:45.661141 - Throttling request, waiting for 1.35 seconds
2025-04-05 17:41:47.753000 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:41:48.492135 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:41:49.344587 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:41:50.058648 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:41:50.797952 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:41:51.586350 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-05 17:41:52.416658 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-05 17:41:53.152381 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:41:54.071672 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:41:54.874834 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:41:55.593545 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:41:56.292717 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:41:57.036767 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:41:57.808592 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:41:57.810605 - Throttling request, waiting for 48.07 seconds
2025-04-05 17:41:57.835209 - Text query received: Hello, world!...
2025-04-05 17:41:57.837217 - Text query received: Hello, world!...
2025-04-05 17:41:57.837217 - Processing request after 0.00s in queue
2025-04-05 17:41:57.837217 - Throttling request, waiting for 48.04 seconds
2025-04-05 17:41:57.838958 - Text query received: Hello, world!...
2025-04-05 17:41:57.838958 - Text query received: Hello, world!...
2025-04-05 17:41:57.838958 - Text query received: Hello, world!...
2025-04-05 17:41:57.838958 - Text query received: Hello, world!...
2025-04-05 17:41:57.838958 - Text query received: Hello, world!...
2025-04-05 17:41:57.838958 - Text query received: Hello, world!...
2025-04-05 17:41:57.841843 - Text query received: Hello, world!...
2025-04-05 17:41:57.841843 - Text query received: Hello, world!...
2025-04-05 17:41:57.841843 - Text query received: Hello, world!...
2025-04-05 17:41:57.841843 - Text query received: Hello, world!...
2025-04-05 17:41:57.841843 - Text query received: Hello, world!...
2025-04-05 17:41:57.841843 - Text query received: Hello, world!...
2025-04-05 17:42:46.692805 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-05 17:42:46.695230 - Throttling request, waiting for 1.31 seconds
2025-04-05 17:42:46.695230 - Processing request after 48.86s in queue
2025-04-05 17:42:46.695230 - Throttling request, waiting for 1.31 seconds
2025-04-05 17:42:46.696985 - Processing request after 48.86s in queue
2025-04-05 17:42:46.698360 - Throttling request, waiting for 1.30 seconds
2025-04-05 17:42:46.698360 - Processing request after 48.86s in queue
2025-04-05 17:42:46.698360 - Throttling request, waiting for 1.30 seconds
2025-04-05 17:42:46.698360 - Processing request after 48.86s in queue
2025-04-05 17:42:46.698360 - Throttling request, waiting for 1.30 seconds
2025-04-05 17:42:46.698360 - Processing request after 48.86s in queue
2025-04-05 17:42:46.698360 - Throttling request, waiting for 1.30 seconds
2025-04-05 17:42:48.759544 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-05 17:42:49.553079 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:42:50.377925 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-05 17:42:51.398346 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:42:52.209520 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:42:53.039376 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:42:53.039376 - Processing request after 55.20s in queue
2025-04-05 17:42:53.853150 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  Back at you!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 10,
        "total_token_count": 14
      }
    }),
)
2025-04-05 17:42:53.855562 - Processing request after 56.02s in queue
2025-04-05 17:42:54.667908 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:42:54.668621 - Processing request after 56.83s in queue
2025-04-05 17:42:55.386227 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:42:55.387343 - Processing request after 57.55s in queue
2025-04-05 17:42:56.086857 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:42:56.086857 - Processing request after 58.25s in queue
2025-04-05 17:42:56.807368 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:42:56.807368 - Processing request after 58.97s in queue
2025-04-05 17:42:57.552264 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:42:57.554521 - Processing request after 59.71s in queue
2025-04-05 17:42:58.250953 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-05 17:42:58.250953 - Processing request after 60.40s in queue
2025-04-05 17:42:59.179124 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:53:40.491932 - Text query received: Hello, world!...
2025-04-09 17:53:40.491932 - Text query received: Hello, world!...
2025-04-09 17:53:40.497176 - Processing request after 0.01s in queue
2025-04-09 17:53:41.912472 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:53:41.912472 - Text query received: Hello, world!...
2025-04-09 17:53:41.912472 - Text query received: Hello, world!...
2025-04-09 17:53:41.916817 - Text query received: Hello, world!...
2025-04-09 17:53:41.916817 - Text query received: Hello, world!...
2025-04-09 17:53:41.918829 - Text query received: Hello, world!...
2025-04-09 17:53:41.918829 - Text query received: Hello, world!...
2025-04-09 17:53:41.918829 - Text query received: Hello, world!...
2025-04-09 17:53:41.918829 - Text query received: Hello, world!...
2025-04-09 17:53:41.923629 - Text query received: Hello, world!...
2025-04-09 17:53:41.926353 - Processing request after 1.43s in queue
2025-04-09 17:53:42.513226 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:53:42.513226 - Processing request after 0.60s in queue
2025-04-09 17:53:43.168336 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  Back at you!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 10,
        "total_token_count": 14
      }
    }),
)
2025-04-09 17:53:43.168336 - Processing request after 1.25s in queue
2025-04-09 17:53:43.755512 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 17:53:43.757527 - Processing request after 1.84s in queue
2025-04-09 17:53:44.359743 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:53:44.362995 - Processing request after 2.44s in queue
2025-04-09 17:53:44.855030 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:53:44.855030 - Text query received: Hello, world!...
2025-04-09 17:53:44.855030 - Text query received: Hello, world!...
2025-04-09 17:53:44.855030 - Text query received: Hello, world!...
2025-04-09 17:53:44.866047 - Processing request after 2.95s in queue
2025-04-09 17:53:45.410263 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:53:45.410263 - Processing request after 3.49s in queue
2025-04-09 17:53:45.993113 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-09 17:53:45.993113 - Processing request after 4.07s in queue
2025-04-09 17:53:46.459502 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:53:46.467053 - Processing request after 4.55s in queue
2025-04-09 17:53:47.223046 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:53:47.223046 - Processing request after 5.30s in queue
2025-04-09 17:53:47.843185 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 17:53:47.843185 - Text query received: Hello, world!...
2025-04-09 17:53:47.853645 - Text query received: Hello, world!...
2025-04-09 17:53:47.853645 - Text query received: Hello, world!...
2025-04-09 17:53:47.853645 - Text query received: Hello, world!...
2025-04-09 17:53:47.853645 - Text query received: Hello, world!...
2025-04-09 17:53:47.853645 - Text query received: Hello, world!...
2025-04-09 17:53:47.853645 - Text query received: Hello, world!...
2025-04-09 17:53:47.853645 - Text query received: Hello, world!...
2025-04-09 17:53:47.853645 - Processing request after 3.00s in queue
2025-04-09 17:53:48.338894 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:53:48.338894 - Processing request after 3.48s in queue
2025-04-09 17:53:48.831661 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:53:48.831661 - Processing request after 3.98s in queue
2025-04-09 17:53:49.322883 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:53:49.322883 - Processing request after 1.48s in queue
2025-04-09 17:53:49.794255 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:53:49.795520 - Processing request after 1.94s in queue
2025-04-09 17:53:49.796080 - Throttling request, waiting for 51.70 seconds
2025-04-09 17:53:49.796622 - Text query received: Hello, world!...
2025-04-09 17:53:49.800138 - Text query received: Hello, world!...
2025-04-09 17:53:49.801758 - Text query received: Hello, world!...
2025-04-09 17:53:49.802343 - Text query received: Hello, world!...
2025-04-09 17:53:49.802874 - Text query received: Hello, world!...
2025-04-09 17:53:49.803401 - Text query received: Hello, world!...
2025-04-09 17:53:49.803401 - Text query received: Hello, world!...
2025-04-09 17:53:49.803981 - Text query received: Hello, world!...
2025-04-09 17:53:49.805086 - Text query received: Hello, world!...
2025-04-09 17:53:49.805790 - Text query received: Hello, world!...
2025-04-09 17:53:49.807335 - Text query received: Hello, world!...
2025-04-09 17:53:49.807872 - Text query received: Hello, world!...
2025-04-09 17:53:49.807872 - Text query received: Hello, world!...
2025-04-09 17:53:49.808934 - Text query received: Hello, world!...
2025-04-09 17:53:49.809565 - Text query received: Hello, world!...
2025-04-09 17:53:49.810129 - Text query received: Hello, world!...
2025-04-09 17:53:49.810660 - Text query received: Hello, world!...
2025-04-09 17:53:49.811287 - Text query received: Hello, world!...
2025-04-09 17:53:49.811820 - Text query received: Hello, world!...
2025-04-09 17:53:49.812861 - Text query received: Hello, world!...
2025-04-09 17:53:49.813389 - Text query received: Hello, world!...
2025-04-09 17:53:49.813928 - Text query received: Hello, world!...
2025-04-09 17:53:49.815025 - Text query received: Hello, world!...
2025-04-09 17:53:49.817043 - Text query received: Hello, world!...
2025-04-09 17:53:49.817043 - Text query received: Hello, world!...
2025-04-09 17:53:49.819052 - Text query received: Hello, world!...
2025-04-09 17:53:49.820063 - Text query received: Hello, world!...
2025-04-09 17:53:49.822074 - Text query received: Hello, world!...
2025-04-09 17:53:49.822074 - Text query received: Hello, world!...
2025-04-09 17:53:49.824085 - Text query received: Hello, world!...
2025-04-09 17:53:49.824085 - Text query received: Hello, world!...
2025-04-09 17:53:49.826095 - Text query received: Hello, world!...
2025-04-09 17:53:49.826095 - Text query received: Hello, world!...
2025-04-09 17:53:49.835827 - Text query received: Hello, world!...
2025-04-09 17:53:49.838355 - Text query received: Hello, world!...
2025-04-09 17:53:49.840735 - Text query received: Hello, world!...
2025-04-09 17:53:49.842741 - Text query received: Hello, world!...
2025-04-09 17:53:49.844479 - Text query received: Hello, world!...
2025-04-09 17:53:49.846487 - Text query received: Hello, world!...
2025-04-09 17:53:49.846487 - Text query received: Hello, world!...
2025-04-09 17:53:49.848496 - Text query received: Hello, world!...
2025-04-09 17:53:49.848496 - Text query received: Hello, world!...
2025-04-09 17:53:49.850509 - Text query received: Hello, world!...
2025-04-09 17:54:41.966309 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:54:41.970347 - Processing request after 54.12s in queue
2025-04-09 17:54:42.473199 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:54:42.473199 - Processing request after 54.62s in queue
2025-04-09 17:54:42.477682 - Throttling request, waiting for 1.04 seconds
2025-04-09 17:54:42.477682 - Processing request after 54.62s in queue
2025-04-09 17:54:42.479198 - Throttling request, waiting for 1.04 seconds
2025-04-09 17:54:42.479198 - Processing request after 54.63s in queue
2025-04-09 17:54:42.479198 - Throttling request, waiting for 1.03 seconds
2025-04-09 17:54:42.479198 - Processing request after 54.63s in queue
2025-04-09 17:54:42.479198 - Throttling request, waiting for 1.03 seconds
2025-04-09 17:54:42.479198 - Text query received: Hello, world!...
2025-04-09 17:54:42.485506 - Text query received: Hello, world!...
2025-04-09 17:54:44.175143 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-09 17:54:44.646530 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:54:45.187341 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:54:45.803227 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 17:54:45.808467 - Processing request after 57.95s in queue
2025-04-09 17:54:46.421896 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:54:46.421896 - Processing request after 56.63s in queue
2025-04-09 17:54:47.035406 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:54:47.035406 - Processing request after 57.24s in queue
2025-04-09 17:54:47.460146 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 17:54:47.461739 - Processing request after 57.66s in queue
2025-04-09 17:54:48.056628 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:54:48.056628 - Processing request after 58.25s in queue
2025-04-09 17:54:48.674312 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:54:48.681454 - Text query received: Hello, world!...
2025-04-09 17:54:48.681454 - Text query received: Hello, world!...
2025-04-09 17:54:48.682681 - Text query received: Hello, world!...
2025-04-09 17:54:48.684543 - Text query received: Hello, world!...
2025-04-09 17:54:48.684543 - Processing request after 58.88s in queue
2025-04-09 17:54:49.221541 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 17:54:49.226811 - Processing request after 59.42s in queue
2025-04-09 17:54:49.766529 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:54:49.767060 - Processing request after 59.96s in queue
2025-04-09 17:54:50.217441 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:54:50.217441 - Processing request after 60.41s in queue
2025-04-09 17:54:50.732528 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 17:54:50.733046 - Processing request after 60.93s in queue
2025-04-09 17:54:50.733663 - Throttling request, waiting for 51.77 seconds
2025-04-09 17:54:50.737085 - Text query received: Hello, world!...
2025-04-09 17:54:50.737665 - Text query received: Hello, world!...
2025-04-09 17:54:50.738728 - Text query received: Hello, world!...
2025-04-09 17:54:50.738728 - Text query received: Hello, world!...
2025-04-09 17:54:50.739748 - Text query received: Hello, world!...
2025-04-09 17:54:50.750601 - Text query received: Hello, world!...
2025-04-09 17:54:50.753122 - Text query received: Hello, world!...
2025-04-09 17:54:50.753122 - Text query received: Hello, world!...
2025-04-09 17:54:50.753122 - Text query received: Hello, world!...
2025-04-09 17:55:43.139318 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:55:43.142621 - Processing request after 113.34s in queue
2025-04-09 17:55:43.762929 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:55:43.764944 - Processing request after 113.96s in queue
2025-04-09 17:55:44.372632 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:55:44.376514 - Processing request after 114.57s in queue
2025-04-09 17:55:45.260725 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 17:55:45.260725 - Processing request after 115.45s in queue
2025-04-09 17:55:45.811525 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:55:45.811525 - Processing request after 116.00s in queue
2025-04-09 17:55:46.405689 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:55:46.422357 - Text query received: Hello, world!...
2025-04-09 17:55:46.424369 - Processing request after 116.61s in queue
2025-04-09 17:55:46.882983 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:55:46.883532 - Processing request after 117.07s in queue
2025-04-09 17:55:47.398545 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 17:55:47.398545 - Processing request after 117.59s in queue
2025-04-09 17:55:48.063465 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:55:48.066870 - Processing request after 118.26s in queue
2025-04-09 17:55:48.685272 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 17:55:48.687285 - Processing request after 118.87s in queue
2025-04-09 17:55:49.161348 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:55:49.165365 - Text query received: Hello, world!...
2025-04-09 17:55:49.167372 - Text query received: Hello, world!...
2025-04-09 17:55:49.167372 - Text query received: Hello, world!...
2025-04-09 17:55:49.167372 - Text query received: Hello, world!...
2025-04-09 17:55:49.168481 - Text query received: Hello, world!...
2025-04-09 17:55:49.168481 - Processing request after 119.36s in queue
2025-04-09 17:55:49.607650 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:55:49.608722 - Processing request after 119.80s in queue
2025-04-09 17:55:50.148483 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 17:55:50.161479 - Processing request after 120.35s in queue
2025-04-09 17:55:50.640054 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:55:50.640054 - Processing request after 120.83s in queue
2025-04-09 17:55:51.888899 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:55:51.889837 - Processing request after 122.07s in queue
2025-04-09 17:55:51.889837 - Throttling request, waiting for 51.63 seconds
2025-04-09 17:55:51.893223 - Text query received: Hello, world!...
2025-04-09 17:55:51.893855 - Text query received: Hello, world!...
2025-04-09 17:55:51.894872 - Text query received: Hello, world!...
2025-04-09 17:55:51.895402 - Text query received: Hello, world!...
2025-04-09 17:55:51.896170 - Text query received: Hello, world!...
2025-04-09 17:55:51.896690 - Text query received: Hello, world!...
2025-04-09 17:55:51.902328 - Text query received: Hello, world!...
2025-04-09 17:55:51.902994 - Text query received: Hello, world!...
2025-04-09 17:55:51.903513 - Text query received: Hello, world!...
2025-04-09 17:56:44.063738 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (Back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 17:56:44.068924 - Processing request after 174.25s in queue
2025-04-09 17:56:44.530357 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:44.530357 - Processing request after 174.71s in queue
2025-04-09 17:56:45.061251 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:45.061251 - Processing request after 175.24s in queue
2025-04-09 17:56:45.550010 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:45.550010 - Processing request after 175.73s in queue
2025-04-09 17:56:46.024331 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:46.025452 - Processing request after 176.20s in queue
2025-04-09 17:56:46.487903 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:46.503894 - Text query received: Hello, world!...
2025-04-09 17:56:46.508734 - Processing request after 176.68s in queue
2025-04-09 17:56:47.049702 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:47.049702 - Processing request after 177.23s in queue
2025-04-09 17:56:47.662563 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:47.662563 - Processing request after 177.84s in queue
2025-04-09 17:56:48.268284 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:48.268284 - Processing request after 178.44s in queue
2025-04-09 17:56:48.777394 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:48.779411 - Processing request after 178.94s in queue
2025-04-09 17:56:49.304419 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:49.315342 - Text query received: Hello, world!...
2025-04-09 17:56:49.317272 - Text query received: Hello, world!...
2025-04-09 17:56:49.318893 - Text query received: Hello, world!...
2025-04-09 17:56:49.318893 - Text query received: Hello, world!...
2025-04-09 17:56:49.319715 - Text query received: Hello, world!...
2025-04-09 17:56:49.319715 - Processing request after 179.48s in queue
2025-04-09 17:56:49.809295 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:49.809823 - Processing request after 179.97s in queue
2025-04-09 17:56:50.314568 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:50.330767 - Processing request after 180.49s in queue
2025-04-09 17:56:50.817556 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:50.827162 - Processing request after 180.98s in queue
2025-04-09 17:56:51.326557 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:56:51.343833 - Processing request after 181.50s in queue
2025-04-09 17:56:51.343833 - Throttling request, waiting for 53.20 seconds
2025-04-09 17:56:51.361511 - Text query received: Hello, world!...
2025-04-09 17:56:51.361511 - Text query received: Hello, world!...
2025-04-09 17:56:51.361511 - Text query received: Hello, world!...
2025-04-09 17:56:51.361511 - Text query received: Hello, world!...
2025-04-09 17:56:51.361511 - Text query received: Hello, world!...
2025-04-09 17:56:51.363523 - Text query received: Hello, world!...
2025-04-09 17:56:51.363523 - Text query received: Hello, world!...
2025-04-09 17:56:51.366997 - Text query received: Hello, world!...
2025-04-09 17:56:51.366997 - Text query received: Hello, world!...
2025-04-09 17:57:45.205366 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:57:45.209479 - Processing request after 235.36s in queue
2025-04-09 17:57:45.699248 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:57:45.700596 - Processing request after 235.85s in queue
2025-04-09 17:57:46.231501 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:57:46.231936 - Processing request after 236.38s in queue
2025-04-09 17:57:46.850378 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:57:46.852391 - Processing request after 237.00s in queue
2025-04-09 17:57:47.454914 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!  How can I help you today?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 12,
        "total_token_count": 16
      }
    }),
)
2025-04-09 17:57:47.454914 - Processing request after 184.98s in queue
2025-04-09 17:57:47.943405 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:57:47.945053 - Text query received: Hello, world!...
2025-04-09 17:57:47.945053 - Processing request after 185.46s in queue
2025-04-09 17:57:48.430544 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:57:48.434460 - Processing request after 179.75s in queue
2025-04-09 17:57:49.119877 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 17:57:49.119877 - Processing request after 180.44s in queue
2025-04-09 17:57:49.715888 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:57:49.716966 - Processing request after 181.03s in queue
2025-04-09 17:57:50.207299 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:57:50.209317 - Processing request after 181.52s in queue
2025-04-09 17:57:50.686020 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:57:50.699011 - Text query received: Hello, world!...
2025-04-09 17:57:50.699011 - Text query received: Hello, world!...
2025-04-09 17:57:50.699011 - Text query received: Hello, world!...
2025-04-09 17:57:50.700018 - Text query received: Hello, world!...
2025-04-09 17:57:50.700018 - Text query received: Hello, world!...
2025-04-09 17:57:50.700018 - Processing request after 179.96s in queue
2025-04-09 17:57:51.345140 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 17:57:51.348940 - Processing request after 180.61s in queue
2025-04-09 17:57:51.966630 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:57:51.968470 - Processing request after 181.23s in queue
2025-04-09 17:57:52.475727 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  Back at you!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 10,
        "total_token_count": 14
      }
    }),
)
2025-04-09 17:57:52.487551 - Processing request after 181.75s in queue
2025-04-09 17:57:52.941779 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:57:52.942287 - Processing request after 182.20s in queue
2025-04-09 17:57:52.943700 - Throttling request, waiting for 52.61 seconds
2025-04-09 17:57:52.948206 - Text query received: Hello, world!...
2025-04-09 17:57:52.949402 - Text query received: Hello, world!...
2025-04-09 17:57:52.949948 - Text query received: Hello, world!...
2025-04-09 17:57:52.950455 - Text query received: Hello, world!...
2025-04-09 17:57:52.951185 - Text query received: Hello, world!...
2025-04-09 17:57:52.952269 - Text query received: Hello, world!...
2025-04-09 17:57:52.956353 - Text query received: Hello, world!...
2025-04-09 17:57:52.956876 - Text query received: Hello, world!...
2025-04-09 17:57:52.958474 - Text query received: Hello, world!...
2025-04-09 17:58:46.021932 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:58:46.023208 - Processing request after 235.27s in queue
2025-04-09 17:58:46.644387 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 17:58:46.646403 - Processing request after 235.89s in queue
2025-04-09 17:58:47.260107 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:58:47.261300 - Processing request after 236.51s in queue
2025-04-09 17:58:47.883558 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:58:47.883558 - Processing request after 237.13s in queue
2025-04-09 17:58:48.486969 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:58:48.486969 - Processing request after 182.06s in queue
2025-04-09 17:58:48.962970 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:58:48.962970 - Processing request after 179.80s in queue
2025-04-09 17:58:49.517821 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:58:49.517821 - Processing request after 180.35s in queue
2025-04-09 17:58:49.992772 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:58:49.993324 - Processing request after 180.83s in queue
2025-04-09 17:58:50.460383 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:58:50.460383 - Processing request after 181.29s in queue
2025-04-09 17:58:50.906276 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 17:58:50.906276 - Processing request after 181.74s in queue
2025-04-09 17:58:51.448491 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 17:58:51.448491 - Processing request after 179.55s in queue
2025-04-09 17:58:52.185302 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-09 17:58:52.185302 - Processing request after 180.29s in queue
2025-04-09 17:58:52.728378 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:58:52.728378 - Processing request after 180.83s in queue
2025-04-09 17:58:53.265848 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 17:58:53.265848 - Processing request after 181.37s in queue
2025-04-09 17:58:53.753855 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:58:53.755008 - Processing request after 181.86s in queue
2025-04-09 17:58:53.756125 - Throttling request, waiting for 52.80 seconds
2025-04-09 17:59:24.827254 - Text query received: Hello, world!...
2025-04-09 17:59:24.827254 - Text query received: Hello, world!...
2025-04-09 17:59:24.827254 - Text query received: Hello, world!...
2025-04-09 17:59:24.827254 - Text query received: Hello, world!...
2025-04-09 17:59:24.827254 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.876679 - Text query received: Hello, world!...
2025-04-09 17:59:24.889096 - Text query received: Hello, world!...
2025-04-09 17:59:24.889096 - Text query received: Hello, world!...
2025-04-09 17:59:24.891106 - Text query received: Hello, world!...
2025-04-09 17:59:24.891106 - Text query received: Hello, world!...
2025-04-09 17:59:24.891106 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.892629 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.908605 - Text query received: Hello, world!...
2025-04-09 17:59:24.924266 - Text query received: Hello, world!...
2025-04-09 17:59:24.924266 - Text query received: Hello, world!...
2025-04-09 17:59:24.924266 - Text query received: Hello, world!...
2025-04-09 17:59:24.924266 - Text query received: Hello, world!...
2025-04-09 17:59:24.924266 - Text query received: Hello, world!...
2025-04-09 17:59:24.924266 - Text query received: Hello, world!...
2025-04-09 17:59:24.924266 - Text query received: Hello, world!...
2025-04-09 17:59:24.924266 - Text query received: Hello, world!...
2025-04-09 17:59:24.924266 - Text query received: Hello, world!...
2025-04-09 17:59:24.929959 - Text query received: Hello, world!...
2025-04-09 17:59:24.929959 - Text query received: Hello, world!...
2025-04-09 17:59:24.929959 - Text query received: Hello, world!...
2025-04-09 17:59:24.929959 - Text query received: Hello, world!...
2025-04-09 17:59:24.931967 - Text query received: Hello, world!...
2025-04-09 17:59:24.931967 - Text query received: Hello, world!...
2025-04-09 17:59:24.931967 - Text query received: Hello, world!...
2025-04-09 17:59:24.931967 - Text query received: Hello, world!...
2025-04-09 17:59:24.933974 - Text query received: Hello, world!...
2025-04-09 17:59:24.933974 - Text query received: Hello, world!...
2025-04-09 17:59:24.933974 - Text query received: Hello, world!...
2025-04-09 17:59:47.272563 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:59:47.276590 - Processing request after 235.38s in queue
2025-04-09 17:59:47.891432 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:59:47.892510 - Processing request after 235.99s in queue
2025-04-09 17:59:48.422700 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-09 17:59:48.424911 - Processing request after 236.52s in queue
2025-04-09 17:59:49.111854 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:59:49.111854 - Processing request after 237.21s in queue
2025-04-09 17:59:49.731693 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:59:49.732754 - Processing request after 183.22s in queue
2025-04-09 17:59:50.282751 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at ya!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 17:59:50.288189 - Processing request after 180.97s in queue
2025-04-09 17:59:50.774789 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:59:50.776808 - Processing request after 181.46s in queue
2025-04-09 17:59:51.246878 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:59:51.248410 - Processing request after 181.93s in queue
2025-04-09 17:59:51.706564 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:59:51.706564 - Processing request after 182.39s in queue
2025-04-09 17:59:52.332322 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 17:59:52.332322 - Processing request after 183.01s in queue
2025-04-09 17:59:52.842558 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-09 17:59:52.857047 - Processing request after 181.50s in queue
2025-04-09 17:59:53.344765 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:59:53.344765 - Processing request after 181.98s in queue
2025-04-09 17:59:53.796353 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:59:53.796353 - Processing request after 182.43s in queue
2025-04-09 17:59:54.446315 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:59:54.446315 - Processing request after 183.08s in queue
2025-04-09 17:59:55.049036 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 17:59:55.051198 - Processing request after 183.69s in queue
2025-04-09 17:59:55.051198 - Throttling request, waiting for 52.52 seconds
2025-04-09 18:00:48.113523 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:00:48.113523 - Processing request after 236.75s in queue
2025-04-09 18:00:48.719800 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:00:48.721818 - Processing request after 237.36s in queue
2025-04-09 18:00:49.332610 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:00:49.332610 - Processing request after 237.97s in queue
2025-04-09 18:00:49.781826 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:00:49.781826 - Processing request after 238.41s in queue
2025-04-09 18:00:50.353593 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:00:50.353593 - Processing request after 182.41s in queue
2025-04-09 18:00:50.806598 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:00:50.821537 - Processing request after 180.12s in queue
2025-04-09 18:00:51.327167 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:00:51.327167 - Processing request after 180.63s in queue
2025-04-09 18:00:51.797809 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:00:51.801289 - Processing request after 181.10s in queue
2025-04-09 18:00:52.250866 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:00:52.250866 - Processing request after 181.55s in queue
2025-04-09 18:00:52.809785 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:00:52.809785 - Processing request after 182.11s in queue
2025-04-09 18:00:53.321659 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:00:53.322706 - Processing request after 180.37s in queue
2025-04-09 18:00:53.788194 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:00:53.788194 - Processing request after 180.84s in queue
2025-04-09 18:00:54.286031 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:00:54.289508 - Processing request after 181.34s in queue
2025-04-09 18:00:54.732650 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:00:54.732650 - Processing request after 181.78s in queue
2025-04-09 18:00:55.464268 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:00:55.464268 - Processing request after 182.51s in queue
2025-04-09 18:00:55.464268 - Throttling request, waiting for 53.11 seconds
2025-04-09 18:01:49.129738 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:01:49.130824 - Processing request after 236.18s in queue
2025-04-09 18:01:49.738004 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:01:49.740126 - Processing request after 236.78s in queue
2025-04-09 18:01:50.363610 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:01:50.364511 - Processing request after 237.41s in queue
2025-04-09 18:01:50.835464 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:01:50.851183 - Processing request after 237.89s in queue
2025-04-09 18:01:51.371886 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at ya!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:01:51.372953 - Processing request after 146.55s in queue
2025-04-09 18:01:51.927524 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:01:51.946183 - Processing request after 147.12s in queue
2025-04-09 18:01:52.614380 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:01:52.614380 - Processing request after 147.79s in queue
2025-04-09 18:01:53.072307 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:01:53.072307 - Processing request after 148.25s in queue
2025-04-09 18:01:53.557697 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:01:53.557697 - Processing request after 148.73s in queue
2025-04-09 18:01:54.037765 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:01:54.037765 - Processing request after 149.16s in queue
2025-04-09 18:01:54.537832 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:01:54.537832 - Processing request after 149.66s in queue
2025-04-09 18:01:54.990410 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:01:54.994432 - Processing request after 150.12s in queue
2025-04-09 18:01:55.690724 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at ya!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:01:55.690724 - Processing request after 150.81s in queue
2025-04-09 18:01:56.296332 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:01:56.296332 - Processing request after 151.42s in queue
2025-04-09 18:01:56.918688 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:01:56.918688 - Processing request after 152.04s in queue
2025-04-09 18:01:56.918688 - Throttling request, waiting for 52.66 seconds
2025-04-09 18:02:24.893167 - Text query received: Hello, world!...
2025-04-09 18:02:24.894636 - Text query received: Hello, world!...
2025-04-09 18:02:24.894636 - Processing request after 0.00s in queue
2025-04-09 18:02:26.195209 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:02:26.197229 - Text query received: Hello, world!...
2025-04-09 18:02:26.197229 - Text query received: Hello, world!...
2025-04-09 18:02:26.197229 - Text query received: Hello, world!...
2025-04-09 18:02:26.197229 - Text query received: Hello, world!...
2025-04-09 18:02:26.197229 - Text query received: Hello, world!...
2025-04-09 18:02:26.197229 - Text query received: Hello, world!...
2025-04-09 18:02:26.197229 - Text query received: Hello, world!...
2025-04-09 18:02:26.197229 - Text query received: Hello, world!...
2025-04-09 18:02:26.208648 - Processing request after 1.31s in queue
2025-04-09 18:02:26.371708 - Error: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 33
}
]
2025-04-09 18:02:26.387534 - Rate limit detected. Backing off for 62.82 seconds
2025-04-09 18:02:26.387534 - Processing request after 0.19s in queue
2025-04-09 18:02:26.387534 - Throttling request, waiting for 62.82 seconds
2025-04-09 18:02:26.387534 - Processing request after 0.19s in queue
2025-04-09 18:02:26.387534 - Throttling request, waiting for 62.82 seconds
2025-04-09 18:02:26.387534 - Processing request after 0.19s in queue
2025-04-09 18:02:26.387534 - Throttling request, waiting for 62.82 seconds
2025-04-09 18:02:26.387534 - Processing request after 0.19s in queue
2025-04-09 18:02:26.387534 - Throttling request, waiting for 62.82 seconds
2025-04-09 18:02:26.387534 - Text query received: Hello, world!...
2025-04-09 18:02:26.387534 - Text query received: Hello, world!...
2025-04-09 18:02:26.387534 - Text query received: Hello, world!...
2025-04-09 18:02:26.387534 - Text query received: Hello, world!...
2025-04-09 18:02:26.387534 - Text query received: Hello, world!...
2025-04-09 18:02:26.387534 - Text query received: Hello, world!...
2025-04-09 18:02:26.387534 - Text query received: Hello, world!...
2025-04-09 18:02:26.387534 - Text query received: Hello, world!...
2025-04-09 18:02:26.387534 - Text query received: Hello, world!...
2025-04-09 18:02:26.387534 - Text query received: Hello, world!...
2025-04-09 18:02:26.387534 - Text query received: Hello, world!...
2025-04-09 18:02:26.400305 - Text query received: Hello, world!...
2025-04-09 18:02:26.400305 - Text query received: Hello, world!...
2025-04-09 18:02:26.400305 - Text query received: Hello, world!...
2025-04-09 18:02:26.400305 - Text query received: Hello, world!...
2025-04-09 18:02:26.403298 - Text query received: Hello, world!...
2025-04-09 18:02:26.404080 - Text query received: Hello, world!...
2025-04-09 18:02:26.404080 - Text query received: Hello, world!...
2025-04-09 18:02:26.404080 - Text query received: Hello, world!...
2025-04-09 18:02:26.404080 - Text query received: Hello, world!...
2025-04-09 18:02:26.406404 - Text query received: Hello, world!...
2025-04-09 18:02:26.406404 - Text query received: Hello, world!...
2025-04-09 18:02:26.406404 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.408633 - Text query received: Hello, world!...
2025-04-09 18:02:26.419014 - Text query received: Hello, world!...
2025-04-09 18:02:26.419014 - Text query received: Hello, world!...
2025-04-09 18:02:26.419014 - Text query received: Hello, world!...
2025-04-09 18:02:26.419014 - Text query received: Hello, world!...
2025-04-09 18:02:26.419014 - Text query received: Hello, world!...
2025-04-09 18:02:26.419014 - Text query received: Hello, world!...
2025-04-09 18:02:26.419014 - Text query received: Hello, world!...
2025-04-09 18:02:26.425371 - Text query received: Hello, world!...
2025-04-09 18:02:26.425957 - Text query received: Hello, world!...
2025-04-09 18:02:26.426490 - Text query received: Hello, world!...
2025-04-09 18:02:26.427277 - Text query received: Hello, world!...
2025-04-09 18:02:26.427802 - Text query received: Hello, world!...
2025-04-09 18:02:26.428320 - Text query received: Hello, world!...
2025-04-09 18:02:26.428950 - Text query received: Hello, world!...
2025-04-09 18:02:26.429537 - Text query received: Hello, world!...
2025-04-09 18:02:26.430128 - Text query received: Hello, world!...
2025-04-09 18:02:26.430715 - Text query received: Hello, world!...
2025-04-09 18:02:26.431825 - Text query received: Hello, world!...
2025-04-09 18:02:26.432365 - Text query received: Hello, world!...
2025-04-09 18:02:26.432892 - Text query received: Hello, world!...
2025-04-09 18:02:26.434206 - Text query received: Hello, world!...
2025-04-09 18:02:26.434924 - Text query received: Hello, world!...
2025-04-09 18:02:26.435446 - Text query received: Hello, world!...
2025-04-09 18:02:26.436667 - Text query received: Hello, world!...
2025-04-09 18:02:26.436667 - Text query received: Hello, world!...
2025-04-09 18:02:26.436667 - Text query received: Hello, world!...
2025-04-09 18:02:26.436667 - Text query received: Hello, world!...
2025-04-09 18:02:26.436667 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.466139 - Text query received: Hello, world!...
2025-04-09 18:02:26.482140 - Text query received: Hello, world!...
2025-04-09 18:03:29.201883 - Throttling request, waiting for 0.00 seconds
2025-04-09 18:03:29.201883 - Throttling request, waiting for 0.00 seconds
2025-04-09 18:03:29.201883 - Throttling request, waiting for 0.00 seconds
2025-04-09 18:03:29.201883 - Throttling request, waiting for 0.00 seconds
2025-04-09 18:03:29.201883 - Throttling request, waiting for 0.00 seconds
2025-04-09 18:03:29.689355 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:03:30.229010 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:03:30.917425 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:03:31.521206 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:03:32.141964 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:03:32.141964 - Processing request after 65.94s in queue
2025-04-09 18:03:32.960707 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:03:32.960707 - Processing request after 66.76s in queue
2025-04-09 18:03:33.570285 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:03:33.570285 - Processing request after 67.37s in queue
2025-04-09 18:03:34.189312 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:03:34.189312 - Processing request after 67.99s in queue
2025-04-09 18:03:34.807884 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:03:34.807884 - Processing request after 68.42s in queue
2025-04-09 18:03:35.427063 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 18:03:35.436478 - Text query received: Hello, world!...
2025-04-09 18:03:35.439563 - Text query received: Hello, world!...
2025-04-09 18:03:35.440868 - Text query received: Hello, world!...
2025-04-09 18:03:35.441818 - Text query received: Hello, world!...
2025-04-09 18:03:35.442324 - Text query received: Hello, world!...
2025-04-09 18:03:35.442914 - Processing request after 69.06s in queue
2025-04-09 18:03:35.931241 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:03:35.932781 - Processing request after 69.55s in queue
2025-04-09 18:03:36.450425 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:03:36.450994 - Processing request after 70.06s in queue
2025-04-09 18:03:36.927119 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:03:36.927119 - Processing request after 70.54s in queue
2025-04-09 18:03:37.420212 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:03:37.420212 - Processing request after 71.03s in queue
2025-04-09 18:03:38.090883 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:03:38.090883 - Text query received: Hello, world!...
2025-04-09 18:03:38.090883 - Text query received: Hello, world!...
2025-04-09 18:03:38.090883 - Text query received: Hello, world!...
2025-04-09 18:03:38.098328 - Text query received: Hello, world!...
2025-04-09 18:03:38.098328 - Text query received: Hello, world!...
2025-04-09 18:03:38.098328 - Processing request after 71.71s in queue
2025-04-09 18:03:38.098328 - Throttling request, waiting for 52.12 seconds
2025-04-09 18:03:38.098328 - Processing request after 71.71s in queue
2025-04-09 18:03:38.098328 - Throttling request, waiting for 52.12 seconds
2025-04-09 18:03:38.098328 - Processing request after 71.71s in queue
2025-04-09 18:03:38.098328 - Throttling request, waiting for 52.12 seconds
2025-04-09 18:03:38.098328 - Processing request after 71.71s in queue
2025-04-09 18:03:38.100336 - Throttling request, waiting for 52.12 seconds
2025-04-09 18:03:38.100336 - Processing request after 71.71s in queue
2025-04-09 18:03:38.100336 - Throttling request, waiting for 52.12 seconds
2025-04-09 18:03:38.100336 - Text query received: Hello, world!...
2025-04-09 18:03:38.101332 - Text query received: Hello, world!...
2025-04-09 18:03:38.103340 - Text query received: Hello, world!...
2025-04-09 18:03:38.103340 - Text query received: Hello, world!...
2025-04-09 18:03:38.103340 - Text query received: Hello, world!...
2025-04-09 18:04:30.776858 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 18:04:31.257630 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:04:31.710272 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:04:32.151459 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:04:32.758583 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:04:32.763266 - Processing request after 126.36s in queue
2025-04-09 18:04:33.279438 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:04:33.281450 - Processing request after 126.88s in queue
2025-04-09 18:04:33.790725 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:04:33.790725 - Processing request after 127.39s in queue
2025-04-09 18:04:34.323632 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:04:34.325649 - Processing request after 127.93s in queue
2025-04-09 18:04:35.023297 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:04:35.023297 - Processing request after 128.62s in queue
2025-04-09 18:04:35.629816 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:04:35.637543 - Text query received: Hello, world!...
2025-04-09 18:04:35.637543 - Text query received: Hello, world!...
2025-04-09 18:04:35.637543 - Text query received: Hello, world!...
2025-04-09 18:04:35.637543 - Text query received: Hello, world!...
2025-04-09 18:04:35.639554 - Text query received: Hello, world!...
2025-04-09 18:04:35.639554 - Processing request after 129.24s in queue
2025-04-09 18:04:36.242772 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:04:36.242772 - Processing request after 129.84s in queue
2025-04-09 18:04:36.696352 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:04:36.698373 - Processing request after 130.29s in queue
2025-04-09 18:04:37.193978 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:04:37.193978 - Processing request after 130.79s in queue
2025-04-09 18:04:37.880402 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!  How can I help you today?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 12,
        "total_token_count": 16
      }
    }),
)
2025-04-09 18:04:37.880402 - Processing request after 131.47s in queue
2025-04-09 18:04:38.508285 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:04:38.519920 - Text query received: Hello, world!...
2025-04-09 18:04:38.519920 - Text query received: Hello, world!...
2025-04-09 18:04:38.523338 - Text query received: Hello, world!...
2025-04-09 18:04:38.523338 - Text query received: Hello, world!...
2025-04-09 18:04:38.523338 - Text query received: Hello, world!...
2025-04-09 18:04:38.523338 - Processing request after 132.12s in queue
2025-04-09 18:04:38.526118 - Throttling request, waiting for 52.71 seconds
2025-04-09 18:04:38.526118 - Processing request after 132.12s in queue
2025-04-09 18:04:38.526118 - Throttling request, waiting for 52.70 seconds
2025-04-09 18:04:38.527114 - Processing request after 132.12s in queue
2025-04-09 18:04:38.527114 - Throttling request, waiting for 52.70 seconds
2025-04-09 18:04:38.527114 - Processing request after 132.12s in queue
2025-04-09 18:04:38.527114 - Throttling request, waiting for 52.70 seconds
2025-04-09 18:04:38.529750 - Processing request after 132.12s in queue
2025-04-09 18:04:38.530116 - Throttling request, waiting for 52.70 seconds
2025-04-09 18:04:38.530582 - Text query received: Hello, world!...
2025-04-09 18:04:38.530582 - Text query received: Hello, world!...
2025-04-09 18:04:38.533194 - Text query received: Hello, world!...
2025-04-09 18:04:38.533194 - Text query received: Hello, world!...
2025-04-09 18:04:38.533194 - Text query received: Hello, world!...
2025-04-09 18:05:31.750949 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:32.365788 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:32.971307 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:33.532335 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:34.204260 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:34.213884 - Processing request after 187.81s in queue
2025-04-09 18:05:34.815429 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:34.816589 - Processing request after 188.41s in queue
2025-04-09 18:05:35.332716 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:35.332716 - Processing request after 188.92s in queue
2025-04-09 18:05:36.046485 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 18:05:36.048499 - Processing request after 189.64s in queue
2025-04-09 18:05:36.689105 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:36.690848 - Processing request after 190.28s in queue
2025-04-09 18:05:37.176803 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:37.180208 - Text query received: Hello, world!...
2025-04-09 18:05:37.180208 - Text query received: Hello, world!...
2025-04-09 18:05:37.180208 - Text query received: Hello, world!...
2025-04-09 18:05:37.180208 - Text query received: Hello, world!...
2025-04-09 18:05:37.180208 - Text query received: Hello, world!...
2025-04-09 18:05:37.180208 - Processing request after 190.77s in queue
2025-04-09 18:05:37.883969 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:37.887114 - Processing request after 191.48s in queue
2025-04-09 18:05:38.499443 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:38.502900 - Processing request after 192.09s in queue
2025-04-09 18:05:39.111686 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:39.115384 - Processing request after 192.71s in queue
2025-04-09 18:05:39.765121 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:39.765121 - Processing request after 193.36s in queue
2025-04-09 18:05:40.348650 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:05:40.354724 - Text query received: Hello, world!...
2025-04-09 18:05:40.355250 - Text query received: Hello, world!...
2025-04-09 18:05:40.355778 - Text query received: Hello, world!...
2025-04-09 18:05:40.356372 - Text query received: Hello, world!...
2025-04-09 18:05:40.356940 - Text query received: Hello, world!...
2025-04-09 18:05:40.356940 - Processing request after 193.95s in queue
2025-04-09 18:05:40.357471 - Throttling request, waiting for 51.87 seconds
2025-04-09 18:05:40.357471 - Processing request after 193.95s in queue
2025-04-09 18:05:40.357471 - Throttling request, waiting for 51.87 seconds
2025-04-09 18:05:40.358746 - Processing request after 193.95s in queue
2025-04-09 18:05:40.359059 - Throttling request, waiting for 51.87 seconds
2025-04-09 18:05:40.359059 - Processing request after 193.95s in queue
2025-04-09 18:05:40.359059 - Throttling request, waiting for 51.87 seconds
2025-04-09 18:05:40.359059 - Processing request after 193.94s in queue
2025-04-09 18:05:40.359059 - Throttling request, waiting for 51.87 seconds
2025-04-09 18:05:40.360125 - Text query received: Hello, world!...
2025-04-09 18:05:40.360125 - Text query received: Hello, world!...
2025-04-09 18:05:40.360125 - Text query received: Hello, world!...
2025-04-09 18:05:40.360125 - Text query received: Hello, world!...
2025-04-09 18:05:40.360125 - Text query received: Hello, world!...
2025-04-09 18:06:32.715018 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:33.386605 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:34.011940 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:34.618467 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:35.096476 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:35.100509 - Processing request after 248.68s in queue
2025-04-09 18:06:35.641169 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:35.641169 - Processing request after 249.22s in queue
2025-04-09 18:06:36.260850 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:36.264637 - Processing request after 249.85s in queue
2025-04-09 18:06:36.871440 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:36.871440 - Processing request after 250.45s in queue
2025-04-09 18:06:37.329902 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:37.336269 - Processing request after 250.92s in queue
2025-04-09 18:06:37.807581 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:37.814965 - Text query received: Hello, world!...
2025-04-09 18:06:37.814965 - Text query received: Hello, world!...
2025-04-09 18:06:37.818314 - Text query received: Hello, world!...
2025-04-09 18:06:37.818314 - Text query received: Hello, world!...
2025-04-09 18:06:37.820080 - Text query received: Hello, world!...
2025-04-09 18:06:37.820688 - Processing request after 251.40s in queue
2025-04-09 18:06:38.511232 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:06:38.511232 - Processing request after 252.09s in queue
2025-04-09 18:06:39.129250 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:39.131690 - Processing request after 252.71s in queue
2025-04-09 18:06:39.609272 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:39.617922 - Processing request after 253.19s in queue
2025-04-09 18:06:40.106326 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:40.109914 - Processing request after 253.68s in queue
2025-04-09 18:06:40.773380 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:06:40.782411 - Text query received: Hello, world!...
2025-04-09 18:06:40.782964 - Text query received: Hello, world!...
2025-04-09 18:06:40.783367 - Text query received: Hello, world!...
2025-04-09 18:06:40.783874 - Text query received: Hello, world!...
2025-04-09 18:06:40.784685 - Text query received: Hello, world!...
2025-04-09 18:06:40.784685 - Processing request after 254.36s in queue
2025-04-09 18:06:40.785763 - Throttling request, waiting for 52.46 seconds
2025-04-09 18:06:40.785763 - Processing request after 254.36s in queue
2025-04-09 18:06:40.786287 - Throttling request, waiting for 52.45 seconds
2025-04-09 18:06:40.786621 - Processing request after 254.36s in queue
2025-04-09 18:06:40.786621 - Throttling request, waiting for 52.45 seconds
2025-04-09 18:06:40.787189 - Processing request after 254.36s in queue
2025-04-09 18:06:40.787189 - Throttling request, waiting for 52.45 seconds
2025-04-09 18:06:40.787189 - Processing request after 254.36s in queue
2025-04-09 18:06:40.787189 - Throttling request, waiting for 52.45 seconds
2025-04-09 18:06:40.787189 - Text query received: Hello, world!...
2025-04-09 18:06:40.788809 - Text query received: Hello, world!...
2025-04-09 18:06:40.789457 - Text query received: Hello, world!...
2025-04-09 18:06:40.789457 - Text query received: Hello, world!...
2025-04-09 18:06:40.789457 - Text query received: Hello, world!...
2025-04-09 18:07:33.799750 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:07:34.423516 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:07:35.042445 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:07:35.648915 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:07:36.272910 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-09 18:07:36.291193 - Processing request after 309.86s in queue
2025-04-09 18:07:36.799866 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:07:36.801882 - Processing request after 310.37s in queue
2025-04-09 18:07:37.435423 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:07:37.436854 - Processing request after 311.00s in queue
2025-04-09 18:07:37.897070 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:07:37.897070 - Processing request after 311.46s in queue
2025-04-09 18:07:38.408643 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:07:38.410656 - Processing request after 311.98s in queue
2025-04-09 18:07:38.926520 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:07:38.933878 - Text query received: Hello, world!...
2025-04-09 18:07:38.938917 - Text query received: Hello, world!...
2025-04-09 18:07:38.938917 - Text query received: Hello, world!...
2025-04-09 18:07:38.938917 - Text query received: Hello, world!...
2025-04-09 18:07:38.938917 - Text query received: Hello, world!...
2025-04-09 18:07:38.938917 - Processing request after 312.50s in queue
2025-04-09 18:07:39.437817 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:07:39.439834 - Processing request after 313.00s in queue
2025-04-09 18:07:39.908607 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:07:39.908607 - Processing request after 313.47s in queue
2025-04-09 18:07:40.585827 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:07:40.585827 - Processing request after 314.15s in queue
2025-04-09 18:07:41.176645 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:07:41.178660 - Processing request after 314.74s in queue
2025-04-09 18:07:41.640867 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:07:41.648561 - Text query received: Hello, world!...
2025-04-09 18:07:41.648561 - Text query received: Hello, world!...
2025-04-09 18:07:41.650575 - Text query received: Hello, world!...
2025-04-09 18:07:41.651085 - Text query received: Hello, world!...
2025-04-09 18:07:41.651085 - Text query received: Hello, world!...
2025-04-09 18:07:41.651085 - Processing request after 315.21s in queue
2025-04-09 18:07:41.651085 - Throttling request, waiting for 52.59 seconds
2025-04-09 18:07:41.651085 - Processing request after 315.21s in queue
2025-04-09 18:07:41.653291 - Throttling request, waiting for 52.59 seconds
2025-04-09 18:07:41.653291 - Processing request after 315.19s in queue
2025-04-09 18:07:41.653291 - Throttling request, waiting for 52.59 seconds
2025-04-09 18:07:41.653291 - Processing request after 315.19s in queue
2025-04-09 18:07:41.653291 - Throttling request, waiting for 52.59 seconds
2025-04-09 18:07:41.653291 - Processing request after 315.19s in queue
2025-04-09 18:07:41.653291 - Throttling request, waiting for 52.59 seconds
2025-04-09 18:07:41.654820 - Text query received: Hello, world!...
2025-04-09 18:07:41.657204 - Text query received: Hello, world!...
2025-04-09 18:07:41.658031 - Text query received: Hello, world!...
2025-04-09 18:07:41.658031 - Text query received: Hello, world!...
2025-04-09 18:07:41.658031 - Text query received: Hello, world!...
2025-04-09 18:08:34.840643 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:08:35.458338 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:08:36.067314 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:08:36.691815 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:08:37.299303 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:08:37.312748 - Processing request after 370.85s in queue
2025-04-09 18:08:37.765347 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:08:37.766425 - Processing request after 371.30s in queue
2025-04-09 18:08:38.260725 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:08:38.262072 - Processing request after 371.80s in queue
2025-04-09 18:08:38.727892 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:08:38.727892 - Processing request after 372.26s in queue
2025-04-09 18:08:39.351462 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:08:39.351462 - Processing request after 372.89s in queue
2025-04-09 18:08:39.957219 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  Back at you!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 10,
        "total_token_count": 14
      }
    }),
)
2025-04-09 18:08:39.957219 - Text query received: Hello, world!...
2025-04-09 18:08:39.957219 - Text query received: Hello, world!...
2025-04-09 18:08:39.970528 - Text query received: Hello, world!...
2025-04-09 18:08:39.971924 - Text query received: Hello, world!...
2025-04-09 18:08:39.972885 - Text query received: Hello, world!...
2025-04-09 18:08:39.972885 - Processing request after 373.51s in queue
2025-04-09 18:08:40.527381 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:08:40.529724 - Processing request after 374.06s in queue
2025-04-09 18:08:41.176935 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:08:41.176935 - Processing request after 374.71s in queue
2025-04-09 18:08:41.797888 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 18:08:41.797888 - Processing request after 375.33s in queue
2025-04-09 18:08:42.422239 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  Back at you!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 10,
        "total_token_count": 14
      }
    }),
)
2025-04-09 18:08:42.422239 - Processing request after 375.96s in queue
2025-04-09 18:08:43.028087 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:08:43.037338 - Text query received: Hello, world!...
2025-04-09 18:08:43.037338 - Text query received: Hello, world!...
2025-04-09 18:08:43.037338 - Text query received: Hello, world!...
2025-04-09 18:08:43.039616 - Text query received: Hello, world!...
2025-04-09 18:08:43.040321 - Text query received: Hello, world!...
2025-04-09 18:08:43.040321 - Processing request after 376.57s in queue
2025-04-09 18:08:43.040321 - Throttling request, waiting for 52.19 seconds
2025-04-09 18:08:43.040321 - Processing request after 376.57s in queue
2025-04-09 18:08:43.040321 - Throttling request, waiting for 52.19 seconds
2025-04-09 18:08:43.040321 - Processing request after 376.57s in queue
2025-04-09 18:08:43.040321 - Throttling request, waiting for 52.19 seconds
2025-04-09 18:08:43.040321 - Processing request after 376.57s in queue
2025-04-09 18:08:43.043035 - Throttling request, waiting for 52.19 seconds
2025-04-09 18:08:43.043035 - Processing request after 376.58s in queue
2025-04-09 18:08:43.043035 - Throttling request, waiting for 52.19 seconds
2025-04-09 18:08:43.043035 - Text query received: Hello, world!...
2025-04-09 18:08:43.043035 - Text query received: Hello, world!...
2025-04-09 18:08:43.043035 - Text query received: Hello, world!...
2025-04-09 18:08:43.043035 - Text query received: Hello, world!...
2025-04-09 18:08:43.043035 - Text query received: Hello, world!...
2025-04-09 18:09:35.862103 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:09:36.493636 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:09:37.099655 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:09:37.576448 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:09:38.052780 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:09:38.064836 - Processing request after 431.60s in queue
2025-04-09 18:09:38.746138 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:09:38.748004 - Processing request after 432.28s in queue
2025-04-09 18:09:39.281155 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-09 18:09:39.283186 - Processing request after 432.82s in queue
2025-04-09 18:09:39.908081 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:09:39.908081 - Processing request after 433.43s in queue
2025-04-09 18:09:40.339295 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:09:40.339810 - Processing request after 433.86s in queue
2025-04-09 18:09:40.991733 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:09:40.993751 - Text query received: Hello, world!...
2025-04-09 18:09:40.993751 - Text query received: Hello, world!...
2025-04-09 18:09:40.993751 - Text query received: Hello, world!...
2025-04-09 18:09:41.002130 - Text query received: Hello, world!...
2025-04-09 18:09:41.002130 - Text query received: Hello, world!...
2025-04-09 18:09:41.003155 - Processing request after 365.56s in queue
2025-04-09 18:09:41.580583 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:09:41.583721 - Processing request after 366.14s in queue
2025-04-09 18:09:42.063136 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:09:42.068695 - Processing request after 366.63s in queue
2025-04-09 18:09:42.518996 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:09:42.520003 - Processing request after 367.08s in queue
2025-04-09 18:09:43.043951 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:09:43.044961 - Processing request after 367.60s in queue
2025-04-09 18:09:43.648362 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:09:43.659685 - Text query received: Hello, world!...
2025-04-09 18:09:43.661692 - Text query received: Hello, world!...
2025-04-09 18:09:43.661692 - Text query received: Hello, world!...
2025-04-09 18:09:43.661692 - Text query received: Hello, world!...
2025-04-09 18:09:43.661692 - Text query received: Hello, world!...
2025-04-09 18:09:43.662975 - Processing request after 365.57s in queue
2025-04-09 18:09:43.662975 - Throttling request, waiting for 52.58 seconds
2025-04-09 18:09:43.662975 - Processing request after 365.57s in queue
2025-04-09 18:09:43.662975 - Throttling request, waiting for 52.58 seconds
2025-04-09 18:09:43.662975 - Processing request after 365.57s in queue
2025-04-09 18:09:43.662975 - Throttling request, waiting for 52.58 seconds
2025-04-09 18:09:43.662975 - Processing request after 365.56s in queue
2025-04-09 18:09:43.662975 - Throttling request, waiting for 52.58 seconds
2025-04-09 18:09:43.662975 - Processing request after 365.56s in queue
2025-04-09 18:09:43.662975 - Throttling request, waiting for 52.58 seconds
2025-04-09 18:09:43.662975 - Text query received: Hello, world!...
2025-04-09 18:09:43.662975 - Text query received: Hello, world!...
2025-04-09 18:09:43.667281 - Text query received: Hello, world!...
2025-04-09 18:09:43.668071 - Text query received: Hello, world!...
2025-04-09 18:09:43.668587 - Text query received: Hello, world!...
2025-04-09 18:10:36.754262 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:10:37.207569 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:10:37.934095 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 18:10:38.396406 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:10:38.905381 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:10:38.905381 - Processing request after 420.80s in queue
2025-04-09 18:10:39.394576 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:10:39.394576 - Processing request after 421.29s in queue
2025-04-09 18:10:39.974800 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:10:39.974800 - Processing request after 421.87s in queue
2025-04-09 18:10:40.532623 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 18:10:40.533737 - Processing request after 422.43s in queue
2025-04-09 18:10:41.195957 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:10:41.195957 - Processing request after 423.09s in queue
2025-04-09 18:10:41.681264 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:10:41.690917 - Text query received: Hello, world!...
2025-04-09 18:10:41.692271 - Text query received: Hello, world!...
2025-04-09 18:10:41.692271 - Text query received: Hello, world!...
2025-04-09 18:10:41.693334 - Text query received: Hello, world!...
2025-04-09 18:10:41.693334 - Text query received: Hello, world!...
2025-04-09 18:10:41.693334 - Processing request after 366.06s in queue
2025-04-09 18:10:42.177954 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  Back at you!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 10,
        "total_token_count": 14
      }
    }),
)
2025-04-09 18:10:42.177954 - Processing request after 366.54s in queue
2025-04-09 18:10:42.746932 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:10:42.746932 - Processing request after 367.11s in queue
2025-04-09 18:10:43.219270 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:10:43.219270 - Processing request after 367.58s in queue
2025-04-09 18:10:43.699213 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:10:43.699213 - Processing request after 368.06s in queue
2025-04-09 18:10:44.482025 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 18:10:44.484040 - Text query received: Hello, world!...
2025-04-09 18:10:44.492120 - Text query received: Hello, world!...
2025-04-09 18:10:44.492120 - Text query received: Hello, world!...
2025-04-09 18:10:44.493700 - Text query received: Hello, world!...
2025-04-09 18:10:44.494178 - Text query received: Hello, world!...
2025-04-09 18:10:44.495256 - Processing request after 365.98s in queue
2025-04-09 18:10:44.495256 - Throttling request, waiting for 52.76 seconds
2025-04-09 18:10:44.495256 - Processing request after 365.98s in queue
2025-04-09 18:10:44.495256 - Throttling request, waiting for 52.76 seconds
2025-04-09 18:10:44.495256 - Processing request after 365.97s in queue
2025-04-09 18:10:44.495256 - Throttling request, waiting for 52.76 seconds
2025-04-09 18:10:44.495256 - Processing request after 365.97s in queue
2025-04-09 18:10:44.497860 - Throttling request, waiting for 52.76 seconds
2025-04-09 18:10:44.497860 - Processing request after 365.97s in queue
2025-04-09 18:10:44.498441 - Throttling request, waiting for 52.75 seconds
2025-04-09 18:10:44.498441 - Text query received: Hello, world!...
2025-04-09 18:10:44.498441 - Text query received: Hello, world!...
2025-04-09 18:10:44.502276 - Text query received: Hello, world!...
2025-04-09 18:10:44.502276 - Text query received: Hello, world!...
2025-04-09 18:10:44.502276 - Text query received: Hello, world!...
2025-04-09 18:11:37.930676 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:11:38.505191 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at ya!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:11:39.156073 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:11:39.767690 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 18:11:40.263544 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:11:40.267325 - Text query received: Hello, world!...
2025-04-09 18:11:40.268502 - Text query received: Hello, world!...
2025-04-09 18:11:40.269027 - Text query received: Hello, world!...
2025-04-09 18:11:40.269573 - Processing request after 421.74s in queue
2025-04-09 18:11:40.796369 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 18:11:40.802824 - Processing request after 422.27s in queue
2025-04-09 18:11:41.417130 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:11:41.417130 - Processing request after 422.88s in queue
2025-04-09 18:11:41.905894 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:11:41.907902 - Processing request after 423.37s in queue
2025-04-09 18:11:42.343410 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:11:42.343410 - Processing request after 423.81s in queue
2025-04-09 18:11:43.052304 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:11:43.059036 - Text query received: Hello, world!...
2025-04-09 18:11:43.060929 - Text query received: Hello, world!...
2025-04-09 18:11:43.062065 - Processing request after 365.88s in queue
2025-04-09 18:11:43.540264 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:11:43.540264 - Processing request after 366.36s in queue
2025-04-09 18:11:44.077456 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:11:44.078762 - Processing request after 366.90s in queue
2025-04-09 18:11:44.694047 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:11:44.694047 - Processing request after 367.51s in queue
2025-04-09 18:11:45.248503 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:11:45.248503 - Processing request after 368.07s in queue
2025-04-09 18:11:45.917420 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-09 18:11:45.926858 - Text query received: Hello, world!...
2025-04-09 18:11:45.926858 - Text query received: Hello, world!...
2025-04-09 18:11:45.926858 - Text query received: Hello, world!...
2025-04-09 18:11:45.928996 - Text query received: Hello, world!...
2025-04-09 18:11:45.929447 - Text query received: Hello, world!...
2025-04-09 18:11:45.929447 - Processing request after 365.57s in queue
2025-04-09 18:11:45.929447 - Throttling request, waiting for 52.32 seconds
2025-04-09 18:11:45.929447 - Processing request after 365.57s in queue
2025-04-09 18:11:45.929447 - Throttling request, waiting for 52.32 seconds
2025-04-09 18:11:45.931006 - Processing request after 365.58s in queue
2025-04-09 18:11:45.931006 - Throttling request, waiting for 52.31 seconds
2025-04-09 18:11:45.931006 - Processing request after 365.57s in queue
2025-04-09 18:11:45.931006 - Throttling request, waiting for 52.31 seconds
2025-04-09 18:11:45.931006 - Processing request after 365.57s in queue
2025-04-09 18:11:45.931006 - Throttling request, waiting for 52.31 seconds
2025-04-09 18:11:45.934356 - Text query received: Hello, world!...
2025-04-09 18:11:45.934356 - Text query received: Hello, world!...
2025-04-09 18:11:45.935372 - Text query received: Hello, world!...
2025-04-09 18:11:45.935372 - Text query received: Hello, world!...
2025-04-09 18:11:45.936395 - Text query received: Hello, world!...
2025-04-09 18:12:38.783714 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:12:39.282391 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:12:40.331962 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:12:40.806903 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:12:41.421420 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:12:41.421420 - Processing request after 421.06s in queue
2025-04-09 18:12:41.868434 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:12:41.869994 - Processing request after 421.51s in queue
2025-04-09 18:12:42.625191 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:12:42.630986 - Processing request after 422.27s in queue
2025-04-09 18:12:43.263377 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:12:43.263920 - Processing request after 422.90s in queue
2025-04-09 18:12:43.803271 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-09 18:12:43.803271 - Processing request after 423.44s in queue
2025-04-09 18:12:44.484621 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:12:44.484621 - Processing request after 366.67s in queue
2025-04-09 18:12:45.112012 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:12:45.114490 - Processing request after 367.30s in queue
2025-04-09 18:12:45.717112 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:12:45.718127 - Processing request after 367.90s in queue
2025-04-09 18:12:46.336993 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:12:46.336993 - Processing request after 368.52s in queue
2025-04-09 18:12:46.830802 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:12:46.830802 - Processing request after 369.01s in queue
2025-04-09 18:12:47.572384 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:12:47.574639 - Processing request after 366.79s in queue
2025-04-09 18:12:47.574639 - Throttling request, waiting for 51.68 seconds
2025-04-09 18:12:47.574639 - Processing request after 366.79s in queue
2025-04-09 18:12:47.574639 - Throttling request, waiting for 51.68 seconds
2025-04-09 18:12:47.574639 - Processing request after 366.79s in queue
2025-04-09 18:12:47.576646 - Throttling request, waiting for 51.68 seconds
2025-04-09 18:12:47.576646 - Processing request after 366.79s in queue
2025-04-09 18:12:47.576646 - Throttling request, waiting for 51.68 seconds
2025-04-09 18:12:47.576646 - Processing request after 366.79s in queue
2025-04-09 18:12:47.576646 - Throttling request, waiting for 51.68 seconds
2025-04-09 18:13:39.791892 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:13:40.408319 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at ya!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:13:40.893963 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:13:41.352636 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:13:41.879201 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 18:13:41.879201 - Processing request after 421.09s in queue
2025-04-09 18:13:42.333031 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:13:42.335148 - Processing request after 421.55s in queue
2025-04-09 18:13:42.869451 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:13:42.872495 - Processing request after 422.08s in queue
2025-04-09 18:13:43.392085 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:13:43.392085 - Processing request after 422.60s in queue
2025-04-09 18:13:43.922083 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:13:43.922083 - Processing request after 423.13s in queue
2025-04-09 18:13:44.493162 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:13:44.495181 - Processing request after 365.56s in queue
2025-04-09 18:13:45.112316 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:13:45.112316 - Processing request after 366.17s in queue
2025-04-09 18:13:45.112316 - Throttling request, waiting for 1.00 seconds
2025-04-09 18:13:45.112316 - Processing request after 366.17s in queue
2025-04-09 18:13:45.112316 - Throttling request, waiting for 1.00 seconds
2025-04-09 18:13:45.118407 - Processing request after 366.18s in queue
2025-04-09 18:13:45.724933 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:13:45.727022 - Processing request after 366.79s in queue
2025-04-09 18:13:46.337661 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:13:46.957848 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  Back at you!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 10,
        "total_token_count": 14
      }
    }),
)
2025-04-09 18:13:47.422891 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:13:47.422891 - Processing request after 365.77s in queue
2025-04-09 18:13:47.422891 - Throttling request, waiting for 52.83 seconds
2025-04-09 18:13:47.422891 - Processing request after 365.77s in queue
2025-04-09 18:13:47.427485 - Throttling request, waiting for 52.83 seconds
2025-04-09 18:13:47.427485 - Processing request after 365.78s in queue
2025-04-09 18:13:47.427485 - Throttling request, waiting for 52.82 seconds
2025-04-09 18:13:47.427485 - Processing request after 365.78s in queue
2025-04-09 18:13:47.427485 - Throttling request, waiting for 52.82 seconds
2025-04-09 18:13:47.427485 - Processing request after 365.78s in queue
2025-04-09 18:13:47.427485 - Throttling request, waiting for 52.82 seconds
2025-04-09 18:14:40.759438 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:14:41.223059 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:14:41.841678 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:14:42.463947 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  Back at you!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 10,
        "total_token_count": 14
      }
    }),
)
2025-04-09 18:14:43.066692 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:14:43.066692 - Processing request after 421.41s in queue
2025-04-09 18:14:43.537710 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:14:43.537710 - Processing request after 421.88s in queue
2025-04-09 18:14:44.093604 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:14:44.093604 - Processing request after 422.44s in queue
2025-04-09 18:14:44.552093 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:14:44.554109 - Processing request after 422.90s in queue
2025-04-09 18:14:45.084923 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:14:45.086937 - Processing request after 423.43s in queue
2025-04-09 18:14:45.736609 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:14:45.743506 - Processing request after 365.79s in queue
2025-04-09 18:14:46.345242 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:14:46.347475 - Processing request after 366.39s in queue
2025-04-09 18:14:46.965966 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:14:46.967822 - Processing request after 367.00s in queue
2025-04-09 18:14:47.572569 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:14:47.572569 - Processing request after 367.60s in queue
2025-04-09 18:14:48.415007 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:14:48.415821 - Processing request after 368.44s in queue
2025-04-09 18:14:49.020134 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:14:49.023309 - Processing request after 365.99s in queue
2025-04-09 18:14:49.023904 - Throttling request, waiting for 52.24 seconds
2025-04-09 18:14:49.024918 - Processing request after 365.99s in queue
2025-04-09 18:14:49.025426 - Throttling request, waiting for 52.24 seconds
2025-04-09 18:14:49.025978 - Processing request after 365.99s in queue
2025-04-09 18:14:49.026709 - Throttling request, waiting for 52.24 seconds
2025-04-09 18:14:49.027757 - Processing request after 365.99s in queue
2025-04-09 18:14:49.029330 - Throttling request, waiting for 52.23 seconds
2025-04-09 18:14:49.029808 - Processing request after 365.99s in queue
2025-04-09 18:14:49.030314 - Throttling request, waiting for 52.23 seconds
2025-04-09 18:15:41.740853 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:15:42.294931 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:15:42.867227 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:15:43.487696 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:15:44.023642 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at ya!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:15:44.025273 - Processing request after 420.98s in queue
2025-04-09 18:15:44.720993 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at ya!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:15:44.722991 - Processing request after 421.68s in queue
2025-04-09 18:15:45.237429 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:15:45.237429 - Processing request after 422.19s in queue
2025-04-09 18:15:45.690652 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:15:45.694289 - Processing request after 422.65s in queue
2025-04-09 18:15:46.361031 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  Back at you!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 10,
        "total_token_count": 14
      }
    }),
)
2025-04-09 18:15:46.361031 - Processing request after 423.32s in queue
2025-04-09 18:15:46.966230 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:15:46.968738 - Processing request after 365.97s in queue
2025-04-09 18:15:47.591249 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 18:15:47.593268 - Processing request after 366.60s in queue
2025-04-09 18:15:48.197983 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:15:48.199999 - Processing request after 367.21s in queue
2025-04-09 18:15:48.817853 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:15:48.817853 - Processing request after 367.82s in queue
2025-04-09 18:15:49.422955 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:15:49.422955 - Processing request after 368.42s in queue
2025-04-09 18:15:50.045026 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:15:50.045026 - Processing request after 366.39s in queue
2025-04-09 18:15:50.045026 - Throttling request, waiting for 52.23 seconds
2025-04-09 18:15:50.045026 - Processing request after 366.38s in queue
2025-04-09 18:15:50.045026 - Throttling request, waiting for 52.23 seconds
2025-04-09 18:15:50.047957 - Processing request after 366.39s in queue
2025-04-09 18:15:50.047957 - Throttling request, waiting for 52.23 seconds
2025-04-09 18:15:50.047957 - Processing request after 366.39s in queue
2025-04-09 18:15:50.047957 - Throttling request, waiting for 52.23 seconds
2025-04-09 18:15:50.047957 - Processing request after 366.39s in queue
2025-04-09 18:15:50.047957 - Throttling request, waiting for 52.23 seconds
2025-04-09 18:16:42.885765 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:16:43.489816 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:16:44.038298 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-09 18:16:44.507869 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:16:45.125927 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:16:45.127939 - Processing request after 421.46s in queue
2025-04-09 18:16:45.617560 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:16:45.617560 - Processing request after 421.95s in queue
2025-04-09 18:16:46.089418 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:16:46.102426 - Processing request after 422.44s in queue
2025-04-09 18:16:46.614033 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-09 18:16:46.629724 - Processing request after 422.96s in queue
2025-04-09 18:16:47.091419 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:16:47.107396 - Processing request after 423.44s in queue
2025-04-09 18:16:47.582136 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:16:47.598275 - Processing request after 365.91s in queue
2025-04-09 18:16:48.202007 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:16:48.204017 - Processing request after 366.51s in queue
2025-04-09 18:16:48.823035 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-09 18:16:48.823035 - Processing request after 367.13s in queue
2025-04-09 18:16:49.439631 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  It's nice to meet you too.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 15,
        "total_token_count": 19
      }
    }),
)
2025-04-09 18:16:49.439631 - Processing request after 367.75s in queue
2025-04-09 18:16:50.042618 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:16:50.043591 - Processing request after 368.35s in queue
2025-04-09 18:16:50.579500 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:16:50.579500 - Processing request after 366.10s in queue
2025-04-09 18:16:50.579500 - Throttling request, waiting for 52.70 seconds
2025-04-09 18:16:50.579500 - Processing request after 366.09s in queue
2025-04-09 18:16:50.579500 - Throttling request, waiting for 52.70 seconds
2025-04-09 18:16:50.579500 - Processing request after 366.09s in queue
2025-04-09 18:16:50.579500 - Throttling request, waiting for 52.70 seconds
2025-04-09 18:16:50.579500 - Processing request after 366.09s in queue
2025-04-09 18:16:50.579500 - Throttling request, waiting for 52.70 seconds
2025-04-09 18:16:50.579500 - Processing request after 366.09s in queue
2025-04-09 18:16:50.587951 - Throttling request, waiting for 52.70 seconds
2025-04-09 18:17:43.918696 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 18:17:44.508744 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at ya!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:17:45.115941 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (And hello to you too!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 13,
        "total_token_count": 17
      }
    }),
)
2025-04-09 18:17:45.621433 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:17:46.157490 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  Back at you!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 10,
        "total_token_count": 14
      }
    }),
)
2025-04-09 18:17:46.160539 - Processing request after 421.66s in queue
2025-04-09 18:17:46.783689 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:17:46.783689 - Processing request after 422.29s in queue
2025-04-09 18:17:47.388945 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:17:47.388945 - Processing request after 422.89s in queue
2025-04-09 18:17:48.011699 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:17:48.011699 - Processing request after 423.51s in queue
2025-04-09 18:17:48.621214 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:17:48.621214 - Processing request after 424.12s in queue
2025-04-09 18:17:49.100509 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:17:49.100509 - Processing request after 368.83s in queue
2025-04-09 18:17:49.644161 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:17:49.644161 - Processing request after 369.38s in queue
2025-04-09 18:17:50.262205 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:17:50.264214 - Processing request after 370.00s in queue
2025-04-09 18:17:50.767779 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:17:50.768968 - Processing request after 367.71s in queue
2025-04-09 18:17:51.284229 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  Back at you!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 10,
        "total_token_count": 14
      }
    }),
)
2025-04-09 18:17:51.286245 - Processing request after 368.22s in queue
2025-04-09 18:17:51.903534 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:17:51.905621 - Processing request after 365.98s in queue
2025-04-09 18:17:51.907637 - Throttling request, waiting for 52.39 seconds
2025-04-09 18:17:51.907637 - Processing request after 365.98s in queue
2025-04-09 18:17:51.907637 - Throttling request, waiting for 52.39 seconds
2025-04-09 18:17:51.907637 - Processing request after 365.98s in queue
2025-04-09 18:17:51.907637 - Throttling request, waiting for 52.39 seconds
2025-04-09 18:17:51.909651 - Processing request after 365.98s in queue
2025-04-09 18:17:51.909651 - Throttling request, waiting for 52.39 seconds
2025-04-09 18:17:51.909651 - Processing request after 365.98s in queue
2025-04-09 18:17:51.909651 - Throttling request, waiting for 52.39 seconds
2025-04-09 18:18:44.810014 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:18:45.294601 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:18:45.881652 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:18:46.360722 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:18:46.824636 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:18:46.824636 - Processing request after 420.89s in queue
2025-04-09 18:18:47.387778 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at ya!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:18:47.388385 - Processing request after 421.45s in queue
2025-04-09 18:18:47.883644 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  Back at you!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 10,
        "total_token_count": 14
      }
    }),
)
2025-04-09 18:18:47.883644 - Processing request after 421.95s in queue
2025-04-09 18:18:48.427253 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 18:18:48.427253 - Processing request after 422.49s in queue
2025-04-09 18:18:48.934963 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 18:18:48.936389 - Processing request after 423.00s in queue
2025-04-09 18:18:49.566565 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 19:09:09.295276 - Text query received: Hello, world!...
2025-04-09 19:09:09.295276 - Text query received: Hello, world!...
2025-04-09 19:09:09.307420 - Processing request after 0.01s in queue
2025-04-09 19:09:10.675079 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at ya!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 19:09:10.677389 - Text query received: Hello, world!...
2025-04-09 19:09:10.677389 - Text query received: Hello, world!...
2025-04-09 19:09:10.679974 - Text query received: Hello, world!...
2025-04-09 19:09:10.679974 - Text query received: Hello, world!...
2025-04-09 19:09:10.679974 - Text query received: Hello, world!...
2025-04-09 19:09:10.679974 - Text query received: Hello, world!...
2025-04-09 19:09:10.688742 - Text query received: Hello, world!...
2025-04-09 19:09:10.688742 - Text query received: Hello, world!...
2025-04-09 19:09:10.688742 - Text query received: Hello, world!...
2025-04-09 19:09:10.688742 - Processing request after 1.39s in queue
2025-04-09 19:09:11.208893 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 19:09:11.212386 - Processing request after 0.53s in queue
2025-04-09 19:09:11.751892 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello there!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 4,
        "total_token_count": 8
      }
    }),
)
2025-04-09 19:09:11.754487 - Processing request after 1.07s in queue
2025-04-09 19:09:12.249313 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 19:09:12.249313 - Processing request after 1.57s in queue
2025-04-09 19:09:12.904666 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 19:09:12.904666 - Processing request after 2.22s in queue
2025-04-09 19:09:13.553095 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 19:09:13.566253 - Text query received: Hello, world!...
2025-04-09 19:09:13.568940 - Text query received: Hello, world!...
2025-04-09 19:09:13.568940 - Processing request after 2.89s in queue
2025-04-09 19:09:14.166173 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 19:09:14.167588 - Processing request after 3.48s in queue
2025-04-09 19:09:14.791658 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!  (back at you!)\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 11,
        "total_token_count": 15
      }
    }),
)
2025-04-09 19:09:14.792774 - Processing request after 4.10s in queue
2025-04-09 19:09:15.387746 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 19:09:15.389999 - Processing request after 4.70s in queue
2025-04-09 19:09:16.011406 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 19:09:16.013802 - Processing request after 5.33s in queue
2025-04-09 19:09:16.618004 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 19:09:16.618004 - Text query received: Hello, world!...
2025-04-09 19:09:16.630648 - Text query received: Hello, world!...
2025-04-09 19:09:16.635285 - Processing request after 3.07s in queue
2025-04-09 19:09:17.138066 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 19:09:17.143098 - Processing request after 3.57s in queue
2025-04-09 19:09:17.648194 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 19:09:17.648194 - Processing request after 1.03s in queue
2025-04-09 19:09:18.131634 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-09 19:09:18.131634 - Processing request after 1.50s in queue
2025-04-09 19:09:18.670056 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hello, world!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 4,
        "candidates_token_count": 5,
        "total_token_count": 9
      }
    }),
)
2025-04-16 20:13:25.402758 - Text query received: hi...
2025-04-16 20:13:25.404238 - Processing request after 0.00s in queue
2025-04-16 20:13:25.404238 - Fatal error in request: 'ModelConfigurator' object has no attribute 'lock'
2025-04-16 20:13:25.404238 - Error in retrieveWithTextOnly: 'ModelConfigurator' object has no attribute 'lock'
2025-04-16 20:14:46.626565 - Text query received: hi...
2025-04-16 20:14:46.626565 - Processing request after 0.00s in queue
2025-04-16 20:14:46.627566 - Fatal error in request: 'ModelConfigurator' object has no attribute 'lock'
2025-04-16 20:14:46.627566 - Error in retrieveWithTextOnly: 'ModelConfigurator' object has no attribute 'lock'
2025-04-16 20:19:20.453789 - Text query received: hi...
2025-04-16 20:19:20.453789 - Processing request after 0.00s in queue
2025-04-16 20:19:21.990163 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 233,
        "candidates_token_count": 9,
        "total_token_count": 242
      }
    }),
)
2025-04-16 20:19:33.224933 - Text query received: what is the mass of the sun...
2025-04-16 20:19:33.227937 - Processing request after 0.00s in queue
2025-04-16 20:19:33.817649 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I\u2019m sorry, I only talk about tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 249,
        "candidates_token_count": 12,
        "total_token_count": 261
      }
    }),
)
2025-04-16 20:19:46.461044 - Text query received: can you tell about tomatoes...
2025-04-16 20:19:46.462045 - Processing request after 0.00s in queue
2025-04-16 20:19:48.196979 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Okay, I can tell you about tomatoes. Here's some information:\n\n*   **Botanical Definition:** Tomatoes are botanically classified as fruits because they develop from the flower of the tomato plant and contain seeds.\n*   **Culinary Use:** However, they are generally used as vegetables in cooking due to their savory flavor profile.\n*   **Varieties:** There are thousands of tomato varieties, ranging in size, shape, color, and flavor. Some popular types include beefsteak, cherry, plum, and heirloom tomatoes.\n*   **Nutritional Value:** Tomatoes are a good source of vitamins A and C, as well as antioxidants like lycopene.\n*   **Uses:** They can be eaten raw, cooked, or processed into various products like sauces, ketchup, and juice.\n*   **Growing Conditions:** Tomatoes thrive in warm weather and require plenty of sunlight and well-drained soil.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "citation_metadata": {
            "citation_sources": [
              {
                "start_index": 93,
                "end_index": 213,
                "uri": "https://www.rainbowcoloringpages.com/food-drinks/drinks/red-smoothie/"
              }
            ]
          }
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 266,
        "candidates_token_count": 186,
        "total_token_count": 452
      }
    }),
)
2025-04-16 20:20:25.874669 - Image query received: ...
2025-04-16 20:20:25.875945 - Processing request after 0.00s in queue
2025-04-16 20:20:28.737213 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The tomato has blossom rot disease.\n\n1.  **Description:** Blossom end rot is a physiological disorder that affects tomatoes (and other fruits like peppers, squash, and watermelons). It's characterized by a dark, leathery, sunken spot on the blossom end (the bottom) of the fruit.\n\n2.  **Possible reasons:**\n    *   **Calcium Deficiency:** The most common cause is a calcium deficiency in the developing fruit. This doesn't necessarily mean there's a lack of calcium in the soil, but rather that the plant isn't able to transport enough calcium to the fruit.\n    *   **Inconsistent Watering:** Fluctuations in soil moisture (too dry followed by too wet) can interfere with calcium uptake.\n    *   **Root Damage:** Damaged roots may not be able to absorb enough calcium.\n    *   **High Nitrogen Levels:** Excessive nitrogen fertilization can sometimes interfere with calcium uptake.\n    *   **High Salt Levels:** High salt levels in the soil can also hinder calcium absorption.\n\n3.  **Suggested Solutions:**\n    *   **Consistent Watering:** Maintain consistent soil moisture, especially during fruit development. Use mulch to help retain moisture.\n    *   **Soil Testing:** Get your soil tested to determine if there's a calcium deficiency and adjust accordingly.\n    *   **Calcium Supplement:** If a soil test indicates a deficiency, amend the soil with a calcium supplement like lime or gypsum. You can also use a foliar spray containing calcium chloride, but this is a temporary solution.\n    *   **Avoid Over-Fertilizing:** Use a balanced fertilizer and avoid excessive nitrogen.\n    *   **Improve Drainage:** Ensure good soil drainage to prevent waterlogged conditions.\n    *   **Protect Roots:** Avoid damaging the roots when cultivating around the plants.\n    *   **Remove Affected Fruit:** Remove any affected fruit to encourage the plant to produce healthy fruit.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 654,
        "candidates_token_count": 396,
        "total_token_count": 1050
      }
    }),
)
2025-04-16 20:23:29.291687 - Text query received: so I can eat this tomato?...
2025-04-16 20:23:29.291687 - Processing request after 0.00s in queue
2025-04-16 20:23:30.664021 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "No, you should not eat the tomato with blossom end rot. While the unaffected parts of the tomato might be technically safe to eat, the affected area is not palatable and may contain bacteria. It's best to discard the entire fruit to avoid any potential health risks.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 856,
        "candidates_token_count": 55,
        "total_token_count": 911
      }
    }),
)
2025-04-16 20:23:48.248989 - Text query received: ah I  see thanks ...
2025-04-16 20:23:48.248989 - Processing request after 0.00s in queue
2025-04-16 20:23:48.958078 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "You're welcome! Is there anything else I can help you with regarding tomatoes?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 917,
        "candidates_token_count": 18,
        "total_token_count": 935
      }
    }),
)
2025-04-16 20:36:31.564297 - Text query received: red or yellow?...
2025-04-16 20:36:31.565448 - Processing request after 0.00s in queue
2025-04-16 20:36:33.351785 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "That depends on what you're looking for! Red tomatoes are the most common and often have a classic tomato flavor - a good balance of sweetness and acidity. Yellow tomatoes are often milder and sweeter, with a lower acid content.\n\nHere's a quick breakdown:\n\n*   **Red Tomatoes:**\n    *   Classic tomato flavor\n    *   Good balance of sweetness and acidity\n    *   Versatile for cooking and eating raw\n*   **Yellow Tomatoes:**\n    *   Milder and sweeter flavor\n    *   Lower acidity\n    *   Good for people sensitive to acid\n    *   Can add a different color to salads and dishes\n\nUltimately, the best choice depends on your personal preference and what you plan to use the tomato for. Do you have a specific dish in mind? That might help me recommend one over the other.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 939,
        "candidates_token_count": 174,
        "total_token_count": 1113
      }
    }),
)
2025-04-17 10:21:50.562822 - Text query received: hi...
2025-04-17 10:21:50.563940 - Processing request after 0.00s in queue
2025-04-17 10:21:52.392376 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-04-17 10:23:30.528411 - Text query received: Hi! How can I help you?...
2025-04-17 10:23:30.528411 - Processing request after 0.00s in queue
2025-04-17 10:23:31.208991 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 177,
        "candidates_token_count": 9,
        "total_token_count": 186
      }
    }),
)
2025-04-17 10:30:59.106186 - Text query received: , and the latest message is ==...
2025-04-17 10:30:59.106186 - Processing request after 0.00s in queue
2025-04-17 10:31:00.668180 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Okay, I understand. I will only provide information about tomatoes, respond to general greetings politely, and state my limitations when asked about unrelated topics.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 30,
        "total_token_count": 200
      }
    }),
)
2025-04-17 10:32:20.415789 - Text query received: , and the latest message is ==...
2025-04-17 10:32:20.415789 - Processing request after 0.00s in queue
2025-04-17 10:32:21.000699 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-04-18 00:36:57.100917 - Text query received: hi...
2025-04-18 00:36:57.104022 - Processing request after 0.00s in queue
2025-04-18 00:36:59.025357 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Okay, I understand. I will only provide information about tomatoes and follow the specific guidelines you've outlined.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 23,
        "total_token_count": 193
      }
    }),
)
2025-04-18 00:39:13.816291 - Text query received: hello...
2025-04-18 00:39:13.816291 - Processing request after 0.00s in queue
2025-04-18 00:39:15.141218 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 194,
        "candidates_token_count": 9,
        "total_token_count": 203
      }
    }),
)
2025-04-18 00:39:25.976072 - Text query received: I can eat raw tomatoes...
2025-04-18 00:39:25.977072 - Processing request after 0.00s in queue
2025-04-18 00:39:26.668329 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Okay, I understand. I will only provide information about tomatoes and follow the specific guidelines you've outlined.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 208,
        "candidates_token_count": 23,
        "total_token_count": 231
      }
    }),
)
2025-04-18 00:39:38.591509 - Text query received: hi...
2025-04-18 00:39:38.592420 - Processing request after 0.00s in queue
2025-04-18 00:39:39.363347 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-04-18 00:39:46.423360 - Text query received: can I eat raw tomatoes?...
2025-04-18 00:39:46.424393 - Processing request after 0.00s in queue
2025-04-18 00:39:47.765185 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Yes, you can eat raw tomatoes! Here are some things to consider:\n\n*   **Safety:** Raw tomatoes are generally safe to eat.\n*   **Taste:** Some people prefer the taste of raw tomatoes, enjoying their fresh, slightly acidic flavor.\n*   **Nutritional Value:** Eating tomatoes raw preserves their heat-sensitive nutrients like Vitamin C.\n*   **Preparation:** Wash tomatoes thoroughly before eating them raw.\n*   **Versatility:** Raw tomatoes can be enjoyed in salads, sandwiches, or simply sliced and eaten on their own.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 185,
        "candidates_token_count": 113,
        "total_token_count": 298
      }
    }),
)
2025-04-18 00:39:59.837785 - Text query received: oh wow...
2025-04-18 00:39:59.838918 - Processing request after 0.00s in queue
2025-04-18 00:40:00.253055 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 300,
        "candidates_token_count": 9,
        "total_token_count": 309
      }
    }),
)
2025-04-18 00:40:54.699875 - Text query received: what can I do with tomato...
2025-04-18 00:40:54.700887 - Processing request after 0.00s in queue
2025-04-18 00:40:56.173751 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Here are some things you can do with tomatoes:\n\n*   **Eat them raw:** Enjoy them in salads, sandwiches, or as a snack.\n*   **Make sauce:** Use them to create delicious pasta sauces, pizza sauces, or dipping sauces.\n*   **Roast them:** Roasting brings out their sweetness and intensifies their flavor.\n*   **Make soup:** Tomato soup is a classic and comforting dish.\n*   **Can or preserve them:** Preserve your tomato harvest for use throughout the year.\n*   **Make salsa:** A fresh and flavorful addition to any meal.\n*   **Grill them:** Grilled tomatoes are a great side dish or topping for burgers.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 315,
        "candidates_token_count": 141,
        "total_token_count": 456
      }
    }),
)
2025-04-18 00:41:02.880554 - Text query received: nice...
2025-04-18 00:41:02.880554 - Processing request after 0.00s in queue
2025-04-18 00:41:03.549866 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Okay, I understand. I will only provide information about tomatoes and follow the other rules you've outlined.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 457,
        "candidates_token_count": 23,
        "total_token_count": 480
      }
    }),
)
2025-04-18 00:41:21.894151 - Text query received: thank you...
2025-04-18 00:41:21.895156 - Processing request after 0.00s in queue
2025-04-18 00:41:22.368712 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "You're welcome! Is there anything else I can help you with regarding tomatoes?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 482,
        "candidates_token_count": 18,
        "total_token_count": 500
      }
    }),
)
2025-04-24 01:44:29.844360 - Text query received: my tomato has black spots why ...
2025-04-24 01:44:29.845869 - Processing request after 0.00s in queue
2025-04-24 01:44:32.228941 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The black spots on your tomato could be caused by a few different things. Here are the most common possibilities:\n\n*   **Blossom-End Rot:** Despite the name, blossom-end rot can sometimes appear as dark, sunken spots on the sides of the tomato, not just the blossom end. It's caused by a calcium deficiency, often due to inconsistent watering.\n\n*   **Septoria Leaf Spot:** While primarily a leaf disease, Septoria can sometimes affect the fruit. Look for small, dark spots on the leaves with light centers.\n\n*   **Anthracnose:** This fungal disease causes small, sunken, circular spots that can enlarge and merge together. It's more common in warm, humid conditions.\n\n*   **Early Blight:** This fungal disease can cause dark, leathery spots on the fruit, often near the stem. It also affects the leaves.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 178,
        "candidates_token_count": 182,
        "total_token_count": 360
      }
    }),
)
2025-04-24 10:44:13.141432 - Text query received: hi gpt...
2025-04-24 10:44:13.142453 - Processing request after 0.00s in queue
2025-04-24 10:44:14.735467 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 172,
        "candidates_token_count": 9,
        "total_token_count": 181
      }
    }),
)
2025-04-24 10:44:21.632462 - Text query received: hi...
2025-04-24 10:44:21.633487 - Processing request after 0.00s in queue
2025-04-24 10:44:22.300903 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 182,
        "candidates_token_count": 9,
        "total_token_count": 191
      }
    }),
)
2025-04-24 10:44:30.962662 - Text query received: what is the mass of sun...
2025-04-24 10:44:30.963663 - Processing request after 0.00s in queue
2025-04-24 10:44:31.698085 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I\u2019m sorry, I only talk about tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 197,
        "candidates_token_count": 12,
        "total_token_count": 209
      }
    }),
)
2025-04-24 10:45:08.822304 - Text query received: if its blossom end rot then wh...
2025-04-24 10:45:08.823239 - Processing request after 0.00s in queue
2025-04-24 10:45:10.642886 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "If it's blossom-end rot, here's what you can do:\n\n*   **Water Consistently:** Inconsistent watering is the biggest culprit. Aim for even moisture in the soil. Avoid letting the soil dry out completely, then overwatering.\n*   **Check Soil Calcium:** A soil test can tell you if your soil is deficient in calcium.\n*   **Add Calcium (If Needed):** If the soil test indicates a deficiency, you can add a calcium supplement.\n*   **Improve Soil Drainage:** Poor drainage can interfere with calcium uptake.\n*   **Foliar Spray (Temporary Fix):** A foliar spray of calcium chloride can provide a temporary boost of calcium to the plant.\n*   **Remove Affected Fruit:** Remove tomatoes with blossom-end rot to encourage the plant to put energy into producing healthy fruit.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 367,
        "candidates_token_count": 174,
        "total_token_count": 541
      }
    }),
)
2025-04-24 11:02:42.203705 - Text query received: hi...
2025-04-24 11:02:42.205606 - Processing request after 0.00s in queue
2025-04-24 11:02:43.380991 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-04-24 11:06:46.218815 - Text query received: hi...
2025-04-24 11:06:46.219824 - Processing request after 0.00s in queue
2025-04-24 11:06:47.125137 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-04-24 11:07:59.834264 - Text query received: i am noticing rust spots on se...
2025-04-24 11:07:59.835251 - Processing request after 0.00s in queue
2025-04-24 11:08:01.488412 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I\u2019m sorry, I only talk about tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 203,
        "candidates_token_count": 12,
        "total_token_count": 215
      }
    }),
)
2025-04-24 11:08:25.685727 - Text query received: ok, tell me about some common ...
2025-04-24 11:08:25.686720 - Processing request after 0.00s in queue
2025-04-24 11:08:27.924929 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Here are some common diseases that can affect tomatoes:\n\n*   **Early Blight:** This fungal disease causes dark brown spots on leaves, starting with the lower ones. The spots may have concentric rings. It can also affect stems and fruit.\n\n*   **Late Blight:** Another fungal disease, late blight can rapidly destroy tomato plants, especially in cool, wet weather. It causes irregular, water-soaked spots on leaves and can quickly spread to stems and fruit.\n\n*   **Septoria Leaf Spot:** Small, circular spots with dark borders and light centers appear on the lower leaves. It can cause significant defoliation.\n\n*   **Fusarium Wilt:** A soilborne fungal disease that blocks the plant's vascular system, causing wilting, yellowing, and eventual death.\n\n*   **Verticillium Wilt:** Similar to Fusarium wilt, this soilborne fungus causes wilting and yellowing, but the symptoms may be less severe.\n\n*   **Blossom End Rot:** This isn't a disease but a physiological disorder caused by calcium deficiency. The blossom end of the tomato develops a dark, sunken, leathery spot.\n\n*   **Tomato Mosaic Virus (TMV):** A viral disease that causes mottled leaves, stunted growth, and reduced fruit production.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 226,
        "candidates_token_count": 262,
        "total_token_count": 488
      }
    }),
)
2025-04-24 11:09:36.347678 - Text query received: if I want to keep my tomatoes ...
2025-04-24 11:09:36.348677 - Processing request after 0.00s in queue
2025-04-24 11:09:39.589677 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "To keep your tomatoes safe from diseases, consider these steps:\n\n*   **Choose Disease-Resistant Varieties:** Select tomato varieties that are labeled as resistant to common diseases in your area. Look for codes like V (Verticillium wilt), F (Fusarium wilt), N (nematodes), T (Tobacco mosaic virus), and A (Alternaria).\n\n*   **Start with Healthy Seedlings:** If you're not starting from seed, purchase healthy-looking tomato seedlings from a reputable source. Avoid plants with spots, yellowing, or other signs of disease.\n\n*   **Proper Spacing and Air Circulation:** Space your tomato plants adequately to allow for good air circulation. This helps to reduce humidity and prevent fungal diseases from spreading.\n\n*   **Watering Practices:** Water tomato plants at the base to avoid wetting the foliage. Water in the morning so the leaves have time to dry before evening. Avoid overwatering, as this can create conditions favorable for disease.\n\n*   **Mulching:** Apply a layer of mulch around the base of the plants to prevent soil from splashing onto the leaves, which can spread soilborne diseases.\n\n*   **Pruning:** Prune tomato plants to improve air circulation and remove suckers (the shoots that grow between the main stem and branches).\n\n*   **Crop Rotation:** Avoid planting tomatoes in the same location year after year. Rotate crops to prevent the buildup of soilborne pathogens.\n\n*   **Sanitation:** Remove and destroy any infected leaves or plants promptly. Clean up plant debris at the end of the growing season to reduce the overwintering of pathogens.\n\n*   **Fungicides:** If necessary, use fungicides preventatively or at the first sign of disease. Choose products specifically labeled for use on tomatoes and follow the instructions carefully.\n\n*   **Soil Health:** Maintain healthy soil with good drainage and fertility. Healthy plants are better able to resist disease.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 506,
        "candidates_token_count": 389,
        "total_token_count": 895
      }
    }),
)
2025-04-24 11:10:27.018275 - Text query received: thank you for the help...
2025-04-24 11:10:27.019278 - Processing request after 0.00s in queue
2025-04-24 11:10:27.945814 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "You're welcome! Is there anything else I can help you with regarding tomatoes?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 900,
        "candidates_token_count": 18,
        "total_token_count": 918
      }
    }),
)
2025-04-24 11:16:36.735489 - Text query received: ...
2025-04-24 11:16:36.736557 - Processing request after 0.00s in queue
2025-04-24 11:16:38.102879 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I'm sorry, I only talk about tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 177,
        "candidates_token_count": 12,
        "total_token_count": 189
      }
    }),
)
2025-04-24 11:17:04.860762 - Text query received: in the above picture what can ...
2025-04-24 11:17:04.861648 - Processing request after 0.00s in queue
2025-04-24 11:17:05.345618 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I\u2019m sorry, I only talk about tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 198,
        "candidates_token_count": 12,
        "total_token_count": 210
      }
    }),
)
2025-04-24 11:19:04.235176 - Text query received: hi...
2025-04-24 11:19:04.235689 - Processing request after 0.00s in queue
2025-04-24 11:19:05.716434 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-04-24 11:19:42.207762 - Image query received: what plan is this?...
2025-04-24 11:19:42.208307 - Processing request after 0.00s in queue
2025-04-24 11:19:43.809904 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The tomato has splitting disease.\n\n1.  **Description:** Splitting in tomatoes refers to cracks that appear on the fruit, usually around the stem end, but sometimes vertically down the sides.\n\n2.  **Possible Reasons:** Splitting is typically caused by inconsistent watering, rapid changes in soil moisture, or over-fertilization. These conditions lead to a sudden increase in water uptake, causing the fruit to expand faster than the skin can accommodate. Temperature fluctuations can also contribute.\n\n3.  **Suggested Solutions:**\n    *   Maintain consistent watering.\n    *   Use mulch to regulate soil moisture.\n    *   Avoid over-fertilizing, especially with nitrogen.\n    *   Harvest fruits as they ripen to prevent them from becoming overripe and prone to splitting.\n\nwhat plan is this?\nThis question is not relevant to tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 375,
        "candidates_token_count": 176,
        "total_token_count": 551
      }
    }),
)
2025-04-24 11:22:26.983120 - Image query received: what is wrong with this plant?...
2025-04-24 11:22:26.983654 - Processing request after 0.00s in queue
2025-04-24 11:22:27.550855 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Apologies, I cannot detect any disease in this photo.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 566,
        "candidates_token_count": 12,
        "total_token_count": 578
      }
    }),
)
2025-04-24 11:22:59.357445 - Text query received: what plant is this then?...
2025-04-24 11:22:59.358346 - Processing request after 0.00s in queue
2025-04-24 11:22:59.999931 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I\u2019m sorry, I only talk about tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 375,
        "candidates_token_count": 12,
        "total_token_count": 387
      }
    }),
)
2025-04-24 11:24:27.921700 - Text query received: tell me about the rust fungus...
2025-04-24 11:24:27.922693 - Processing request after 0.00s in queue
2025-04-24 11:24:28.517942 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I\u2019m sorry, I only talk about tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 393,
        "candidates_token_count": 12,
        "total_token_count": 405
      }
    }),
)
2025-04-24 11:24:47.263475 - Text query received: tell me about splitting in tom...
2025-04-24 11:24:47.264474 - Processing request after 0.00s in queue
2025-04-24 11:24:48.793329 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Splitting in tomatoes refers to cracks that appear on the fruit, usually around the stem end, but sometimes vertically down the sides.\n\n*   **Possible Reasons:** Splitting is typically caused by inconsistent watering, rapid changes in soil moisture, or over-fertilization. These conditions lead to a sudden increase in water uptake, causing the fruit to expand faster than the skin can accommodate. Temperature fluctuations can also contribute.\n\n*   **Suggested Solutions:**\n    *   Maintain consistent watering.\n    *   Use mulch to regulate soil moisture.\n    *   Avoid over-fertilizing, especially with nitrogen.\n    *   Harvest fruits as they ripen to prevent them from becoming overripe and prone to splitting.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 411,
        "candidates_token_count": 146,
        "total_token_count": 557
      }
    }),
)
2025-04-24 11:25:31.232086 - Text query received: tell me about how to properly ...
2025-04-24 11:25:31.232989 - Processing request after 0.00s in queue
2025-04-24 11:25:33.141645 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "*   **Soil Testing:** Before planting, test your soil to determine its nutrient levels. This will help you choose the right type and amount of fertilizer.\n*   **Nutrient Needs:** Tomatoes are heavy feeders, requiring nitrogen (N), phosphorus (P), and potassium (K), as well as micronutrients like calcium and magnesium.\n*   **Fertilizer Types:**\n    *   **Organic:** Compost, aged manure, bone meal, and rock phosphate are good organic options.\n    *   **Inorganic:** Balanced fertilizers like 10-10-10 or tomato-specific fertilizers are effective.\n*   **Application:**\n    *   **At Planting:** Mix compost or a balanced fertilizer into the planting hole.\n    *   **Side-Dressing:** Apply fertilizer along the sides of the plants a few weeks after transplanting and again when the first fruits set.\n*   **Watering:** Water thoroughly after fertilizing to help the nutrients reach the roots.\n*   **Avoid Over-Fertilizing:** Too much fertilizer can lead to excessive foliage growth and reduced fruit production.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 570,
        "candidates_token_count": 226,
        "total_token_count": 796
      }
    }),
)
2025-04-24 11:28:43.735886 - Text query received: tell me about splitting in tom...
2025-04-24 11:28:43.736885 - Processing request after 0.00s in queue
2025-04-24 11:28:51.205169 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Okay, I understand. Let's talk tomatoes!\n\nHere's information about splitting in tomatoes and fertilizer use:\n\n**Tomato Splitting (Cracking):**\n\n*   **What it is:** Tomato splitting, also known as cracking, is when the skin of the tomato fruits breaks or cracks. This is a common problem, especially after periods of heavy rain or inconsistent watering.\n\n*   **Types of Splits:**\n\n    *   **Radial Cracks:** These cracks radiate outward from the stem end of the tomato.\n    *   **Concentric Cracks:** These cracks form circles around the stem end.\n\n*   **Causes:**\n\n    *   **Inconsistent Watering:** The primary cause. When the soil is dry for an extended period and then suddenly receives a large amount of water (from rain or irrigation), the tomato fruit rapidly expands. The skin can't keep up with the growth, causing it to split.\n    *   **Temperature Fluctuations:** Sudden changes in temperature can also contribute to splitting.\n    *   **Variety:** Some tomato varieties are more prone to splitting than others. Varieties with thinner skins are generally more susceptible.\n    *   **Over-Fertilization:** Excessive nitrogen can lead to rapid, uneven growth, increasing the risk of splitting.\n    *   **Fruit Maturity:** Overripe tomatoes are more prone to cracking.\n\n*   **Prevention:**\n\n    *   **Consistent Watering:** Maintain even soil moisture. Water deeply and regularly, especially during dry spells. Use a soaker hose or drip irrigation to deliver water directly to the roots.\n    *   **Mulching:** Apply a layer of mulch (straw, wood chips, etc.) around the plants to help retain soil moisture and regulate soil temperature.\n    *   **Choose Crack-Resistant Varieties:** When selecting tomato varieties, look for those that are known to be crack-resistant.\n    *   **Harvest Promptly:** Harvest tomatoes as soon as they are ripe to prevent overripening and cracking.\n    *   **Provide Shade:** In very hot climates, providing some afternoon shade can help reduce temperature stress.\n\n*   **Are Split Tomatoes Safe to Eat?** Yes, split tomatoes are generally safe to eat, as long as the inside of the fruit looks and smells normal. Cut away any damaged or discolored areas before eating. However, split tomatoes are more susceptible to rot and pests, so use them quickly.\n\n**Fertilizer Use for Tomatoes:**\n\n*   **Soil Testing:** Ideally, before planting, get your soil tested. This will provide valuable information about the nutrient levels in your soil and help you determine what type and amount of fertilizer you need.\n\n*   **Types of Fertilizer:**\n\n    *   **Granular Fertilizer:** Slow-release fertilizers that are mixed into the soil before planting or side-dressed during the growing season.\n    *   **Liquid Fertilizer:** Water-soluble fertilizers that are diluted and applied to the soil or foliage. These provide a quick boost of nutrients.\n    *   **Organic Amendments:** Compost, aged manure, and other organic materials can improve soil fertility and structure.\n\n*   **Fertilizer Numbers (N-P-K):** The numbers on a fertilizer label represent the percentage of nitrogen (N), phosphorus (P), and potassium (K) in the fertilizer. Tomatoes generally benefit from a balanced fertilizer, especially early in the season.\n\n*   **Fertilizing at Planting:**\n\n    *   Amend the soil with compost or aged manure.\n    *   Use a starter fertilizer high in phosphorus (the middle number in the N-P-K ratio) to promote root development. Examples: 10-20-10 or 15-30-15. Follow label instructions for application rates.\n\n*   **Side-Dressing:**\n\n    *   Side-dressing is applying fertilizer to the soil around the plants during the growing season.\n    *   Apply a balanced fertilizer (e.g., 10-10-10 or 14-14-14) every 4-6 weeks.\n    *   Alternatively, use a fertilizer specifically formulated for tomatoes.\n    *   Avoid getting fertilizer on the foliage. Water the fertilizer into the soil after application.\n\n*   **Foliar Feeding:**\n\n    *   Foliar feeding involves spraying a diluted fertilizer solution directly onto the leaves. This can be a quick way to address nutrient deficiencies.\n    *   Use a fertilizer specifically designed for foliar feeding.\n    *   Apply in the early morning or late evening to avoid leaf burn.\n\n*   **Important Considerations:**\n\n    *   **Don't Over-Fertilize:** Too much fertilizer can lead to excessive vegetative growth (lots of leaves, but few fruits) and can also increase the risk of blossom-end rot.\n    *   **Water After Fertilizing:** Watering after applying fertilizer helps to dissolve the fertilizer and move it into the soil where the roots can access it.\n    *   **Observe Your Plants:** Pay attention to the appearance of your tomato plants. Yellowing leaves, stunted growth, or other signs of nutrient deficiency can indicate that they need more fertilizer.\n    *   **Adjust Based on Soil Test:** The best approach is always to adjust your fertilization plan based on the results of a soil test.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 192,
        "candidates_token_count": 1102,
        "total_token_count": 1294
      }
    }),
)
2025-04-24 11:30:40.553933 - Text query received: tell me about the main nutrien...
2025-04-24 11:30:40.553933 - Processing request after 0.00s in queue
2025-04-24 11:30:41.895740 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Tomatoes need several key nutrients to thrive. Here's a breakdown of the main ones:\n\n*   **Nitrogen (N):** Essential for leafy growth and overall plant vigor. However, too much nitrogen can lead to excessive foliage at the expense of fruit production.\n*   **Phosphorus (P):** Crucial for root development, flowering, and fruit set. It helps the plant convert sunlight into energy.\n*   **Potassium (K):** Important for fruit development, ripening, and disease resistance. It also helps regulate water uptake.\n*   **Calcium (Ca):** Prevents blossom-end rot, a common problem in tomatoes. Calcium is essential for cell wall development.\n*   **Magnesium (Mg):** Plays a role in chlorophyll production, which is necessary for photosynthesis. A deficiency can cause yellowing of the leaves.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 808,
        "candidates_token_count": 172,
        "total_token_count": 980
      }
    }),
)
2025-04-24 14:07:44.678386 - Text query received: oye...
2025-04-24 14:07:44.680331 - Processing request after 0.00s in queue
2025-04-24 14:07:46.173545 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-04-24 14:07:53.576734 - Text query received: hi...
2025-04-24 14:07:53.577718 - Processing request after 0.00s in queue
2025-04-24 14:07:54.258711 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 180,
        "candidates_token_count": 9,
        "total_token_count": 189
      }
    }),
)
2025-04-24 14:08:12.759311 - Image query received: ...
2025-04-24 14:08:12.759824 - Processing request after 0.00s in queue
2025-04-24 14:08:14.701298 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The tomato has Blossom end rot.\n\n1.  **Description:** Blossom end rot is a physiological disorder that appears as a dark, sunken, leathery spot on the blossom end of the tomato.\n\n2.  **Possible Reasons:** It's primarily caused by calcium deficiency in the developing fruit, often due to:\n    *   Inconsistent watering (leading to fluctuations in calcium uptake).\n    *   Low calcium levels in the soil.\n    *   Root damage.\n    *   High levels of nitrogen or salts in the soil, which can interfere with calcium uptake.\n\n3.  **Suggested Solutions:**\n    *   Ensure consistent watering to avoid fluctuations in soil moisture.\n    *   Test your soil and amend with calcium if necessary (e.g., add lime or gypsum).\n    *   Avoid over-fertilizing with nitrogen.\n    *   Mulch around plants to help retain soil moisture.\n    *   Foliar sprays containing calcium can provide a temporary boost, but addressing the underlying soil issues is crucial.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 398,
        "candidates_token_count": 216,
        "total_token_count": 614
      }
    }),
)
2025-04-24 14:13:27.415121 - Image query received: is this plant damaged?...
2025-04-24 14:13:27.415635 - Processing request after 0.00s in queue
2025-04-24 14:13:28.845215 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Apologies, I cannot detect any disease in this photo.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 381,
        "candidates_token_count": 12,
        "total_token_count": 393
      }
    }),
)
2025-04-24 14:15:42.477644 - Image query received: is this bell pepper damaged?...
2025-04-24 14:15:42.477644 - Processing request after 0.00s in queue
2025-04-24 14:15:43.223836 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Apologies, I cannot detect any disease in this photo.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 400,
        "candidates_token_count": 12,
        "total_token_count": 412
      }
    }),
)
2025-04-24 14:17:09.822210 - Image query received: are these leaves damaged?...
2025-04-24 14:17:09.823221 - Processing request after 0.00s in queue
2025-04-24 14:17:10.579995 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Apologies, I cannot detect any disease in this photo.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 416,
        "candidates_token_count": 12,
        "total_token_count": 428
      }
    }),
)
2025-04-24 14:17:52.031637 - Image query received: what plant is this...
2025-04-24 14:17:52.032245 - Processing request after 0.00s in queue
2025-04-24 14:17:52.764867 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Apologies, I cannot detect any disease in this photo.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 432,
        "candidates_token_count": 12,
        "total_token_count": 444
      }
    }),
)
2025-04-24 14:19:41.329296 - Image query received: are these tomato damaged?...
2025-04-24 14:19:41.329817 - Processing request after 0.00s in queue
2025-04-24 14:19:42.340554 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Apologies, I cannot detect any disease in this photo.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 449,
        "candidates_token_count": 12,
        "total_token_count": 461
      }
    }),
)
2025-04-24 14:20:33.855236 - Image query received: is this tomato damaged?...
2025-04-24 14:20:33.855792 - Processing request after 0.00s in queue
2025-04-24 14:20:34.508509 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Apologies, I cannot detect any disease in this photo.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 460,
        "candidates_token_count": 12,
        "total_token_count": 472
      }
    }),
)
2025-04-24 14:21:11.025849 - Image query received: what is wrong here...
2025-04-24 14:21:11.026368 - Processing request after 0.00s in queue
2025-04-24 14:21:12.624081 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The tomato has Blossom end rot.\n\n1.  Blossom end rot is a physiological disorder in tomatoes (and other fruits like peppers and watermelons) caused by a calcium deficiency at the blossom end of the fruit. It appears as a dark, leathery, sunken spot.\n2.  Possible reasons for the disease:\n *   Calcium deficiency in the soil.\n *   Inconsistent watering, leading to the plant's inability to absorb calcium efficiently.\n *   Root damage.\n *   High nitrogen levels in the soil.\n3.  Suggested solutions to mitigate or cure the disease:\n *   Ensure consistent watering.\n *   Add calcium to the soil (if a soil test indicates a deficiency).\n *   Avoid over-fertilizing with nitrogen.\n *   Mulch around plants to help retain soil moisture.\n *   Remove affected fruits to encourage the plant to produce healthy tomatoes.\n\nwhat is wrong here\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 487,
        "candidates_token_count": 193,
        "total_token_count": 680
      }
    }),
)
2025-04-24 14:22:42.156920 - Text query received: how many tomatos do i need to ...
2025-04-24 14:22:42.157975 - Processing request after 0.00s in queue
2025-04-24 14:22:43.262501 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "It depends on how much tomato sauce you want to make and the size of your tomatoes. However, a general estimate is:\n\n*   **For a quart of tomato sauce:** You'll likely need around 20-25 medium-sized tomatoes.\n*   **For a smaller batch (about 2 cups):** Aim for 10-12 medium tomatoes.\n\nKeep in mind that the exact number can vary based on the tomato variety (some are meatier than others) and how thick you want your sauce.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 480,
        "candidates_token_count": 111,
        "total_token_count": 591
      }
    }),
)
2025-04-24 14:23:20.139002 - Text query received: can you tell me the steps requ...
2025-04-24 14:23:20.139941 - Processing request after 0.00s in queue
2025-04-24 14:23:22.809250 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Okay, here are the steps required to make tomato sauce:\n\n*   **Wash and Prep the Tomatoes:** Thoroughly wash your tomatoes. You can remove the skins for a smoother sauce (see instructions below), or leave them on for a more rustic sauce. Core the tomatoes and roughly chop them.\n\n    *   *To remove skins (optional):* Score an \"X\" on the bottom of each tomato. Briefly boil the tomatoes (1-2 minutes) until the skins start to peel back. Immediately transfer them to an ice bath to stop the cooking process. The skins should now easily slip off.\n*   **Saut\u00e9 Aromatics (Optional):** In a large pot or Dutch oven, heat some olive oil over medium heat. Add chopped onions, garlic, and any other aromatics you like (such as carrots, celery, or herbs). Saut\u00e9 until softened and fragrant.\n*   **Cook the Tomatoes:** Add the chopped tomatoes to the pot. Bring to a simmer, then reduce the heat to low. Cook, uncovered, for at least 1-2 hours, or until the tomatoes have broken down and the sauce has thickened to your desired consistency. Stir occasionally to prevent sticking.\n*   **Season:** Season the sauce with salt, pepper, sugar (if needed to balance acidity), and any other herbs or spices you prefer (such as basil, oregano, thyme, or red pepper flakes).\n*   **Blend (Optional):** For a smoother sauce, use an immersion blender to blend the sauce directly in the pot. Alternatively, you can carefully transfer the sauce to a regular blender in batches.\n*   **Adjust and Simmer:** Taste the sauce and adjust seasonings as needed. Simmer for another 15-30 minutes to allow the flavors to meld.\n*   **Cool and Store:** Allow the sauce to cool completely before storing it in airtight containers in the refrigerator or freezer.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 603,
        "candidates_token_count": 394,
        "total_token_count": 997
      }
    }),
)
2025-04-24 14:24:01.480516 - Text query received: could you tell me how i can di...
2025-04-24 14:24:01.482423 - Processing request after 0.00s in queue
2025-04-24 14:24:01.986787 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I\u2019m sorry, I only talk about tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 1014,
        "candidates_token_count": 12,
        "total_token_count": 1026
      }
    }),
)
2025-04-24 14:24:56.282966 - Image query received: ...
2025-04-24 14:24:56.284112 - Processing request after 0.00s in queue
2025-04-24 14:24:56.860714 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Apologies, I cannot detect any disease in this photo.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 1230,
        "candidates_token_count": 12,
        "total_token_count": 1242
      }
    }),
)
2025-04-24 14:33:35.441516 - Image query received: what diseses is there?...
2025-04-24 14:33:35.442904 - Processing request after 0.00s in queue
2025-04-24 14:33:37.025342 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Apologies, I cannot detect any disease in this photo.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 377,
        "candidates_token_count": 12,
        "total_token_count": 389
      }
    }),
)
2025-04-24 14:34:16.727046 - Image query received: diseases?...
2025-04-24 14:34:16.727690 - Processing request after 0.00s in queue
2025-04-24 14:34:17.680483 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Apologies, I cannot detect any disease in this photo.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 398,
        "candidates_token_count": 12,
        "total_token_count": 410
      }
    }),
)
2025-04-24 14:35:18.941437 - Text query received: what are the major plant disea...
2025-04-24 14:35:18.942435 - Processing request after 0.00s in queue
2025-04-24 14:35:19.961749 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I\u2019m sorry, I only talk about tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 208,
        "candidates_token_count": 12,
        "total_token_count": 220
      }
    }),
)
2025-04-24 14:36:24.819828 - Text query received: i have some plant and there le...
2025-04-24 14:36:24.820724 - Processing request after 0.00s in queue
2025-04-24 14:36:26.197255 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Yellowing leaves on a tomato plant can be caused by several things. Here are some possibilities:\n\n*   **Nutrient deficiencies:** Lack of nitrogen, magnesium, or iron can cause yellowing.\n*   **Watering issues:** Overwatering or underwatering can stress the plant.\n*   **Disease:** Several diseases, like early blight or Septoria leaf spot, can cause yellowing.\n*   **Pests:** Certain pests can damage leaves and cause them to yellow.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 233,
        "candidates_token_count": 98,
        "total_token_count": 331
      }
    }),
)
2025-04-24 14:36:57.444826 - Text query received: hi. whats is up?...
2025-04-24 14:36:57.446728 - Processing request after 0.00s in queue
2025-04-24 14:36:58.387098 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 175,
        "candidates_token_count": 9,
        "total_token_count": 184
      }
    }),
)
2025-04-24 14:38:14.258077 - Text query received: i have some plant and there le...
2025-04-24 14:38:14.259076 - Processing request after 0.00s in queue
2025-04-24 14:38:14.877514 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I\u2019m sorry, I only talk about tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 197,
        "candidates_token_count": 12,
        "total_token_count": 209
      }
    }),
)
2025-04-24 14:41:14.711049 - Text query received: what is called pink spot on th...
2025-04-24 14:41:14.713038 - Processing request after 0.00s in queue
2025-04-24 14:41:17.424738 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on your description of \"pink spot on tomato leaves,\" it's most likely referring to **Edema** (also sometimes spelled Oedema).\n\nHere's what you should know about Edema in tomatoes:\n\n*   **Cause:** Edema is a physiological disorder, *not* a disease caused by a pathogen. It occurs when tomato plants take up more water than they can transpire (release through their leaves). This excess water causes cells to rupture, leading to the formation of blisters or swellings.\n\n*   **Appearance:** These blisters often appear as small, raised bumps or spots, typically on the undersides of leaves, but can also occur on stems and petioles. They often start out watery or translucent, and can turn pinkish, tan, or brown as they age and the cells die.\n\n*   **Conditions:** Edema is most common in cool, humid conditions, especially when soil is consistently wet and air circulation is poor. Overwatering and poor drainage can exacerbate the problem.\n\n*   **Impact:** While edema can look alarming, it's generally not a serious threat to the overall health or yield of your tomato plants.\n\n*   **Management:**\n    *   Improve drainage: Ensure your pots or garden beds have good drainage.\n    *   Water properly: Water deeply but less frequently, allowing the soil to dry out slightly between waterings.\n    *   Improve air circulation: Space plants adequately to allow for good airflow. You can also use a fan to improve air circulation in enclosed spaces like greenhouses.\n    *   Manage humidity: If possible, reduce humidity levels in your growing environment.\n    *   Avoid over-fertilizing: Excessive fertilization can contribute to edema.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 179,
        "candidates_token_count": 353,
        "total_token_count": 532
      }
    }),
)
2025-04-24 20:54:32.996950 - Text query received: my tomato has black spots why ...
2025-04-24 20:54:33.002049 - Processing request after 0.00s in queue
2025-04-24 20:54:35.550944 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The black spots on your tomato are likely due to one of a few common problems. Here's a breakdown of the possibilities:\n\n*   **Blossom End Rot (BER):** Despite the name, BER can sometimes appear as black, sunken spots on the *side* or bottom of the tomato, not just the blossom end. It's caused by a calcium deficiency, usually due to inconsistent watering.\n\n*   **Septoria Leaf Spot:** While primarily a leaf disease, in severe cases, Septoria can affect the fruit, causing small, dark spots.\n\n*   **Anthracnose:** This fungal disease causes small, circular, sunken spots that can be black or dark brown. These spots often enlarge and merge.\n\n*   **Bacterial Speck/Spot:** These diseases cause small, dark, slightly raised spots on the fruit. They're often surrounded by a yellow halo.\n\nTo get a more accurate diagnosis, consider these factors:\n\n*   **Location of the spots:** Are they on the leaves, stem, or only the fruit?\n*   **Appearance of the spots:** Are they sunken, raised, or flat? Are they dry or mushy?\n*   **Size and shape of the spots:** Are they small and circular, or large and irregular?\n*   **Presence of yellow halos:** Are the spots surrounded by yellowing?\n\nProviding more detail can help narrow down the cause and suggest appropriate treatment!\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 178,
        "candidates_token_count": 295,
        "total_token_count": 473
      }
    }),
)
2025-04-24 20:54:54.169786 - Text query received: my potato too has black spots...
2025-04-24 20:54:54.170293 - Processing request after 0.00s in queue
2025-04-24 20:54:54.803847 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I\u2019m sorry, I only talk about tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 479,
        "candidates_token_count": 12,
        "total_token_count": 491
      }
    }),
)
2025-04-24 20:55:41.301906 - Text query received: why?...
2025-04-24 20:55:41.301906 - Processing request after 0.00s in queue
2025-04-24 20:55:41.961005 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Understood. I will only provide information about tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 493,
        "candidates_token_count": 11,
        "total_token_count": 504
      }
    }),
)
2025-04-24 20:56:00.839809 - Text query received: what is yellow halos?...
2025-04-24 20:56:00.840804 - Processing request after 0.00s in queue
2025-04-24 20:56:01.777076 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "In the context of tomato diseases, a yellow halo refers to a yellowish discoloration that appears as a ring or border around a dark spot on the fruit or leaves. The presence of a yellow halo is often a characteristic symptom of certain bacterial or fungal infections.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 509,
        "candidates_token_count": 51,
        "total_token_count": 560
      }
    }),
)
2025-04-24 21:10:30.488042 - Image query received: ...
2025-04-24 21:10:30.493654 - Processing request after 0.00s in queue
2025-04-24 21:10:33.117036 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Okay, I understand. Let's proceed.\n\nBased on the label *rust leaf*, the *Apple leaf* has *rust leaf* disease.\n\n1.  *Rust leaf* is a fungal disease that affects various plants, causing reddish-brown or orange pustules on the leaves.\n\n2.  Possible reasons for *rust leaf* disease:\n    *   High humidity and moisture\n    *   Poor air circulation\n    *   Overhead watering\n    *   Infected plant debris\n    *   Presence of alternate hosts (for some rust fungi)\n\n3.  Suggested solutions to mitigate or cure *rust leaf* disease:\n    *   Remove and destroy infected leaves.\n    *   Improve air circulation around the plant.\n    *   Avoid overhead watering.\n    *   Apply a fungicide containing copper, sulfur, or neem oil.\n    *   Clean up plant debris around the plant.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 373,
        "candidates_token_count": 191,
        "total_token_count": 564
      }
    }),
)
2025-04-24 21:15:04.227407 - Image query received: any disease here?...
2025-04-24 21:15:04.227919 - Processing request after 0.00s in queue
2025-04-24 21:15:05.638157 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "This *tomato* is healthy and it has no disease.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 565,
        "candidates_token_count": 13,
        "total_token_count": 578
      }
    }),
)
2025-04-24 21:15:31.184698 - Image query received: ...
2025-04-24 21:15:31.185680 - Processing request after 0.00s in queue
2025-04-24 21:15:31.760215 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "This *tomato* is healthy and it has no disease.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 576,
        "candidates_token_count": 13,
        "total_token_count": 589
      }
    }),
)
2025-04-24 21:16:10.721598 - Image query received: ...
2025-04-24 21:16:10.722134 - Processing request after 0.00s in queue
2025-04-24 21:16:11.953585 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The *tomato* has *Blossom end rot* disease.\n\n1.  *Blossom end rot* is a physiological disorder that affects tomatoes, characterized by a dark, sunken, leathery spot on the blossom end of the fruit.\n\n2.  Possible reasons for *Blossom end rot*:\n    *   Calcium deficiency in the fruit\n    *   Inconsistent watering\n    *   Root damage\n    *   High nitrogen levels in the soil\n    *   High salt levels in the soil\n\n3.  Suggested solutions to mitigate or cure *Blossom end rot*:\n    *   Ensure consistent watering.\n    *   Add calcium to the soil (e.g., bone meal, gypsum).\n    *   Avoid over-fertilizing with nitrogen.\n    *   Mulch around the plants to retain moisture.\n    *   Test soil pH and adjust if necessary.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 601,
        "candidates_token_count": 186,
        "total_token_count": 787
      }
    }),
)
2025-04-24 21:18:03.676815 - Image query received: ...
2025-04-24 21:18:03.677874 - Processing request after 0.00s in queue
2025-04-24 21:18:05.389720 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Okay, I understand. I will follow these guidelines for diagnosing and responding to queries about tomato diseases, specifically *Blossom end rot*.\n\nGiven the label *Blossom end rot*, the *tomato* has *Blossom end rot* disease.\n\n1.  *Blossom end rot* is a physiological disorder that affects tomatoes, characterized by a dark, sunken, leathery spot on the blossom end of the fruit.\n\n2.  Possible reasons for *Blossom end rot*:\n    *   Calcium deficiency in the fruit\n    *   Inconsistent watering\n    *   Root damage\n    *   High nitrogen levels in the soil\n    *   High salt levels in the soil\n\n3.  Suggested solutions to mitigate or cure *Blossom end rot*:\n    *   Ensure consistent watering.\n    *   Add calcium to the soil (e.g., bone meal, gypsum).\n    *   Avoid over-fertilizing with nitrogen.\n    *   Mulch around the plants to retain moisture.\n    *   Test soil pH and adjust if necessary.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 788,
        "candidates_token_count": 223,
        "total_token_count": 1011
      }
    }),
)
2025-04-24 22:13:07.764690 - Text query received: oh wow...
2025-04-24 22:13:07.765691 - Processing request after 0.00s in queue
2025-04-24 22:13:09.211790 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Understood. I will only provide information about tomatoes and follow the specified guidelines for greetings and unrelated queries.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 805,
        "candidates_token_count": 21,
        "total_token_count": 826
      }
    }),
)
2025-04-24 23:35:47.288775 - Text query received: hello...
2025-04-24 23:35:47.289773 - Processing request after 0.00s in queue
2025-04-24 23:35:48.438524 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-04-24 23:36:11.703488 - Image query received: what is this plant?...
2025-04-24 23:36:11.704029 - Processing request after 0.00s in queue
2025-04-24 23:36:12.178372 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "This is a **tomato**.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 473,
        "candidates_token_count": 7,
        "total_token_count": 480
      }
    }),
)
2025-04-24 23:36:19.788289 - Text query received: what disease it has?...
2025-04-24 23:36:19.789292 - Processing request after 0.00s in queue
2025-04-24 23:36:21.371818 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Based on the image, the tomato plant appears to have **blossom-end rot**.\n\nHere's why:\n\n*   **Dark, sunken spot:** The most obvious sign is the dark, leathery, sunken spot on the bottom (blossom end) of the tomato.\n\nBlossom-end rot is *not* a disease caused by a pathogen, but a **physiological disorder** caused by calcium deficiency in the developing fruit. Here are the common causes:\n\n*   **Inconsistent watering:** Fluctuations in soil moisture (too dry or too wet) can prevent the plant from properly absorbing calcium.\n*   **Calcium deficiency in the soil:** Although less common, the soil may actually be lacking in calcium.\n*   **Root damage:** Damaged roots can't absorb nutrients efficiently.\n*   **High nitrogen levels:** Excessive nitrogen fertilization can interfere with calcium uptake.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 196,
        "candidates_token_count": 180,
        "total_token_count": 376
      }
    }),
)
2025-04-24 23:36:42.417106 - Text query received: any solution for this...
2025-04-24 23:36:42.417106 - Processing request after 0.00s in queue
2025-04-24 23:36:44.515014 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Here are some solutions for blossom-end rot in tomatoes:\n\n*   **Water consistently:** Maintain even soil moisture. Avoid letting the soil dry out completely, and avoid overwatering. Use mulch to help retain moisture.\n\n*   **Soil test:** Get your soil tested to determine its calcium levels and pH. Tomatoes prefer a slightly acidic soil (pH 6.0-6.8).\n\n*   **Add calcium (if needed):** If the soil test indicates a calcium deficiency, amend the soil with a calcium source like:\n    *   **Agricultural lime:** Apply before planting, following package instructions.\n    *   **Gypsum:** Can be applied at planting or during the growing season.\n    *   **Calcium foliar sprays:** These can provide a quick boost of calcium directly to the plant, but they are not a long-term solution.\n\n*   **Improve drainage:** If your soil is poorly draining, amend it with organic matter like compost to improve drainage.\n\n*   **Avoid excessive nitrogen fertilization:** Use a balanced fertilizer formulated for tomatoes.\n\n*   **Remove affected fruit:** Remove tomatoes that already have blossom-end rot. This will encourage the plant to put its energy into producing healthy fruit.\n\n*   **Protect roots:** Avoid damaging the roots when weeding or cultivating around the plants.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 380,
        "candidates_token_count": 270,
        "total_token_count": 650
      }
    }),
)
2025-04-24 23:38:06.090268 - Text query received: what do you mean by protect ro...
2025-04-24 23:38:06.090268 - Processing request after 0.00s in queue
2025-04-24 23:38:08.173037 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "By \"protect roots,\" I mean taking steps to avoid damaging the tomato plant's root system. Here's why it's important and how to do it:\n\n*   **Why protect roots?** The roots are responsible for absorbing water and nutrients, including calcium. Damaged roots can't do this effectively, which can contribute to blossom-end rot and other problems.\n\n*   **How to protect roots:**\n\n    *   **Careful weeding:** When weeding around your tomato plants, avoid using sharp tools that could cut or damage the roots. Hand-pull weeds gently.\n    *   **Avoid deep cultivation:** Don't dig too deeply near the plants when cultivating the soil. The roots are often close to the surface.\n    *   **Mulch:** Mulch helps retain moisture and suppress weeds, reducing the need for cultivation and weeding.\n    *   **Water deeply and less frequently:** This encourages deeper root growth, making the plants more resilient.\n    *   **Avoid transplant shock:** When transplanting seedlings, handle them carefully to minimize root disturbance.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 658,
        "candidates_token_count": 221,
        "total_token_count": 879
      }
    }),
)
2025-04-24 23:43:53.032609 - Text query received: I can make paste from tomato?...
2025-04-24 23:43:53.033116 - Processing request after 0.00s in queue
2025-04-24 23:43:54.572404 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Yes, you can definitely make paste from tomatoes! It's a great way to preserve them and use them in sauces, soups, and stews.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 886,
        "candidates_token_count": 32,
        "total_token_count": 918
      }
    }),
)
2025-04-24 23:44:09.986734 - Text query received: the above tomato, can I eat it...
2025-04-24 23:44:09.987745 - Processing request after 0.00s in queue
2025-04-24 23:44:11.683490 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Regarding the tomato with blossom-end rot, it's generally **not recommended** to eat the affected portion. Here's why:\n\n*   **Taste and Texture:** The dark, leathery area is likely to be very bitter and have an unpleasant texture.\n*   **Potential for Secondary Issues:** While blossom-end rot itself is not caused by a pathogen, the damaged area can become susceptible to secondary fungal or bacterial infections.\n\n**However:**\n\n*   **If the rot is very small and localized:** You *could* cut away the affected area, making sure to remove a generous margin around it, and eat the rest of the tomato if it looks and smells normal.\n*   **Use your judgment:** If the rot is extensive, or if the tomato smells off or shows signs of mold, it's best to discard the entire fruit.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 927,
        "candidates_token_count": 177,
        "total_token_count": 1104
      }
    }),
)
2025-04-24 23:45:55.276367 - Image query received: ...
2025-04-24 23:45:55.277560 - Processing request after 0.00s in queue
2025-04-24 23:45:56.553368 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Apple leaf** has the **rust leaf** disease.\n\n**Rust leaf** appears as small, orange or reddish-brown pustules on the leaves.\n\nPossible reasons: Wet and humid conditions favor the spread of rust fungi.\n\nSuggested solutions: Remove and destroy infected leaves, improve air circulation, and apply a fungicide.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 1387,
        "candidates_token_count": 69,
        "total_token_count": 1456
      }
    }),
)
2025-04-24 23:46:20.610931 - Text query received: is this concerning?...
2025-04-24 23:46:20.610931 - Processing request after 0.00s in queue
2025-04-24 23:46:21.251619 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I\u2019m sorry, I only talk about tomatoes.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 1178,
        "candidates_token_count": 12,
        "total_token_count": 1190
      }
    }),
)
2025-04-24 23:47:37.982254 - Image query received: any disease here?...
2025-04-24 23:47:37.982766 - Processing request after 0.00s in queue
2025-04-24 23:47:38.636626 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **tomato** is healthy and has no disease.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 1470,
        "candidates_token_count": 12,
        "total_token_count": 1482
      }
    }),
)
2025-04-24 23:47:46.728873 - Text query received: so I can eat it?...
2025-04-24 23:47:46.729875 - Processing request after 0.00s in queue
2025-04-24 23:47:47.333460 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Yes, you can eat it.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 1212,
        "candidates_token_count": 8,
        "total_token_count": 1220
      }
    }),
)
2025-04-25 00:12:12.342712 - Text query received: what was the leaf I provided e...
2025-04-25 00:12:12.344377 - Processing request after 0.00s in queue
2025-04-25 00:12:13.288829 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I\u2019m sorry, I only talk about plants.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 1228,
        "candidates_token_count": 12,
        "total_token_count": 1240
      }
    }),
)
2025-04-25 00:16:19.018490 - Text query received: what was the leaf I provided e...
2025-04-25 00:16:19.019493 - Processing request after 0.00s in queue
2025-04-25 00:16:19.820608 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I'm sorry, I only talk about plants.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 1248,
        "candidates_token_count": 12,
        "total_token_count": 1260
      }
    }),
)
2025-04-25 00:16:40.992112 - Image query received: ...
2025-04-25 00:16:40.992654 - Processing request after 0.00s in queue
2025-04-25 00:16:41.759743 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Apple leaf** has the **rust leaf** disease.\n\nWet and humid conditions favor the spread of rust fungi.\n\nRemove and destroy infected leaves, improve air circulation, and apply a fungicide.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 1542,
        "candidates_token_count": 43,
        "total_token_count": 1585
      }
    }),
)
2025-04-25 00:17:02.880941 - Image query received: ...
2025-04-25 00:17:02.882052 - Processing request after 0.00s in queue
2025-04-25 00:17:03.779397 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **tomato** has the **Blossom end rot** disease. This condition is characterized by a dark, sunken lesion at the blossom end of the fruit.\n\nPossible reasons:\n*   Calcium deficiency in the developing fruit.\n*   Inconsistent watering.\n*   Root damage.\n\nSuggested solutions:\n*   Ensure consistent watering practices.\n*   Apply calcium-containing foliar sprays.\n*   Improve soil drainage.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 453,
        "candidates_token_count": 89,
        "total_token_count": 542
      }
    }),
)
2025-04-25 00:17:58.171719 - Text query received: I can eat this tomato?...
2025-04-25 00:17:58.171719 - Processing request after 0.00s in queue
2025-04-25 00:17:59.013557 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The edibility of a tomato with blossom end rot depends on the extent of the damage:\n\n*   **Small, localized rot:** If the affected area is small and you cut it away, the rest of the tomato is generally safe to eat.\n*   **Extensive rot or mold:** If the rot is widespread or there's mold present, it's best to discard the entire tomato.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 265,
        "candidates_token_count": 82,
        "total_token_count": 347
      }
    }),
)
2025-04-25 00:18:24.566416 - Text query received: spot is very big so I cannot e...
2025-04-25 00:18:24.567427 - Processing request after 0.00s in queue
2025-04-25 00:18:25.157940 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Given that the spot is very big, it is best to discard the tomato.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 357,
        "candidates_token_count": 17,
        "total_token_count": 374
      }
    }),
)
2025-04-25 00:18:44.276305 - Image query received: ...
2025-04-25 00:18:44.277355 - Processing request after 0.00s in queue
2025-04-25 00:18:45.443849 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Apple leaf** has the **rust leaf** disease. This disease is characterized by orange or reddish-brown spots on the leaves.\n\nPossible reasons:\n*   Fungal infection.\n*   Presence of alternate hosts like juniper trees nearby.\n*   Wet and humid conditions.\n\nSuggested solutions:\n*   Remove and destroy infected leaves.\n*   Apply fungicide treatments.\n*   Remove nearby juniper trees to break the disease cycle.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 656,
        "candidates_token_count": 94,
        "total_token_count": 750
      }
    }),
)
2025-04-25 00:24:08.759841 - Text query received: is it not apple scab?...
2025-04-25 00:24:08.760843 - Processing request after 0.00s in queue
2025-04-25 00:24:10.827916 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "You're right! It could also be **apple scab**. Apple scab also presents with spots on the leaves, but they are typically olive-green to brown and can become velvety.\n\nTo differentiate:\n\n*   **Apple Rust:** Look for bright orange or reddish-brown spots. Often raised or bumpy. Requires a juniper host nearby.\n*   **Apple Scab:** Look for olive-green to brown, velvety spots. Can affect fruit as well as leaves.\n\nIf the spots are more olive-green/brown and velvety, then it's more likely to be apple scab.\n\nThe solutions for apple scab include:\n\n*   Rake and destroy fallen leaves in the autumn.\n*   Prune trees to improve air circulation.\n*   Apply fungicide treatments.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 475,
        "candidates_token_count": 162,
        "total_token_count": 637
      }
    }),
)
2025-04-25 00:51:14.669472 - Image query received: ...
2025-04-25 00:51:14.672488 - Processing request after 0.00s in queue
2025-04-25 00:51:15.864460 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **tomato** is healthy and has no disease.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 909,
        "candidates_token_count": 12,
        "total_token_count": 921
      }
    }),
)
2025-04-25 00:51:32.379411 - Text query received: what was the leaf before?...
2025-04-25 00:51:32.380512 - Processing request after 0.00s in queue
2025-04-25 00:51:32.810461 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 656,
        "candidates_token_count": 9,
        "total_token_count": 665
      }
    }),
)
2025-04-25 00:51:50.916124 - Text query received: the last image contained what ...
2025-04-25 00:51:50.917635 - Processing request after 0.00s in queue
2025-04-25 00:51:51.365025 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The last image contained an **apple leaf**.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 672,
        "candidates_token_count": 10,
        "total_token_count": 682
      }
    }),
)
2025-04-25 00:52:09.928952 - Text query received: wasn't it tomato?...
2025-04-25 00:52:09.929958 - Processing request after 0.00s in queue
2025-04-25 00:52:10.530489 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I apologize for the error. You are absolutely correct. The image before the healthy tomato was an **apple leaf** affected by, most likely, **apple rust**.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 688,
        "candidates_token_count": 34,
        "total_token_count": 722
      }
    }),
)
2025-04-25 00:52:45.186259 - Text query received: hoye mate...
2025-04-25 00:52:45.186259 - Processing request after 0.00s in queue
2025-04-25 00:52:45.630467 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 725,
        "candidates_token_count": 9,
        "total_token_count": 734
      }
    }),
)
2025-04-25 00:53:44.050315 - Text query received: hi...
2025-04-25 00:53:44.051316 - Processing request after 0.00s in queue
2025-04-25 00:53:44.625043 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-04-25 00:53:49.963952 - Text query received: hi...
2025-04-25 00:53:49.964953 - Processing request after 0.00s in queue
2025-04-25 00:53:50.535282 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-04-25 00:54:05.893526 - Text query received: hoye mate...
2025-04-25 00:54:05.894537 - Processing request after 0.00s in queue
2025-04-25 00:54:06.594992 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 172,
        "candidates_token_count": 9,
        "total_token_count": 181
      }
    }),
)
2025-04-25 00:54:16.619249 - Text query received: tell me about bell paper leave...
2025-04-25 00:54:16.620262 - Processing request after 0.00s in queue
2025-04-25 00:54:18.076474 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I can't find a plant specifically called \"bell paper leaves.\" It's possible this is a regional or common name, or perhaps a slight misspelling.\n\nTo help me give you the information you're looking for, could you provide more details? For example:\n\n*   **Do you know the scientific name?** (e.g., *Solanum lycopersicum* for tomato)\n*   **What does the plant look like?** (e.g., size, leaf shape, flower color)\n*   **Where did you hear about it?** (e.g., a gardening book, a friend)\n*   **What is it used for?** (e.g., food, medicine, decoration)\n\nWith more information, I can try to identify the plant and provide you with relevant details.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 187,
        "candidates_token_count": 175,
        "total_token_count": 362
      }
    }),
)
2025-04-25 00:54:46.539444 - Text query received: do you know about bell pepper?...
2025-04-25 00:54:46.539444 - Processing request after 0.00s in queue
2025-04-25 00:54:48.450359 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Yes, I know about bell peppers! They are a popular and versatile vegetable. Here's some information about them:\n\n*   **Botanical Name:** *Capsicum annuum*\n*   **Family:** Solanaceae (the same family as tomatoes, potatoes, and eggplants)\n*   **Description:** Bell peppers are fruits, though they are used as vegetables in cooking. They come in a variety of colors, including green, red, yellow, orange, and even purple. They have a mild, slightly sweet flavor.\n*   **Growing Conditions:**\n    *   Bell peppers need warm weather and plenty of sunshine (at least 6-8 hours per day).\n    *   They prefer well-drained soil that is rich in organic matter.\n    *   Start seeds indoors 6-8 weeks before the last expected frost.\n    *   Transplant seedlings outdoors after the danger of frost has passed.\n    *   Water regularly, especially during dry spells.\n*   **Uses:** Bell peppers are used in a wide variety of dishes, including salads, stir-fries, soups, stews, and stuffed peppers. They can be eaten raw or cooked.\n*   **Nutritional Value:** Bell peppers are a good source of vitamins A and C, as well as fiber and antioxidants.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP",
          "citation_metadata": {
            "citation_sources": [
              {
                "start_index": 658,
                "end_index": 793,
                "uri": "https://kadiyamnursery.com/products/pentas-pink-plant"
              }
            ]
          }
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 369,
        "candidates_token_count": 271,
        "total_token_count": 640
      }
    }),
)
2025-04-25 00:55:05.666880 - Text query received: tehn why don't you know about ...
2025-04-25 00:55:05.667885 - Processing request after 0.00s in queue
2025-04-25 00:55:07.346622 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "You're right to call me out on that! I apologize for the oversight. I should have been able to connect \"bell paper leaves\" to bell peppers.\n\nHere's what I can tell you about bell pepper leaves:\n\n*   **Appearance:** Bell pepper leaves are typically oval-shaped with a pointed tip. They are usually a deep green color.\n*   **Function:** Like all leaves, bell pepper leaves are responsible for photosynthesis, the process by which the plant converts sunlight into energy.\n*   **Edibility:** While technically edible, bell pepper leaves are not typically eaten. They can be bitter and don't have a particularly appealing flavor or texture.\n*   **Potential Problems:**\n    *   **Pests:** Bell pepper leaves can be susceptible to various pests, such as aphids, flea beetles, and spider mites.\n    *   **Diseases:** They can also be affected by diseases like bacterial leaf spot and powdery mildew.\n    *   **Nutrient Deficiencies:** The appearance of the leaves can indicate nutrient deficiencies in the plant. For example, yellowing leaves might indicate a nitrogen deficiency.\n\nIs there anything else you'd like to know about bell pepper leaves?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 652,
        "candidates_token_count": 247,
        "total_token_count": 899
      }
    }),
)
2025-04-25 00:55:41.688053 - Text query received: sorry for mispronouncing the b...
2025-04-25 00:55:41.689309 - Processing request after 0.00s in queue
2025-04-25 00:55:42.231382 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "No worries at all! I'm glad I could help. Is there anything else I can tell you about bell peppers or any other plants?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 907,
        "candidates_token_count": 30,
        "total_token_count": 937
      }
    }),
)
2025-04-25 00:56:37.586441 - Image query received: can I eat this tomato?...
2025-04-25 00:56:37.586976 - Processing request after 0.00s in queue
2025-04-25 00:56:38.531484 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **tomato** has the **Blossom end rot** disease.\n\n**Blossom end rot** is a condition caused by calcium deficiency at the blossom end of the fruit, leading to a dark, sunken spot.\n\nPossible reasons:\n*   Inconsistent watering.\n*   Calcium deficiency in the soil.\n*   Root damage.\n\nSuggested solutions:\n*   Ensure consistent watering.\n*   Add calcium to the soil.\n*   Avoid root damage during cultivation.\n\nYou can eat the tomato, but cut off the affected part.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 1232,
        "candidates_token_count": 114,
        "total_token_count": 1346
      }
    }),
)
2025-04-25 00:56:53.164198 - Text query received: is it due to rain?...
2025-04-25 00:56:53.165202 - Processing request after 0.00s in queue
2025-04-25 00:56:54.495866 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Whether rain is a factor in blossom end rot depends on the specific circumstances:\n\n*   **Inconsistent Watering:** Rain can definitely contribute to blossom end rot if it leads to inconsistent watering. For example, if you have a period of heavy rain followed by a dry spell, the plant may not be able to take up calcium properly.\n*   **Soil Conditions:** If your soil is already low in calcium, heavy rain can leach out even more calcium, making the problem worse.\n*   **Drainage:** Poor drainage can also be a factor. If the soil stays waterlogged for extended periods, it can damage the roots and prevent them from absorbing nutrients properly.\n\nSo, while rain itself doesn't directly cause blossom end rot, it can certainly be a contributing factor if it leads to inconsistent watering, calcium leaching, or poor drainage.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 1063,
        "candidates_token_count": 171,
        "total_token_count": 1234
      }
    }),
)
2025-04-25 16:12:22.986061 - Text query received: hi...
2025-04-25 16:12:22.987061 - Processing request after 0.00s in queue
2025-04-25 16:12:23.982661 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-04-25 16:24:54.033650 - Text query received: My tomato has black spots. Why...
2025-04-25 16:24:54.034656 - Processing request after 0.00s in queue
2025-04-25 16:24:57.047906 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The black spots on your tomato could be due to several reasons. Here are a few possibilities:\n\n*   **Blossom End Rot:** Despite the name, blossom end rot can sometimes manifest as black spots, especially in its early stages. This is caused by a calcium deficiency, often due to inconsistent watering. The spots will usually be on the bottom (blossom end) of the fruit.\n\n*   **Septoria Leaf Spot:** While primarily a leaf disease, Septoria can sometimes affect the fruit. Look for small, dark, water-soaked spots on the leaves that eventually develop white centers with dark borders. If the infection is severe, it can spread to the fruit.\n\n*   **Anthracnose:** This fungal disease causes small, sunken, circular spots on the fruit that can be black or dark brown. These spots can enlarge and merge, leading to fruit rot.\n\n*   **Early Blight:** This fungal disease usually starts on the leaves, but can also affect the fruit. Look for dark, sunken spots, often with concentric rings.\n\nTo properly diagnose the issue, examine the spots closely and check the leaves for any signs of disease. Providing more details about the appearance of the spots and the condition of the plant would help narrow down the possibilities.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 179,
        "candidates_token_count": 257,
        "total_token_count": 436
      }
    }),
)
2025-04-25 16:28:58.599825 - Text query received: My tomato has black spots why ...
2025-04-25 16:28:58.601344 - Processing request after 0.00s in queue
2025-04-25 16:28:59.171512 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 178,
        "candidates_token_count": 9,
        "total_token_count": 187
      }
    }),
)
2025-04-25 16:32:03.616723 - Text query received: My tomato has black spots why ...
2025-04-25 16:32:03.616723 - Processing request after 0.00s in queue
2025-04-25 16:32:05.869512 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The black spots on your tomato could be due to a few different reasons:\n\n*   **Blossom End Rot:** Despite the name, blossom end rot can sometimes appear as dark, sunken spots, especially in its early stages. This is caused by a calcium deficiency, often due to inconsistent watering.\n\n*   **Septoria Leaf Spot:** While primarily a leaf disease, Septoria can sometimes affect the fruit. Look for small, dark spots on the leaves as well.\n\n*   **Anthracnose:** This fungal disease causes small, circular, sunken spots that can enlarge and merge. It's more common in warm, humid conditions.\n\n*   **Early Blight:** This fungal disease can cause dark spots on both leaves and fruit. The spots on the fruit are usually sunken and leathery.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 178,
        "candidates_token_count": 164,
        "total_token_count": 342
      }
    }),
)
2025-04-25 16:32:37.188073 - Image query received: what is the disease here?...
2025-04-25 16:32:37.188627 - Processing request after 0.00s in queue
2025-04-25 16:32:37.985913 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **tomato** has the **Blossom end rot** disease. This is caused by a calcium deficiency, often due to inconsistent watering. Possible solutions include ensuring consistent watering, testing the soil pH, and adding calcium supplements if needed.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 653,
        "candidates_token_count": 48,
        "total_token_count": 701
      }
    }),
)
2025-04-25 16:32:57.510776 - Text query received: Is it due to rain?...
2025-04-25 16:32:57.511778 - Processing request after 0.00s in queue
2025-04-25 16:32:58.617556 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "While rain itself doesn't directly cause blossom end rot, it can contribute to the problem in a few ways:\n\n*   **Inconsistent Soil Moisture:** Heavy rain followed by dry periods creates fluctuations in soil moisture. This makes it difficult for the plant to consistently absorb calcium, even if it's present in the soil.\n\n*   **Leaching of Nutrients:** Excessive rain can leach calcium and other essential nutrients out of the soil, reducing their availability to the plant.\n\n*   **Root Damage:** Prolonged periods of waterlogged soil can damage the roots, impairing their ability to absorb water and nutrients, including calcium.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 402,
        "candidates_token_count": 130,
        "total_token_count": 532
      }
    }),
)
2025-04-25 16:33:26.999732 - Text query received: My apple leaf has brown spots ...
2025-04-25 16:33:26.999732 - Processing request after 0.00s in queue
2025-04-25 16:33:28.644714 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The brown spots on your apple leaf could be due to several reasons:\n\n*   **Apple Scab:** This is a fungal disease that causes olive-green to brown spots with a velvety texture on leaves. Spots can also appear on fruit.\n*   **Cedar Apple Rust:** This fungal disease causes yellow-orange spots on apple leaves that eventually develop into brown, fringed lesions.\n*   **Leaf Spot Diseases:** Several other fungal or bacterial leaf spot diseases can cause brown spots.\n*   **Chemical Burn:** If you've recently sprayed your apple tree with a pesticide or herbicide, the brown spots could be a sign of chemical burn.\n*   **Nutrient Deficiency:** In some cases, nutrient deficiencies can manifest as brown spots on leaves.\n*   **Insect Damage:** Some insects can cause damage that appears as brown spots.\n\nTo determine the exact cause, examine the spots closely and consider other symptoms on the tree. You may need to consult with a local extension office or arborist for a definitive diagnosis and treatment recommendations.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 181,
        "candidates_token_count": 211,
        "total_token_count": 392
      }
    }),
)
2025-05-05 10:30:42.715593 - Text query received: hello...
2025-05-05 10:30:42.716703 - Processing request after 0.00s in queue
2025-05-05 10:30:44.401990 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-05-05 10:31:33.670845 - Image query received: ...
2025-05-05 10:31:33.671364 - Processing request after 0.00s in queue
2025-05-05 10:31:34.756437 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **tomato** has the **Blossom end rot** disease.\n\n**Blossom end rot** is characterized by a dark, sunken spot on the bottom of the fruit.\n\nPossible reasons: calcium deficiency, inconsistent watering, or root damage.\n\nSuggested solutions: Ensure consistent watering, add calcium to the soil, and avoid damaging the roots.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 479,
        "candidates_token_count": 71,
        "total_token_count": 550
      }
    }),
)
2025-05-05 10:31:53.477611 - Image query received: ...
2025-05-05 10:31:53.478313 - Processing request after 0.00s in queue
2025-05-05 10:31:54.118190 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Apologies, I couldn't identify the plant from this image. Therefore, I cannot detect any disease in this photo.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 549,
        "candidates_token_count": 25,
        "total_token_count": 574
      }
    }),
)
2025-05-05 10:39:14.495995 - Image query received: ...
2025-05-05 10:39:14.497066 - Processing request after 0.00s in queue
2025-05-05 10:39:16.104103 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **potato** has the **hollow heart** disease.\n\n**Hollow heart** is characterized by a cavity in the center of the potato.\n\nPossible reasons: rapid growth fluctuations, inconsistent watering, or nutrient imbalances.\n\nSuggested solutions: Maintain consistent watering, ensure balanced fertilization, and avoid rapid growth spurts.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 569,
        "candidates_token_count": 64,
        "total_token_count": 633
      }
    }),
)
2025-05-05 10:42:37.782514 - Image query received: can i eat this...
2025-05-05 10:42:37.783585 - Processing request after 0.00s in queue
2025-05-05 10:42:38.608514 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Yes, you can eat a **potato** with **hollow heart**, but you should cut away the affected area first. The rest of the potato is safe to consume.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 642,
        "candidates_token_count": 35,
        "total_token_count": 677
      }
    }),
)
2025-05-05 10:43:21.817005 - Image query received: whats this...
2025-05-05 10:43:21.818220 - Processing request after 0.00s in queue
2025-05-05 10:43:22.633643 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Apple leaf** has the **rust leaf** disease.\n\n**Rust leaf** is characterized by orange or reddish-brown spots on the leaves.\n\nPossible reasons: fungal infection, poor air circulation, or wet conditions.\n\nSuggested solutions: Apply fungicide, improve air circulation, and avoid overhead watering.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 683,
        "candidates_token_count": 64,
        "total_token_count": 747
      }
    }),
)
2025-05-05 10:46:14.414561 - Image query received: ...
2025-05-05 10:46:14.416173 - Processing request after 0.00s in queue
2025-05-05 10:46:15.212879 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Apple leaf** has the **rust leaf** disease.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 745,
        "candidates_token_count": 14,
        "total_token_count": 759
      }
    }),
)
2025-05-05 10:47:02.431044 - Image query received: ...
2025-05-05 10:47:02.432686 - Processing request after 0.00s in queue
2025-05-05 10:47:03.274452 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **lemon** has the **canker** disease. **Canker** is characterized by lesions on leaves, stems, and fruit.\n\nPossible reasons: bacterial infection, wet conditions, or poor air circulation.\n\nSuggested solutions: Apply copper-based fungicide, prune affected areas, and improve air circulation.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 754,
        "candidates_token_count": 64,
        "total_token_count": 818
      }
    }),
)
2025-05-05 10:47:54.449358 - Image query received: ...
2025-05-05 10:47:54.450770 - Processing request after 0.00s in queue
2025-05-05 10:47:54.953055 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Apple leaf** has the **rust leaf** disease.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 825,
        "candidates_token_count": 14,
        "total_token_count": 839
      }
    }),
)
2025-05-05 10:49:42.106576 - Image query received: ...
2025-05-05 10:49:42.108240 - Processing request after 0.00s in queue
2025-05-05 10:49:42.991694 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Lettuce** has the **Mosaic Virus** disease. **Mosaic Virus** is characterized by light and dark green mottled patterns on the leaves.\n\nPossible reasons: aphids, infected seeds, or contaminated soil.\n\nSuggested solutions: Remove infected plants, control aphids, and use disease-free seeds.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 834,
        "candidates_token_count": 61,
        "total_token_count": 895
      }
    }),
)
2025-05-05 10:49:57.368058 - Image query received: ...
2025-05-05 10:49:57.369169 - Processing request after 0.00s in queue
2025-05-05 10:49:58.098516 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Lettuce** has the **Mosaic Virus** disease.\n\nPossible reasons: aphids, infected seeds, or contaminated soil.\n\nSuggested solutions: Remove infected plants, control aphids, and use disease-free seeds.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 896,
        "candidates_token_count": 44,
        "total_token_count": 940
      }
    }),
)
2025-05-05 10:50:30.446771 - Image query received: whats this...
2025-05-05 10:50:30.447876 - Processing request after 0.00s in queue
2025-05-05 10:50:31.378428 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Lettuce** has the **Mosaic Virus** disease.\n\nPossible reasons: aphids, infected seeds, or contaminated soil.\n\nSuggested solutions: Remove infected plants, control aphids, and use disease-free seeds.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 945,
        "candidates_token_count": 44,
        "total_token_count": 989
      }
    }),
)
2025-05-05 10:55:25.364637 - Image query received: ...
2025-05-05 10:55:25.365553 - Processing request after 0.00s in queue
2025-05-05 10:55:26.968021 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **tomato** has the **Blossom end rot** disease. **Blossom end rot** is characterized by a dark, sunken spot on the bottom of the fruit.\n\nPossible reasons: calcium deficiency, inconsistent watering, or root damage.\n\nSuggested solutions: Ensure consistent watering, add calcium to the soil, and avoid damaging the roots.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 995,
        "candidates_token_count": 70,
        "total_token_count": 1065
      }
    }),
)
2025-05-05 10:57:07.403143 - Text query received: i think its black spots instea...
2025-05-05 10:57:07.404246 - Processing request after 0.00s in queue
2025-05-05 10:57:08.253537 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "You are right! Upon closer inspection, it appears to be **Black Spot** on the tomato.\n\n**Black Spot** is characterized by dark, sunken spots on the fruit.\n\nPossible reasons: fungal infection, wet conditions, or poor air circulation.\n\nSuggested solutions: Apply fungicide, improve air circulation, and avoid overhead watering.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 776,
        "candidates_token_count": 69,
        "total_token_count": 845
      }
    }),
)
2025-05-05 10:59:13.099203 - Image query received: ...
2025-05-05 10:59:13.101413 - Processing request after 0.00s in queue
2025-05-05 10:59:14.296939 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:16.673905 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:17.987251 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:19.237598 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:20.546384 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:21.815152 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:23.206604 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:24.538748 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:25.871825 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:26.723581 - Text query received: hi...
2025-05-05 10:59:27.085373 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:28.429103 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:29.764288 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:33.362653 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:34.643673 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:36.683158 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 10:59:37.706235 - Throttling request, waiting for 36.40 seconds
2025-05-05 11:00:14.391405 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:16.673183 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:17.702171 - Throttling request, waiting for 1.00 seconds
2025-05-05 11:00:18.988344 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:20.259881 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:21.544596 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:22.799566 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:24.056999 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:25.564829 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:25.566831 - Text query received: hi...
2025-05-05 11:00:27.433439 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:28.678030 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:30.039173 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:31.311219 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:32.639373 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:33.649274 - Throttling request, waiting for 1.72 seconds
2025-05-05 11:00:34.643455 - Text query received: hi...
2025-05-05 11:00:35.643757 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:36.941265 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:00:37.927755 - Throttling request, waiting for 37.19 seconds
2025-05-05 11:01:15.761883 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:17.080654 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:18.090227 - Throttling request, waiting for 1.61 seconds
2025-05-05 11:01:20.878339 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:22.201299 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:23.945423 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:25.270170 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:26.601882 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:27.935468 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:29.266154 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:30.596253 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:31.931581 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:33.254805 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:34.514379 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:35.820329 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:37.154105 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:01:38.161100 - Throttling request, waiting for 37.96 seconds
2025-05-05 11:04:00.954985 - Text query received: hi...
2025-05-05 11:04:00.956753 - Processing request after 0.00s in queue
2025-05-05 11:04:02.572595 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-05-05 11:04:26.596693 - Image query received: what disease is here?...
2025-05-05 11:04:26.597214 - Processing request after 0.00s in queue
2025-05-05 11:04:28.055991 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Lettuce** has the **Mosaic Virus** disease. Mosaic virus causes mottling (or mosaic-like) patterns on the leaves, often accompanied by stunting.\n\nReasons for the disease:\n*   Spread by aphids and other sap-sucking insects.\n*   Contaminated seeds.\n*   Infected weeds or nearby plants.\n\nSolutions:\n*   Control aphids with insecticides or biological control methods.\n*   Remove and destroy infected plants.\n*   Use virus-free seeds.\n*   Control weeds that may harbor the virus.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 480,
        "candidates_token_count": 116,
        "total_token_count": 596
      }
    }),
)
2025-05-05 11:04:47.594372 - Text query received: explain it in simpler terms...
2025-05-05 11:04:47.595275 - Processing request after 0.00s in queue
2025-05-05 11:04:48.240841 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 305,
        "candidates_token_count": 9,
        "total_token_count": 314
      }
    }),
)
2025-05-05 11:05:39.099722 - Text query received: explain in simpler terms...
2025-05-05 11:05:39.100712 - Processing request after 0.00s in queue
2025-05-05 11:05:40.456329 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 318,
        "candidates_token_count": 9,
        "total_token_count": 327
      }
    }),
)
2025-05-05 11:06:04.681185 - Image query received: is it due to rain...
2025-05-05 11:06:04.681746 - Processing request after 0.00s in queue
2025-05-05 11:06:05.849756 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **tomato** has the **Blossom end rot** disease. This is not a disease caused by a pathogen, but a physiological disorder.\n\nPossible reasons:\n*   Calcium deficiency in the developing fruit.\n*   Inconsistent watering (either too much or too little).\n*   Root damage.\n\nSuggested solutions:\n*   Ensure consistent watering.\n*   Add calcium to the soil.\n*   Avoid root damage during cultivation.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 479,
        "candidates_token_count": 93,
        "total_token_count": 572
      }
    }),
)
2025-05-05 11:06:48.787663 - Text query received: is it due to rain...
2025-05-05 11:06:48.788561 - Processing request after 0.00s in queue
2025-05-05 11:06:49.651245 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Inconsistent watering, which can be caused by periods of heavy rain followed by drought, can contribute to blossom end rot in tomatoes. The issue isn't the rain itself, but the fluctuation in soil moisture.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 272,
        "candidates_token_count": 43,
        "total_token_count": 315
      }
    }),
)
2025-05-05 11:06:59.210576 - Text query received: can i eat this tomato ...
2025-05-05 11:06:59.211570 - Processing request after 0.00s in queue
2025-05-05 11:07:00.218737 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "It depends on the extent of the blossom end rot.\n\n*   **Small area affected:** You can cut off the affected part and eat the rest of the tomato. The healthy part of the fruit is perfectly safe to eat.\n*   **Large area affected/significant rot:** It's best to discard the tomato. While not harmful, the texture and taste will be unpleasant.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 321,
        "candidates_token_count": 79,
        "total_token_count": 400
      }
    }),
)
2025-05-05 11:09:33.271709 - Image query received: ...
2025-05-05 11:09:33.272221 - Processing request after 0.00s in queue
2025-05-05 11:09:35.482540 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **potato** has the **hollow heart** disease.\n\n**Hollow heart** is a physiological disorder characterized by internal cavities or cracks in the potato tuber.\n\nPossible reasons: Rapid growth fluctuations, irregular irrigation, or nutrient imbalances.\n\nSuggested solutions: Maintain consistent soil moisture, avoid excessive nitrogen fertilization, and ensure proper spacing between plants.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 461,
        "candidates_token_count": 69,
        "total_token_count": 530
      }
    }),
)
2025-05-05 11:09:44.206887 - Text query received: can i eat it...
2025-05-05 11:09:44.207415 - Processing request after 0.00s in queue
2025-05-05 11:09:45.294405 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Whether or not you can eat a potato with hollow heart depends on the severity:\n\n*   **Mild Cases:** If the hollow area is small and the surrounding potato tissue appears normal, it is generally safe to eat after cutting away the affected portion.\n\n*   **Severe Cases:** If the hollow area is large, discolored, or shows signs of rot or mold, it's best to discard the entire potato.\n\n*   **Taste and Texture:** Even in mild cases, the texture around the hollow area may be less appealing.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 243,
        "candidates_token_count": 110,
        "total_token_count": 353
      }
    }),
)
2025-05-05 11:20:02.604453 - Text query received: how can i eat it...
2025-05-05 11:20:02.607235 - Processing request after 0.00s in queue
2025-05-05 11:20:03.937852 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 358,
        "candidates_token_count": 9,
        "total_token_count": 367
      }
    }),
)
2025-05-05 11:20:19.871950 - Text query received: how can i eat ot...
2025-05-05 11:20:19.872834 - Processing request after 0.00s in queue
2025-05-05 11:20:20.442800 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "I\u2019m sorry, I only talk about plants.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 372,
        "candidates_token_count": 12,
        "total_token_count": 384
      }
    }),
)
2025-05-05 11:20:31.976074 - Text query received: how do i eat the plant...
2025-05-05 11:20:31.977077 - Processing request after 0.00s in queue
2025-05-05 11:20:33.456493 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "To safely eat a plant, you need to consider a few things:\n\n*   **Identification:** Make absolutely sure you know what plant it is. Many plants are poisonous. Use a reliable field guide or consult an expert.\n*   **Edible Parts:** Different parts of a plant may be edible (leaves, roots, fruits, seeds), while others are toxic.\n*   **Preparation:** Some plants need to be cooked to remove toxins or improve digestibility. Others can be eaten raw, but only if properly cleaned.\n*   **Allergies:** Be aware of potential allergies. Start with a small amount to see if you have any reaction.\n*   **Harvesting:** Harvest responsibly to ensure the plant's survival and sustainability.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 390,
        "candidates_token_count": 153,
        "total_token_count": 543
      }
    }),
)
2025-05-05 11:22:10.053716 - Text query received: so these are all important?...
2025-05-05 11:22:10.054713 - Processing request after 0.00s in queue
2025-05-05 11:22:11.089775 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Yes, all those points are important when considering eating a plant.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 549,
        "candidates_token_count": 14,
        "total_token_count": 563
      }
    }),
)
2025-05-05 11:47:13.708757 - Text query received: hi...
2025-05-05 11:47:13.710204 - Processing request after 0.00s in queue
2025-05-05 11:47:15.119122 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:16.374765 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:17.862462 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:19.198320 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:20.517893 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:22.199638 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:23.696817 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:25.122605 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:26.466116 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:27.722860 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:29.125771 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:30.455316 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:31.653473 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:33.019792 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:34.294004 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:47:35.305463 - Throttling request, waiting for 39.41 seconds
2025-05-05 11:48:15.103577 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:16.328295 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:17.305788 - Throttling request, waiting for 1.05 seconds
2025-05-05 11:48:18.689870 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:20.015255 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:21.352997 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:22.871953 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:24.063676 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:25.249772 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:26.429845 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:27.444960 - Throttling request, waiting for 1.03 seconds
2025-05-05 11:48:28.774604 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:30.459607 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:31.757469 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:32.977268 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:34.195723 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:35.419251 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:48:36.428231 - Throttling request, waiting for 39.27 seconds
2025-05-05 11:49:06.987727 - Text query received: hi...
2025-05-05 11:49:16.074730 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:17.272356 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:18.254999 - Throttling request, waiting for 1.11 seconds
2025-05-05 11:49:19.584757 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:20.808730 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:22.133845 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:23.358049 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:24.577149 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:25.764740 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:27.002793 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:28.025869 - Throttling request, waiting for 1.46 seconds
2025-05-05 11:49:29.810820 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:31.031496 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:32.259929 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:33.480003 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:35.025233 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:36.256051 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:49:37.273561 - Throttling request, waiting for 39.44 seconds
2025-05-05 11:50:17.110343 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:18.360255 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:19.367826 - Throttling request, waiting for 1.01 seconds
2025-05-05 11:50:20.572807 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:21.924932 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:23.179772 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:24.414021 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:25.636111 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:26.855867 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:28.083603 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:29.110997 - Throttling request, waiting for 1.38 seconds
2025-05-05 11:50:30.712737 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:31.928724 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:33.163722 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:34.416762 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:35.625439 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:36.870860 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:50:37.858583 - Throttling request, waiting for 39.85 seconds
2025-05-05 11:51:04.041276 - Text query received: hi...
2025-05-05 11:51:18.035631 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:19.273290 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:20.294200 - Throttling request, waiting for 1.09 seconds
2025-05-05 11:51:21.592308 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:22.798288 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:24.091126 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:25.319230 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:26.514684 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:27.781700 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:29.021456 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:30.044981 - Throttling request, waiting for 1.46 seconds
2025-05-05 11:51:31.721052 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:32.960863 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:34.188588 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:35.425616 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:36.676207 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:37.894797 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:51:38.916444 - Throttling request, waiting for 39.79 seconds
2025-05-05 11:52:18.936789 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:20.154284 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:21.128168 - Throttling request, waiting for 1.26 seconds
2025-05-05 11:52:22.616117 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:23.800910 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:25.027331 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:26.220897 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:27.455782 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:28.679808 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:29.908316 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:30.926278 - Throttling request, waiting for 1.58 seconds
2025-05-05 11:52:32.741025 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:33.938162 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:35.161966 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:36.387884 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:37.621518 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:38.812134 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:52:39.823004 - Throttling request, waiting for 39.90 seconds
2025-05-05 11:53:20.209725 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:21.549303 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:22.771090 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:24.058199 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:25.269830 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:26.488929 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:27.713939 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:28.935531 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:30.115554 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:31.097033 - Throttling request, waiting for 2.43 seconds
2025-05-05 11:53:33.926920 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:35.141422 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:36.379364 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:37.585076 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:38.796070 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:40.097275 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:53:41.123328 - Throttling request, waiting for 39.60 seconds
2025-05-05 11:54:21.145981 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:22.368890 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:23.589062 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:24.811031 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:26.018026 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:27.214945 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:28.450666 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:29.623837 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:30.802731 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:31.827705 - Throttling request, waiting for 2.71 seconds
2025-05-05 11:54:34.748944 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:35.944809 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:37.161738 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:38.383288 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:39.602035 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:40.811474 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:54:41.826702 - Throttling request, waiting for 39.89 seconds
2025-05-05 11:55:22.046255 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:23.230535 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:24.462541 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:25.666557 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:26.869129 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:28.081452 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:29.316779 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:30.489040 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:31.698494 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:32.678151 - Throttling request, waiting for 2.86 seconds
2025-05-05 11:55:35.778910 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:36.979330 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:38.208548 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:39.464424 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:40.676969 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:41.900261 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:55:42.921334 - Throttling request, waiting for 39.79 seconds
2025-05-05 11:55:45.556707 - Text query received: hi...
2025-05-05 11:56:16.054113 - Text query received: hello...
2025-05-05 11:56:23.206045 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:24.428911 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:25.654010 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:26.869552 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:28.059810 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:29.301055 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:30.526165 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:31.699749 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:32.906445 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:33.891340 - Throttling request, waiting for 2.65 seconds
2025-05-05 11:56:36.777833 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:37.980235 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:39.185996 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:40.419113 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:41.653441 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:42.831603 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:56:43.856563 - Throttling request, waiting for 39.85 seconds
2025-05-05 11:56:46.161773 - Text query received: heus...
2025-05-05 11:57:23.921198 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:25.256420 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:26.485272 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:27.705329 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:28.942859 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:30.165721 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:31.381170 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:32.601034 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:34.063834 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:35.084042 - Throttling request, waiting for 2.48 seconds
2025-05-05 11:57:37.792537 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:39.007083 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:40.205493 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:41.428321 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:42.668577 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:43.887964 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:57:44.894486 - Throttling request, waiting for 39.81 seconds
2025-05-05 11:58:06.317104 - Text query received: hi...
2025-05-05 11:58:17.235356 - Text query received: hi...
2025-05-05 11:58:25.220811 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:26.536893 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:27.752563 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:29.103248 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:30.325873 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:31.544472 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:31.587619 - Text query received: helo...
2025-05-05 11:58:32.891909 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:34.230216 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:35.661906 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:36.670345 - Throttling request, waiting for 1.90 seconds
2025-05-05 11:58:38.799509 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:40.157100 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:41.437547 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:42.664037 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:43.998680 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:45.290331 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:58:46.313154 - Throttling request, waiting for 39.42 seconds
2025-05-05 11:59:26.148452 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:27.347382 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:28.615758 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:29.287610 - Text query received: hello...
2025-05-05 11:59:29.928230 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:31.142981 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:32.349744 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:33.584944 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:34.950070 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:36.384086 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:37.391594 - Throttling request, waiting for 2.17 seconds
2025-05-05 11:59:39.829019 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:41.191254 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:42.548944 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:43.801785 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:45.027951 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:46.208257 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 11:59:47.216008 - Throttling request, waiting for 39.53 seconds
2025-05-05 11:59:49.326593 - Text query received: hi...
2025-05-05 12:00:27.116209 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:28.398032 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:29.652422 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:31.005999 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:32.234271 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:33.648365 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:34.896803 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:36.286258 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:37.621397 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:38.596303 - Throttling request, waiting for 1.98 seconds
2025-05-05 12:00:40.898230 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:42.232250 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:43.446592 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:44.731104 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:46.124160 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:47.288962 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:00:48.308740 - Throttling request, waiting for 39.44 seconds
2025-05-05 12:01:28.145742 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:29.532964 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:30.796037 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:32.308916 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:33.574349 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:34.976920 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:36.239378 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:37.629402 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:38.866649 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:39.851694 - Throttling request, waiting for 1.75 seconds
2025-05-05 12:01:42.031138 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:43.304710 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:44.686731 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:46.020909 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:47.466061 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:48.791232 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:01:49.807927 - Throttling request, waiting for 38.94 seconds
2025-05-05 12:02:28.945988 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:30.132928 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:31.405668 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:32.577847 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:33.792732 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:35.014105 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:36.224645 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:37.451156 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:38.429522 - Throttling request, waiting for 1.22 seconds
2025-05-05 12:02:39.939011 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:40.961183 - Throttling request, waiting for 1.63 seconds
2025-05-05 12:02:42.828322 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:44.051667 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:45.289063 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:46.521122 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:47.832928 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:49.067166 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:02:50.051773 - Throttling request, waiting for 39.70 seconds
2025-05-05 12:03:30.133904 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:31.363276 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:32.594323 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:33.836066 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:35.065253 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:36.365043 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:37.604965 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:38.807394 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:40.157832 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:41.180537 - Throttling request, waiting for 2.43 seconds
2025-05-05 12:03:43.802836 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:45.043965 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:46.261527 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:47.478930 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:48.697971 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:49.889332 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:03:50.908064 - Throttling request, waiting for 39.85 seconds
2025-05-05 12:04:31.346475 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:32.669794 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:33.884187 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:35.124663 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:36.298962 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:37.509408 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:38.728278 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:39.946551 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:41.128554 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:42.149719 - Throttling request, waiting for 2.47 seconds
2025-05-05 12:04:44.817937 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:46.007506 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:47.245518 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:48.446490 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:49.686553 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:50.920775 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:04:51.936877 - Throttling request, waiting for 39.83 seconds
2025-05-05 12:05:32.284472 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:33.495970 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:34.731147 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:35.939761 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:37.175969 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:38.404460 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:39.855582 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:41.069117 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:42.306144 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:43.321055 - Throttling request, waiting for 2.30 seconds
2025-05-05 12:05:45.860321 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:47.121306 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:48.295775 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:49.507733 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:50.740782 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:51.967223 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:05:52.964889 - Throttling request, waiting for 39.80 seconds
2025-05-05 12:06:33.313310 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:34.545656 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:35.756915 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:36.997697 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:38.193216 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:39.418356 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:40.635547 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:41.849037 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:43.069066 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:44.092277 - Throttling request, waiting for 2.55 seconds
2025-05-05 12:06:46.845724 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:48.081916 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:49.304283 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:50.618515 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:51.800274 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:53.058265 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:06:54.053931 - Throttling request, waiting for 39.73 seconds
2025-05-05 12:07:47.689163 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:07:48.893637 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:07:50.122503 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:07:51.341322 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:07:52.557287 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:07:53.764919 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:07:54.982097 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:07:56.178591 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:07:57.384614 - Error: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini
2025-05-05 12:09:16.712070 - Text query received: hi...
2025-05-05 12:09:16.712965 - Processing request after 0.00s in queue
2025-05-05 12:09:18.002313 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-05-05 12:10:51.605540 - Image query received: ...
2025-05-05 12:10:51.606074 - Processing request after 0.00s in queue
2025-05-05 12:10:53.103603 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Lettuce** has the **Mosaic Virus** disease.\n\nThe **Mosaic Virus** causes mottling (light and dark green areas) on the leaves, often accompanied by blistering and distortion. It can stunt plant growth and reduce yield.\n\nPossible reasons for the disease:\n*   Infected seeds or transplants.\n*   Aphids or other sap-sucking insects transmitting the virus.\n*   Contaminated tools or equipment.\n\nSuggested solutions to mitigate or cure the disease:\n*   Remove and destroy infected plants immediately.\n*   Control aphids and other insect vectors.\n*   Use disease-free seeds and transplants.\n*   Disinfect tools and equipment regularly.\n*   Practice crop rotation.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 461,
        "candidates_token_count": 148,
        "total_token_count": 609
      }
    }),
)
2025-05-05 12:11:15.021625 - Text query received: can i eat it...
2025-05-05 12:11:15.022257 - Processing request after 0.00s in queue
2025-05-05 12:11:16.710755 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Regarding lettuce infected with the Mosaic Virus, it's generally **not recommended** to eat it. Here's why:\n\n*   **Appearance and Texture:** The virus can cause the lettuce to be distorted, blistered, and have an unappetizing appearance. The texture may also be affected, making it less palatable.\n*   **Potential for Secondary Issues:** While the Mosaic Virus itself isn't harmful to humans, the affected lettuce may be more susceptible to secondary infections from bacteria or fungi.\n*   **Reduced Nutritional Value:** The virus can interfere with the plant's ability to photosynthesize properly, potentially reducing its nutritional content.\n\nWhile the virus itself isn't known to be directly harmful to humans, the overall quality and potential for other issues make it best to avoid consuming infected lettuce.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 322,
        "candidates_token_count": 168,
        "total_token_count": 490
      }
    }),
)
2025-05-05 12:11:38.561166 - Image query received: ...
2025-05-05 12:11:38.561166 - Processing request after 0.00s in queue
2025-05-05 12:11:39.904863 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **potato** has the **hollow heart** disease.\n\n**Hollow heart** is a physiological disorder characterized by internal cavities or cracks in the potato tuber.\n\nPossible reasons: Rapid growth fluctuations, inconsistent irrigation, and nutrient imbalances (particularly calcium).\n\nSuggested solutions: Maintain consistent soil moisture, avoid excessive nitrogen fertilization, ensure adequate calcium levels, and promote uniform tuber growth.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 461,
        "candidates_token_count": 76,
        "total_token_count": 537
      }
    }),
)
2025-05-06 12:55:54.461143 - Text query received: hi...
2025-05-06 12:55:54.462202 - Processing request after 0.00s in queue
2025-05-06 12:55:56.173217 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-05-06 12:56:30.979477 - Image query received: any disease here...
2025-05-06 12:56:30.980082 - Processing request after 0.00s in queue
2025-05-06 12:56:32.313159 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Lettuce** has the **Mosaic Virus** disease.\n\n**Mosaic Virus** is characterized by light and dark green mottled areas that give the leaves a mosaic pattern.\n\nReasons for the disease:\n*   Infected seeds\n*   Insects\n*   Contaminated soil\n\nSolutions to mitigate or cure the disease:\n*   Remove infected plants\n*   Control insects\n*   Use resistant varieties\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 477,
        "candidates_token_count": 84,
        "total_token_count": 561
      }
    }),
)
2025-05-06 13:05:39.751411 - Text query received: hi...
2025-05-06 13:05:39.751931 - Processing request after 0.00s in queue
2025-05-06 13:05:41.259254 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-05-06 13:08:55.846902 - Image query received: ...
2025-05-06 13:08:55.848158 - Processing request after 0.00s in queue
2025-05-06 13:08:56.642522 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Apologies, I couldn't identify the plant from this image. Therefore, I cannot detect any disease in this photo.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 477,
        "candidates_token_count": 25,
        "total_token_count": 502
      }
    }),
)
2025-05-06 13:13:47.258226 - Image query received: any disease here?...
2025-05-06 13:13:47.259654 - Processing request after 0.00s in queue
2025-05-06 13:13:49.100979 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **potato** has the **hollow heart** disease. **Hollow heart** is a physiological disorder characterized by a cavity in the center of the potato tuber.\n\nPossible reasons:\n*   Rapid growth fluctuations.\n*   Inconsistent soil moisture.\n*   Low calcium levels.\n\nSuggested solutions:\n*   Maintain consistent irrigation.\n*   Ensure balanced fertilization.\n*   Avoid over-fertilization with nitrogen.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 504,
        "candidates_token_count": 88,
        "total_token_count": 592
      }
    }),
)
2025-05-06 13:20:11.752171 - Image query received: ...
2025-05-06 13:20:11.753832 - Processing request after 0.00s in queue
2025-05-06 13:20:13.114874 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Apple leaf** has the **rust leaf** disease. **Rust leaf** is a fungal disease that affects apple trees, causing orange or reddish spots on the leaves.\n\nPossible reasons:\n*   Presence of nearby juniper trees, which act as an alternate host.\n*   Wet and humid conditions.\n\nSuggested solutions:\n*   Remove juniper trees from the vicinity.\n*   Apply appropriate fungicides.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 595,
        "candidates_token_count": 86,
        "total_token_count": 681
      }
    }),
)
2025-05-06 13:20:45.556940 - Image query received: ...
2025-05-06 13:20:45.558040 - Processing request after 0.00s in queue
2025-05-06 13:20:46.752197 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Squash leaf** has the **Powdery mildew leaf** disease. **Powdery mildew** is a fungal disease that appears as white, powdery spots on plant leaves and stems.\n\nPossible reasons:\n*   High humidity and poor air circulation.\n*   Overcrowding of plants.\n\nSuggested solutions:\n*   Improve air circulation.\n*   Apply fungicides.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 696,
        "candidates_token_count": 80,
        "total_token_count": 776
      }
    }),
)
2025-05-06 13:22:10.368629 - Image query received: ...
2025-05-06 13:22:10.369184 - Processing request after 0.00s in queue
2025-05-06 13:22:11.690451 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **grape leaf** has the **black rot** disease. **Black rot** is a fungal disease that affects grapevines, causing leaf spots, berry rot, and stem lesions.\n\nPossible reasons:\n*   Wet and humid conditions.\n*   Poor air circulation.\n*   Infected plant debris.\n\nSuggested solutions:\n*   Improve air circulation.\n*   Remove infected plant parts.\n*   Apply appropriate fungicides.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 763,
        "candidates_token_count": 93,
        "total_token_count": 856
      }
    }),
)
2025-05-06 13:24:51.729817 - Image query received: ...
2025-05-06 13:24:51.731482 - Processing request after 0.00s in queue
2025-05-06 13:24:52.661618 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Apple leaf** has the **rust leaf** disease. **Rust leaf** is a fungal disease that affects apple trees, causing orange or reddish spots on the leaves.\n\nPossible reasons:\n*   Presence of nearby juniper trees, which act as an alternate host.\n*   Wet and humid conditions.\n\nSuggested solutions:\n*   Remove juniper trees from the vicinity.\n*   Apply appropriate fungicides.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 857,
        "candidates_token_count": 86,
        "total_token_count": 943
      }
    }),
)
2025-05-06 13:25:42.701471 - Image query received: ...
2025-05-06 13:25:42.702757 - Processing request after 0.00s in queue
2025-05-06 13:25:44.063213 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Potato leaf** has the **Early blight** disease. **Early blight** is a fungal disease that causes dark, target-like spots on leaves.\n\nPossible reasons:\n*   Infected seeds or tubers.\n*   Warm, humid conditions.\n*   Poor air circulation.\n\nSuggested solutions:\n*   Use disease-free seeds.\n*   Improve air circulation.\n*   Apply appropriate fungicides.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 944,
        "candidates_token_count": 89,
        "total_token_count": 1033
      }
    }),
)
2025-05-06 13:35:04.390539 - Image query received: any disease here?...
2025-05-06 13:35:04.392347 - Processing request after 0.00s in queue
2025-05-06 13:35:05.944988 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **tomato** has the **Blossom end rot** disease. This is a condition where the blossom end of the fruit develops a dark, sunken, and leathery spot. It's often caused by calcium deficiency in the developing fruit, which can be due to inconsistent watering, poor calcium uptake from the soil, or root damage. To address this, ensure consistent watering, amend the soil with calcium if needed (according to a soil test), and avoid over-fertilizing with nitrogen.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 476,
        "candidates_token_count": 101,
        "total_token_count": 577
      }
    }),
)
2025-05-06 13:41:34.831682 - Image query received: ...
2025-05-06 13:41:34.832782 - Processing request after 0.00s in queue
2025-05-06 13:41:36.240775 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Apologies, I couldn't identify the plant from this image. Therefore, I cannot detect any disease in this photo.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 467,
        "candidates_token_count": 25,
        "total_token_count": 492
      }
    }),
)
2025-05-06 13:52:10.023947 - Text query received: how to take care of an almond ...
2025-05-06 13:52:10.024948 - Processing request after 0.00s in queue
2025-05-06 13:52:11.446569 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 177,
        "candidates_token_count": 9,
        "total_token_count": 186
      }
    }),
)
2025-05-06 13:52:36.371095 - Image query received: what us this...
2025-05-06 13:52:36.372340 - Processing request after 0.00s in queue
2025-05-06 13:52:37.872266 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **grape leaf** has the **black rot** disease. **Black rot** is a fungal disease that affects grapevines, causing circular brown spots on leaves, which eventually develop black fruiting bodies.\n\nReasons for the disease:\n*   Wet and humid conditions favor the development of black rot.\n*   Poor air circulation within the vine canopy.\n*   Infected plant debris from previous seasons.\n\nSolutions:\n*   Prune vines to improve air circulation.\n*   Remove and destroy infected leaves and fruit.\n*   Apply a fungicide labeled for use on grapevines, following the product instructions.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 490,
        "candidates_token_count": 129,
        "total_token_count": 619
      }
    }),
)
2025-05-06 13:56:58.894246 - Text query received: is it dangerous?...
2025-05-06 13:56:58.895243 - Processing request after 0.00s in queue
2025-05-06 13:57:01.547545 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "For humans and animals, black rot on grape leaves isn't directly dangerous in the sense of being poisonous or causing immediate harm. However, it can indirectly affect you:\n\n*   **Reduced Grape Harvest:** If the infection is severe, it can significantly reduce the grape yield, impacting the availability of grapes for consumption.\n*   **Aesthetic Concerns:** The diseased leaves and fruit can be unsightly.\n*   **Potential Allergens:** Mold and fungal spores (like those from black rot) can be allergenic to some individuals, potentially causing respiratory issues or skin irritation in sensitive people if they are heavily exposed.\n\nIn short, it's not a direct poison, but it's best to manage the disease to protect your grape harvest and minimize potential allergy issues.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 322,
        "candidates_token_count": 156,
        "total_token_count": 478
      }
    }),
)
2025-05-06 14:06:27.267905 - Image query received: any disease here?...
2025-05-06 14:06:27.268599 - Processing request after 0.00s in queue
2025-05-06 14:06:30.633318 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Lettuce** has the **Mosaic Virus** disease. Mosaic Virus causes mottling (patterns of light and dark green or yellow) on the leaves.\n\nReasons for the disease:\n*   Infected seeds.\n*   Insects, such as aphids, thrips, and leafhoppers, can transmit the virus.\n*   Infected weeds or other crops.\n\nSolutions to mitigate or cure the disease:\n*   Remove infected plants to prevent spread.\n*   Control aphids and other insect vectors.\n*   Use virus-free seeds and resistant varieties.\n*   Control weeds, which may harbor the virus.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 468,
        "candidates_token_count": 130,
        "total_token_count": 598
      }
    }),
)
2025-05-06 14:07:02.813099 - Text query received: can i eat it?...
2025-05-06 14:07:02.814200 - Processing request after 0.00s in queue
2025-05-06 14:07:04.323987 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Whether or not you can eat lettuce infected with Mosaic Virus depends on a few factors:\n\n*   **Severity of infection:** If the infection is mild and only affects a small portion of the plant, it might still be edible. However, the taste and texture might be altered.\n*   **Appearance:** Check for signs of rot or decay. If the lettuce looks or smells bad, it's best to discard it.\n*   **Personal preference:** Some people might be more sensitive to the altered taste or texture of infected lettuce and choose not to eat it.\n\n**General recommendation:** While Mosaic Virus doesn't pose a direct health risk to humans, it's generally recommended to avoid eating infected lettuce, especially if the infection is severe or if there are signs of rot.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 308,
        "candidates_token_count": 160,
        "total_token_count": 468
      }
    }),
)
2025-05-06 14:37:49.726299 - Text query received: hi...
2025-05-06 14:37:49.727354 - Processing request after 0.00s in queue
2025-05-06 14:37:51.246007 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Hi! How can I help you?\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 170,
        "candidates_token_count": 9,
        "total_token_count": 179
      }
    }),
)
2025-05-06 15:43:39.091083 - Image query received: any disease here?...
2025-05-06 15:43:39.091605 - Processing request after 0.00s in queue
2025-05-06 15:43:41.283360 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "The **Lettuce** has the **Mosaic Virus** disease. Mosaic Virus causes mottled green and yellow patterns on the leaves, which may also be wrinkled or distorted.\n\nReasons:\n*   Spread by aphids and other sap-sucking insects.\n*   Contaminated seeds.\n*   Infected weeds or crops nearby.\n\nSolutions:\n*   Control aphids with insecticides or natural predators.\n*   Use virus-free seeds.\n*   Remove infected plants promptly.\n*   Control weeds that may harbor the virus.\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 468,
        "candidates_token_count": 109,
        "total_token_count": 577
      }
    }),
)
2025-05-06 15:48:12.176129 - Text query received: get good...
2025-05-06 15:48:12.177115 - Processing request after 0.00s in queue
2025-05-06 15:48:13.859532 - Success: response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      "candidates": [
        {
          "content": {
            "parts": [
              {
                "text": "Okay, I understand. I will only provide information about plants, respond politely to greetings, and answer plant-related questions helpfully. If a question is unrelated to plants, I will respond with \"I\u2019m sorry, I only talk about plants.\"\n"
              }
            ],
            "role": "model"
          },
          "finish_reason": "STOP"
        }
      ],
      "usage_metadata": {
        "prompt_token_count": 276,
        "candidates_token_count": 52,
        "total_token_count": 328
      }
    }),
)
